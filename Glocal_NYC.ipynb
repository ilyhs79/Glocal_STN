{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV_EdNWClAq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24c9327-04a7-40a9-8d39-2db11658fe4a"
      },
      "source": [
        "# google drive connect\n",
        "Copied_path = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN' # Paste target directory here\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(Copied_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS2vPiaBYItm"
      },
      "source": [
        "# global\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/global.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AauHopRUnGYv"
      },
      "source": [
        "# m8_shop\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m8_Shop.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgFX7E7fo_a8"
      },
      "source": [
        "# DST Network/ilayer.py\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "# from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class iLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        # self.output_dim = output_dim\n",
        "        super(iLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        initial_weight_value = np.random.random(input_shape[1:])\n",
        "        self.W = K.variable(initial_weight_value)\n",
        "        #self.trainable_weights = [self.W]\n",
        "        self.trainable_weight = [self.W]\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return x * self.W\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k925xkl8Uynj"
      },
      "source": [
        "# \"metrics.py\"\n",
        "\n",
        "# import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "# aliases\n",
        "mse = MSE = mean_squared_error\n",
        "# rmse = RMSE = root_mean_square_error\n",
        "\n",
        "\n",
        "def masked_mean_squared_error(y_true, y_pred):\n",
        "    idx = (y_true > 1e-6).nonzero()\n",
        "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "threshold=0.05\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8M9judzxQIB"
      },
      "source": [
        "# \"DeepSTN_net.py\"\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import numpy as np\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n",
        "from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "#import PPT3_network.metrics as metrics\n",
        "\n",
        "# Relu-BN-Conv2D 3x3\n",
        "def conv_unit0(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    #print('kernel=(3,3)')\n",
        "    return unit_model\n",
        "\n",
        "# Relu-BN-Conv2D 1x1\n",
        "def conv_unit1(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    #print('kernel=(1,1)')\n",
        "    return unit_model\n",
        "\n",
        "# efficient version of Res_plus\n",
        "def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    #normal channels\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    #separated channels\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n",
        "    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n",
        "    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n",
        "\n",
        "    #merge\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# new resdual block\n",
        "def Res_plus(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n",
        "\n",
        "    cl_conv1B=plus_conv(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n",
        "\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# normal residual block\n",
        "def Res_normal(name,F,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "def cpt_slice(x, h1, h2):\n",
        "    return x[:,h1:h2,:,:]\n",
        "\n",
        "# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)\n",
        "def T_trans(T,T_F,H,W):\n",
        "\n",
        "    T_in=Input(shape=(T+7,H,W))\n",
        "    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n",
        "    T_act=Activation('relu')(T_mid)\n",
        "    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n",
        "    T_mul=Activation('relu')(T_fin)\n",
        "    T_model=Model(inputs=T_in,outputs=T_mul)\n",
        "\n",
        "    return T_model\n",
        "\n",
        "# transfer Time vector and PoI matrix to time-weighted PoI matrix\n",
        "def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n",
        "    if 1:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        if P_N>=2:\n",
        "            T_x0 =T_trans(T,T_F,H,W)(time_in)\n",
        "            T_x1 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=3:\n",
        "            T_x2 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=4:\n",
        "            T_x3 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=5:\n",
        "            T_x4 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=6:\n",
        "            T_x5 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=7:\n",
        "            T_x6 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=8:\n",
        "            T_x7 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=9:\n",
        "            T_x8 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=10:\n",
        "            T_x9 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=11:\n",
        "            T_x10=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=12:\n",
        "            T_x11=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=13:\n",
        "            T_x12=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=14:\n",
        "            T_x13=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=15:\n",
        "            T_x14=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=16:\n",
        "            T_x15=T_trans(T,T_F,H,W)(time_in)\n",
        "\n",
        "\n",
        "        if P_N==1:\n",
        "            T_x=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N==2:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1])\n",
        "        if P_N==3:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n",
        "        if P_N==4:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n",
        "        if P_N==5:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n",
        "        if P_N==6:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n",
        "        if P_N==7:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n",
        "        if P_N==8:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n",
        "        if P_N==9:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n",
        "        if P_N==10:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n",
        "        if P_N==11:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n",
        "        if P_N==12:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n",
        "        if P_N==13:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n",
        "        if P_N==14:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n",
        "        if P_N==15:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n",
        "        if P_N==16:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n",
        "\n",
        "        poi_time=Multiply()([poi_in,T_x])\n",
        "        if isPT_F:\n",
        "            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n",
        "            print('PT_F = YES')\n",
        "        else:\n",
        "            print('PT_F = NO')\n",
        "        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n",
        "\n",
        "        return PT_model\n",
        "\n",
        "# main model\n",
        "def DeepSTN(H=14,W=12,channel=1,      #H-map_height W-map_width channel-map_channel\n",
        "            c=3,p=4,t=4,              #c-closeness p-period t-trend\n",
        "            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n",
        "            is_plus=True,             #use ResPlus or mornal convolution\n",
        "            is_plus_efficient=False,  #use the efficient version of ResPlus\n",
        "            plus=8,rate=2,            #rate-pooling_rate\n",
        "            is_pt=False,               #use PoI and Time or not\n",
        "            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day\n",
        "            drop=0,\n",
        "            is_summary=False, #show detail\n",
        "            lr=0.0002,\n",
        "            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n",
        "            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n",
        "\n",
        "    all_channel = channel * (c+p+t)\n",
        "\n",
        "    cut0 = int( 0 )\n",
        "    cut1 = int( cut0 + channel*c )\n",
        "    cut2 = int( cut1 + channel*p )\n",
        "    cut3 = int( cut2 + channel*t )\n",
        "\n",
        "    cpt_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n",
        "    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n",
        "    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n",
        "\n",
        "    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n",
        "    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n",
        "    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n",
        "\n",
        "\n",
        "    cpt_global_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_global_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_global_input)\n",
        "    p_global_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_global_input)\n",
        "    t_global_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_global_input)\n",
        "\n",
        "    c_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_global_input)\n",
        "    p_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_global_input)\n",
        "    t_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_global_input)\n",
        "\n",
        "    if is_pt:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n",
        "\n",
        "        poi_time=PT_model([poi_in,time_in])\n",
        "\n",
        "        cpt_con1=Concatenate(axis=1)([c_global_out1,p_global_out1,t_global_out1,c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    else:\n",
        "        cpt_con1=Concatenate(axis=1)([c_global_out1,p_global_out1,t_global_out1,c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*6,conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*6,conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    if is_plus:\n",
        "        if is_plus_efficient:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "        else:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "\n",
        "    else:\n",
        "        for i in range(R_N):\n",
        "            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    cpt_conv2=Activation('relu')(cpt)\n",
        "    cpt_out2=BatchNormalization()(cpt_conv2)\n",
        "    cpt_conv1=Dropout(drop)(cpt_out2)\n",
        "    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n",
        "\n",
        "    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조\n",
        "\n",
        "    cpt_out1=Activation('tanh')(cpt_conv1)\n",
        "\n",
        "    if is_pt:\n",
        "        DeepSTN_model=Model(inputs=[cpt_global_input, cpt_input,poi_in,time_in],outputs=cpt_out1)\n",
        "    else:\n",
        "        DeepSTN_model=Model(inputs=[cpt_global_input, cpt_input],outputs=cpt_out1) # 여기에 글로벌 인풋도 추가\n",
        "\n",
        "    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[rmse,mae])\n",
        "\n",
        "    if is_summary:\n",
        "        DeepSTN_model.summary()\n",
        "\n",
        "    '''\n",
        "    print('***** pre_F : ',pre_F       )\n",
        "    print('***** conv_F: ',conv_F      )\n",
        "    print('***** R_N   : ',R_N         )\n",
        "\n",
        "    print('***** plus  : ',plus*is_plus)\n",
        "    print('***** rate  : ',rate*is_plus)\n",
        "\n",
        "    print('***** P_N   : ',P_N*is_pt   )\n",
        "    print('***** T_F   : ',T_F*is_pt   )\n",
        "    print('***** PT_F  : ',PT_F*is_pt*isPT_F )\n",
        "    print('***** T     : ',T           )\n",
        "\n",
        "    print('***** drop  : ',drop        )\n",
        "    '''\n",
        "    return DeepSTN_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02uqnK3dUwka",
        "outputId": "ec94514a-8c51-4a34-d791-a4114126388a"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 16,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=3\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "R_N=10\n",
        "\n",
        "is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 60.0  min= 0.0\n",
            "mean= -0.9908684895833333  variance= 0.04447277708632065\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 227.0  min= 0.0\n",
            "mean= -0.9869551899779736  variance= 0.060455517211815774\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00068  0.025433 0.006637]\n",
            "Test  score: [0.000372 0.019276 0.004805]\n",
            "RMSE  MAE\n",
            "[[0.0193 0.0048]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0064 0.0016]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000693 0.025661 0.006663]\n",
            "Test  score: [0.000392 0.019803 0.004913]\n",
            "RMSE  MAE\n",
            "[[0.0193 0.0048]\n",
            " [0.0198 0.0049]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.013  0.0032]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000791 0.027384 0.007009]\n",
            "Test  score: [0.000451 0.021236 0.005253]\n",
            "RMSE  MAE\n",
            "[[0.0193 0.0048]\n",
            " [0.0198 0.0049]\n",
            " [0.0212 0.0053]]\n",
            "RMSE  MAE\n",
            "[0.0201 0.005 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTujgh28pvkw"
      },
      "source": [
        "# m0_Ent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m0_Ent.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaareiVrtfkE",
        "outputId": "3020ef3b-acd5-4b17-c32e-6dabeb31e6ff"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 49.0  min= 0.0\n",
            "mean= -0.9962797619047619  variance= 0.02770920949471694\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 227.0  min= 0.0\n",
            "mean= -0.9869551899779736  variance= 0.060455517211815774\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000359 0.018079 0.002731]\n",
            "Test  score: [0.000233 0.015277 0.002085]\n",
            "RMSE  MAE\n",
            "[[0.0153 0.0021]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0051 0.0007]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000369 0.018353 0.002784]\n",
            "Test  score: [0.000245 0.015646 0.00214 ]\n",
            "RMSE  MAE\n",
            "[[0.0153 0.0021]\n",
            " [0.0156 0.0021]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0103 0.0014]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.0003   0.016534 0.00268 ]\n",
            "Test  score: [0.000207 0.01439  0.002156]\n",
            "RMSE  MAE\n",
            "[[0.0153 0.0021]\n",
            " [0.0156 0.0021]\n",
            " [0.0144 0.0022]]\n",
            "RMSE  MAE\n",
            "[0.0151 0.0021]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Pu97ettf0M"
      },
      "source": [
        "# m1_Col\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m1_Col.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBowoAPctgES",
        "outputId": "6c988d4a-116b-428a-c0f6-7d7af269f1ac"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 7.0  min= 0.0\n",
            "mean= -0.9955078124999998  variance= 0.04561460128187354\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 227.0  min= 0.0\n",
            "mean= -0.9869551899779736  variance= 0.060455517211815774\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00116  0.032071 0.00412 ]\n",
            "Test  score: [0.000781 0.027949 0.003295]\n",
            "RMSE  MAE\n",
            "[[0.0279 0.0033]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0093 0.0011]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001218 0.032836 0.003676]\n",
            "Test  score: [0.000834 0.028878 0.002859]\n",
            "RMSE  MAE\n",
            "[[0.0279 0.0033]\n",
            " [0.0289 0.0029]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0189 0.0021]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001158 0.032074 0.004205]\n",
            "Test  score: [0.000784 0.027996 0.003387]\n",
            "RMSE  MAE\n",
            "[[0.0279 0.0033]\n",
            " [0.0289 0.0029]\n",
            " [0.028  0.0034]]\n",
            "RMSE  MAE\n",
            "[0.0283 0.0032]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYwKj5jjtgR_"
      },
      "source": [
        "# m2_ev\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m2_Ev.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhHEwFastgdY",
        "outputId": "b364e80c-e5bf-457f-82cb-df0567d1b5a5"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 2.0  min= 0.0\n",
            "mean= -0.9995963541666667  variance= 0.020408423180341476\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 227.0  min= 0.0\n",
            "mean= -0.9869551899779736  variance= 0.060455517211815774\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.010143 0.000349]\n",
            "Test  score: [0.000233 0.015249 0.000276]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0051 0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.009975 0.000313]\n",
            "Test  score: [0.000233 0.015248 0.000245]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0002]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0102 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.010118 0.000352]\n",
            "Test  score: [0.000232 0.015246 0.000276]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0002]\n",
            " [0.0152 0.0003]]\n",
            "RMSE  MAE\n",
            "[0.0152 0.0003]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwgQZLttgj7"
      },
      "source": [
        "# m4_Night\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m4_Night.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fUmWkrptgqb",
        "outputId": "d4996877-86ee-4ade-aaae-372170b2954e"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 13.0  min= 0.0\n",
            "mean= -0.9996183894230768  variance= 0.010054515338544382\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 227.0  min= 0.0\n",
            "mean= -0.9869551899779736  variance= 0.060455517211815774\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000115 0.007135 0.000369]\n",
            "Test  score: [0.000039 0.006249 0.000277]\n",
            "RMSE  MAE\n",
            "[[0.0062 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0021 0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000116 0.007145 0.000338]\n",
            "Test  score: [0.000039 0.006276 0.000247]\n",
            "RMSE  MAE\n",
            "[[0.0062 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0042 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000116 0.007155 0.000343]\n",
            "Test  score: [0.00004  0.006293 0.000251]\n",
            "RMSE  MAE\n",
            "[[0.0062 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.0063 0.0003]]\n",
            "RMSE  MAE\n",
            "[0.0063 0.0003]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJBT50u6tgxG"
      },
      "source": [
        "# m5_Outdoor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m5_Outdoor.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDucuOaUtg4C",
        "outputId": "a3ed04da-70da-41ac-a5a6-7221d6b8f935"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 48.0  min= 0.0\n",
            "mean= -0.9933797200520835  variance= 0.03452679972215169\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 227.0  min= 0.0\n",
            "mean= -0.9869551899779736  variance= 0.060455517211815774\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000598 0.022122 0.004739]\n",
            "Test  score: [0.000227 0.015058 0.003086]\n",
            "RMSE  MAE\n",
            "[[0.0151 0.0031]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.005 0.001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000596 0.021974 0.004881]\n",
            "Test  score: [0.000219 0.014795 0.003268]\n",
            "RMSE  MAE\n",
            "[[0.0151 0.0031]\n",
            " [0.0148 0.0033]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.01   0.0021]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000609 0.022197 0.004758]\n",
            "Test  score: [0.000221 0.014872 0.003106]\n",
            "RMSE  MAE\n",
            "[[0.0151 0.0031]\n",
            " [0.0148 0.0033]\n",
            " [0.0149 0.0031]]\n",
            "RMSE  MAE\n",
            "[0.0149 0.0032]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxHVvubStg-d"
      },
      "source": [
        "# m6_Pro\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m6_Pro.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS2cSMb8thFU",
        "outputId": "7a26d25f-ad50-41cf-f256-1cd620e22043"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 68.0  min= 0.0\n",
            "mean= -0.9916000306372551  variance= 0.04181876512912886\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 227.0  min= 0.0\n",
            "mean= -0.9869551899779736  variance= 0.060455517211815774\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000685 0.024352 0.006186]\n",
            "Test  score: [0.000328 0.018116 0.004743]\n",
            "RMSE  MAE\n",
            "[[0.0181 0.0047]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.006  0.0016]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000691 0.024458 0.006058]\n",
            "Test  score: [0.000329 0.018146 0.004581]\n",
            "RMSE  MAE\n",
            "[[0.0181 0.0047]\n",
            " [0.0181 0.0046]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0121 0.0031]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000698 0.024691 0.00616 ]\n",
            "Test  score: [0.000336 0.018333 0.004642]\n",
            "RMSE  MAE\n",
            "[[0.0181 0.0047]\n",
            " [0.0181 0.0046]\n",
            " [0.0183 0.0046]]\n",
            "RMSE  MAE\n",
            "[0.0182 0.0047]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4_IdDnLthH3"
      },
      "source": [
        "# m7_Res\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m7_Res.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2aFCbbgthOl",
        "outputId": "74cea758-aa30-4445-f142-df3e686e3f24"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 4.0  min= 0.0\n",
            "mean= -0.997197265625  variance= 0.03973737211395832\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 227.0  min= 0.0\n",
            "mean= -0.9869551899779736  variance= 0.060455517211815774\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001218 0.033212 0.00229 ]\n",
            "Test  score: [0.000635 0.025206 0.001393]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0014]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0084 0.0005]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00121  0.033099 0.002311]\n",
            "Test  score: [0.000632 0.02513  0.001422]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0014]\n",
            " [0.0251 0.0014]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0168 0.0009]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001216 0.03319  0.002324]\n",
            "Test  score: [0.000637 0.025229 0.001424]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0014]\n",
            " [0.0251 0.0014]\n",
            " [0.0252 0.0014]]\n",
            "RMSE  MAE\n",
            "[0.0252 0.0014]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMzzSXycthU3"
      },
      "source": [
        "# m3_Food\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m3_Food.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k9HNB1-thdM",
        "outputId": "8cb08413-2bb9-4fb4-f20b-c281ec92a95e"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 98.0  min= 0.0\n",
            "mean= -0.9913027476615647  variance= 0.054585593962263146\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 227.0  min= 0.0\n",
            "mean= -0.9869551899779736  variance= 0.060455517211815774\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000731 0.02558  0.005743]\n",
            "Test  score: [0.000367 0.019157 0.004177]\n",
            "RMSE  MAE\n",
            "[[0.0192 0.0042]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0064 0.0014]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000807 0.026835 0.005673]\n",
            "Test  score: [0.0004   0.019994 0.003935]\n",
            "RMSE  MAE\n",
            "[[0.0192 0.0042]\n",
            " [0.02   0.0039]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0131 0.0027]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000641 0.024116 0.005378]\n",
            "Test  score: [0.000341 0.018463 0.003929]\n",
            "RMSE  MAE\n",
            "[[0.0192 0.0042]\n",
            " [0.02   0.0039]\n",
            " [0.0185 0.0039]]\n",
            "RMSE  MAE\n",
            "[0.0192 0.004 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILgnq8L2pvvw"
      },
      "source": [
        "# m9_Tra\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m9_Tra.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOaxNvj2pv6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44e871b-8aaa-409d-fe01-151e571b0fad"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 37.0  min= 0.0\n",
            "mean= -0.9933016610360359  variance= 0.037008620307463456\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 227.0  min= 0.0\n",
            "mean= -0.9869551899779736  variance= 0.060455517211815774\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00081  0.026594 0.004902]\n",
            "Test  score: [0.000413 0.020333 0.003505]\n",
            "RMSE  MAE\n",
            "[[0.0203 0.0035]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0068 0.0012]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000486 0.020937 0.005148]\n",
            "Test  score: [0.00027  0.016442 0.004043]\n",
            "RMSE  MAE\n",
            "[[0.0203 0.0035]\n",
            " [0.0164 0.004 ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0123 0.0025]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000554 0.022077 0.004635]\n",
            "Test  score: [0.000295 0.017167 0.00338 ]\n",
            "RMSE  MAE\n",
            "[[0.0203 0.0035]\n",
            " [0.0164 0.004 ]\n",
            " [0.0172 0.0034]]\n",
            "RMSE  MAE\n",
            "[0.018  0.0036]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aprWnDU5pwF7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5qRsgrtxzg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToZhxhT_tx_s"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69FLVj6KtyJj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBHYYmrtyTE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSrTiF06tycZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMSrD5sAtylz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_92N60ZtyvP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIUSHCm4ty4r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD_MEOuyty71"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4FxssZPtzNr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1x7kHVoG5d"
      },
      "source": [
        "# \"DeepSTN_net.py\"\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import numpy as np\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n",
        "from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "import PPT3_network.metrics as metrics\n",
        "\n",
        "# Relu-BN-Conv2D 3x3\n",
        "def conv_unit0(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    print('kernel=(3,3)')\n",
        "    return unit_model\n",
        "\n",
        "# Relu-BN-Conv2D 1x1\n",
        "def conv_unit1(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    print('kernel=(1,1)')\n",
        "    return unit_model\n",
        "\n",
        "# efficient version of Res_plus\n",
        "def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    #normal channels\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    #separated channels\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n",
        "    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n",
        "    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n",
        "\n",
        "    #merge\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# new resdual block\n",
        "def Res_plus(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n",
        "\n",
        "    cl_conv1B=plus_conv(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n",
        "\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# normal residual block\n",
        "def Res_normal(name,F,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "def cpt_slice(x, h1, h2):\n",
        "    return x[:,h1:h2,:,:]\n",
        "\n",
        "# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)\n",
        "def T_trans(T,T_F,H,W):\n",
        "\n",
        "    T_in=Input(shape=(T+7,H,W))\n",
        "    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n",
        "    T_act=Activation('relu')(T_mid)\n",
        "    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n",
        "    T_mul=Activation('relu')(T_fin)\n",
        "    T_model=Model(inputs=T_in,outputs=T_mul)\n",
        "\n",
        "    return T_model\n",
        "\n",
        "# transfer Time vector and PoI matrix to time-weighted PoI matrix\n",
        "def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n",
        "    if 1:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        if P_N>=2:\n",
        "            T_x0 =T_trans(T,T_F,H,W)(time_in)\n",
        "            T_x1 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=3:\n",
        "            T_x2 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=4:\n",
        "            T_x3 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=5:\n",
        "            T_x4 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=6:\n",
        "            T_x5 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=7:\n",
        "            T_x6 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=8:\n",
        "            T_x7 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=9:\n",
        "            T_x8 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=10:\n",
        "            T_x9 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=11:\n",
        "            T_x10=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=12:\n",
        "            T_x11=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=13:\n",
        "            T_x12=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=14:\n",
        "            T_x13=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=15:\n",
        "            T_x14=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=16:\n",
        "            T_x15=T_trans(T,T_F,H,W)(time_in)\n",
        "\n",
        "\n",
        "        if P_N==1:\n",
        "            T_x=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N==2:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1])\n",
        "        if P_N==3:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n",
        "        if P_N==4:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n",
        "        if P_N==5:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n",
        "        if P_N==6:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n",
        "        if P_N==7:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n",
        "        if P_N==8:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n",
        "        if P_N==9:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n",
        "        if P_N==10:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n",
        "        if P_N==11:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n",
        "        if P_N==12:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n",
        "        if P_N==13:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n",
        "        if P_N==14:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n",
        "        if P_N==15:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n",
        "        if P_N==16:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n",
        "\n",
        "        poi_time=Multiply()([poi_in,T_x])\n",
        "        if isPT_F:\n",
        "            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n",
        "            print('PT_F = YES')\n",
        "        else:\n",
        "            print('PT_F = NO')\n",
        "        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n",
        "\n",
        "        return PT_model\n",
        "\n",
        "# main model\n",
        "def DeepSTN(H=12,W=14,channel=1,      #H-map_height W-map_width channel-map_channel\n",
        "            c=3,p=4,t=4,              #c-closeness p-period t-trend\n",
        "            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n",
        "            is_plus=True,             #use ResPlus or mornal convolution\n",
        "            is_plus_efficient=False,  #use the efficient version of ResPlus\n",
        "            plus=8,rate=2,            #rate-pooling_rate\n",
        "            is_pt=True,               #use PoI and Time or not\n",
        "            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day\n",
        "            drop=0,\n",
        "            is_summary=True, #show detail\n",
        "            lr=0.0002,\n",
        "            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n",
        "            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n",
        "\n",
        "    all_channel = channel * (c+p+t)\n",
        "\n",
        "    cut0 = int( 0 )\n",
        "    cut1 = int( cut0 + channel*c )\n",
        "    cut2 = int( cut1 + channel*p )\n",
        "    cut3 = int( cut2 + channel*t )\n",
        "\n",
        "    cpt_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n",
        "    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n",
        "    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n",
        "\n",
        "    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n",
        "    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n",
        "    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n",
        "\n",
        "    if is_pt:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n",
        "\n",
        "        poi_time=PT_model([poi_in,time_in])\n",
        "\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    else:\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    if is_plus:\n",
        "        if is_plus_efficient:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "        else:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "\n",
        "    else:\n",
        "        for i in range(R_N):\n",
        "            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    cpt_conv2=Activation('relu')(cpt)\n",
        "    cpt_out2=BatchNormalization()(cpt_conv2)\n",
        "    cpt_conv1=Dropout(drop)(cpt_out2)\n",
        "    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n",
        "\n",
        "    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조\n",
        "\n",
        "    cpt_out1=Activation('tanh')(cpt_conv1)\n",
        "\n",
        "    if is_pt:\n",
        "        DeepSTN_model=Model(inputs=[cpt_input,poi_in,time_in],outputs=cpt_out1)\n",
        "    else:\n",
        "        DeepSTN_model=Model(inputs=cpt_input,outputs=cpt_out1)\n",
        "\n",
        "    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[metrics.rmse,metrics.mae])\n",
        "\n",
        "    if is_summary:\n",
        "        DeepSTN_model.summary()\n",
        "\n",
        "\n",
        "    print('***** pre_F : ',pre_F       )\n",
        "    print('***** conv_F: ',conv_F      )\n",
        "    print('***** R_N   : ',R_N         )\n",
        "\n",
        "    print('***** plus  : ',plus*is_plus)\n",
        "    print('***** rate  : ',rate*is_plus)\n",
        "\n",
        "    print('***** P_N   : ',P_N*is_pt   )\n",
        "    print('***** T_F   : ',T_F*is_pt   )\n",
        "    print('***** PT_F  : ',PT_F*is_pt*isPT_F )\n",
        "    print('***** T     : ',T           )\n",
        "\n",
        "    print('***** drop  : ',drop        )\n",
        "\n",
        "    return DeepSTN_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56kVKyKoUI5"
      },
      "source": [
        "# \"metrics.py\"\n",
        "\n",
        "# import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "# aliases\n",
        "mse = MSE = mean_squared_error\n",
        "# rmse = RMSE = root_mean_square_error\n",
        "\n",
        "\n",
        "def masked_mean_squared_error(y_true, y_pred):\n",
        "    idx = (y_true > 1e-6).nonzero()\n",
        "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "threshold=0.05\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-NOpElnMI"
      },
      "source": [
        "#ComparisionBikeNYC.py\n",
        "\n",
        "\n",
        "#from __future__ import print_function\n",
        "#from DATA.lzq_read_data_time_poi import lzq_load_data\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#import cPickle as pickle\n",
        "import numpy as np\n",
        "#import math\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "#from ipdb import set_trace\n",
        "#set_trace()\n",
        "\n",
        "#for GPU in Lab\n",
        "device=6\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(device)\n",
        "import tensorflow as tf  #from V1707\n",
        "config=tf.ConfigProto()  #from V1707\n",
        "config.gpu_options.allow_growth=True  #from V1707\n",
        "#config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
        "sess=tf.Session(config=config)  #from V1707\n",
        "#import keras.backend.tensorflow_backend as KTF\n",
        "#KTF._set_session(tf.Session(config=config))\n",
        "import setproctitle  #from V1707\n",
        "setproctitle.setproctitle('Comprison Start! @ ZiqianLin')  #from V1707\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "\n",
        "#hyperparameters\n",
        "epoch = 1#350  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0002  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 18,15,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 4*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=10\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "#DST result\n",
        "if XDST:\n",
        "    setproctitle.setproctitle('BJMobile DST @ ZiqianLin')  #from V1707\n",
        "\n",
        "    print(\"loading data...\")\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    R_N = 4   # number of residual units\n",
        "\n",
        "    from keras.optimizers import Adam\n",
        "    from DST_network.STResNet import stresnet\n",
        "    import DST_network.metrics as metrics\n",
        "\n",
        "    def build_model(external_dim,CFN):\n",
        "        c_conf = (len_closeness, channel, H, W) if len_closeness > 0 else None\n",
        "        p_conf = (len_period,    channel, H, W) if len_period    > 0 else None\n",
        "        t_conf = (len_trend,     channel, H, W) if len_trend     > 0 else None\n",
        "\n",
        "        model = stresnet(c_conf=c_conf, p_conf=p_conf, t_conf=t_conf,\n",
        "                         external_dim=external_dim, nb_residual_unit=R_N, CF=CFN)\n",
        "\n",
        "        adam = Adam(lr=lr)\n",
        "        model.compile(loss='mse', optimizer=adam, metrics=[metrics.rmse,metrics.mae])\n",
        "        model.summary()\n",
        "        #from keras.utils.visualize_util import plot\n",
        "        #plot(model, to_file='model.png', show_shapes=True)\n",
        "        return model\n",
        "\n",
        "\n",
        "    CF=64\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    import time\n",
        "\n",
        "    count=0\n",
        "\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        time_start=time.time()\n",
        "\n",
        "        F='DST_MODEL/dst_model_'+str(iterate)+'_.hdf5'\n",
        "\n",
        "        model = build_model(external_dim=False,CFN=CF)\n",
        "        if trainDST:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                filepath=F,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "                mode='min',\n",
        "                period=1)\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"training model...\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                nb_epoch=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('evaluating using the model that has the best loss on the valid set')\n",
        "        model.load_weights(F)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        score = model.evaluate(X_test, Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DST_SCORE/dst_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "\n",
        "        print('totally cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+ResPlus+PoI&Time\n",
        "if X11:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+ResPlus+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    index=np.arange(9)\n",
        "    P_train=P_train[:,index,:,:]\n",
        "    P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=True\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=True\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_11/MODEL/DeepSTN_11_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train11:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit([X_train,P_train,T_train], Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate([X_test ,P_test ,T_test ], Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_11/SCORE/DeepSTN_11_score3.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+ResPlus\n",
        "if X10:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+ResPlus @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    #index=np.arange(9)\n",
        "    #P_train=P_train[:,index,:,:]\n",
        "    #P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=True\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=False\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_10/MODEL/DeepSTN_10_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train10:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate(X_test,  Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_10/SCORE/DeepSTN_10_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+PoI&Time\n",
        "if X01:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    #index=np.arange(9)\n",
        "    #P_train=P_train[:,index,:,:]\n",
        "    #P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=False\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=True\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_01/MODEL/DeepSTN_01_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train01:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit([X_train,P_train,T_train], Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate([X_test, P_test, T_test ],  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_01/SCORE/DeepSTN_01_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN\n",
        "if X00:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    index=np.arange(9)\n",
        "    P_train=P_train[:,index,:,:]\n",
        "    P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=False\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=False\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train00:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#Comparison\n",
        "X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "print('MODEL                     RMSE  MAE')\n",
        "if 0:\n",
        "    print('ResNet                  :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DST_SCORE/dst_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    print(for_show)\n",
        "if 0:\n",
        "    print('DeepSTN                 :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_00/SCORE/DeepSTN_00_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 1:\n",
        "    print('DeepSTN+ResPlus         :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_10/SCORE/DeepSTN_10_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 0:\n",
        "    print('DeepSTN+PoI&Time        :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_01/SCORE/DeepSTN_01_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 1:\n",
        "    print('DeepSTN+ResPlus+PoI&Time:',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_11/SCORE/DeepSTN_11_score3.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}