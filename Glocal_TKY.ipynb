{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV_EdNWClAq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aebcdb5-50ce-4d5e-f516-ce012ebaca14"
      },
      "source": [
        "# google drive connect\n",
        "Copied_path = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN' # Paste target directory here\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(Copied_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS2vPiaBYItm"
      },
      "source": [
        "# global\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/global.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AauHopRUnGYv"
      },
      "source": [
        "# m8_shop\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m8_Shop.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgFX7E7fo_a8"
      },
      "source": [
        "# DST Network/ilayer.py\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "# from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class iLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        # self.output_dim = output_dim\n",
        "        super(iLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        initial_weight_value = np.random.random(input_shape[1:])\n",
        "        self.W = K.variable(initial_weight_value)\n",
        "        #self.trainable_weights = [self.W]\n",
        "        self.trainable_weight = [self.W]\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return x * self.W\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k925xkl8Uynj"
      },
      "source": [
        "# \"metrics.py\"\n",
        "\n",
        "# import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "# aliases\n",
        "mse = MSE = mean_squared_error\n",
        "# rmse = RMSE = root_mean_square_error\n",
        "\n",
        "\n",
        "def masked_mean_squared_error(y_true, y_pred):\n",
        "    idx = (y_true > 1e-6).nonzero()\n",
        "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "threshold=0.05\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8M9judzxQIB"
      },
      "source": [
        "# \"DeepSTN_net.py\"\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import numpy as np\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n",
        "from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "#import PPT3_network.metrics as metrics\n",
        "\n",
        "# Relu-BN-Conv2D 3x3\n",
        "def conv_unit0(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    #print('kernel=(3,3)')\n",
        "    return unit_model\n",
        "\n",
        "# Relu-BN-Conv2D 1x1\n",
        "def conv_unit1(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    #print('kernel=(1,1)')\n",
        "    return unit_model\n",
        "\n",
        "# efficient version of Res_plus\n",
        "def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    #normal channels\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    #separated channels\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n",
        "    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n",
        "    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n",
        "\n",
        "    #merge\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# new resdual block\n",
        "def Res_plus(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n",
        "\n",
        "    cl_conv1B=plus_conv(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n",
        "\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# normal residual block\n",
        "def Res_normal(name,F,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "def cpt_slice(x, h1, h2):\n",
        "    return x[:,h1:h2,:,:]\n",
        "\n",
        "# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)\n",
        "def T_trans(T,T_F,H,W):\n",
        "\n",
        "    T_in=Input(shape=(T+7,H,W))\n",
        "    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n",
        "    T_act=Activation('relu')(T_mid)\n",
        "    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n",
        "    T_mul=Activation('relu')(T_fin)\n",
        "    T_model=Model(inputs=T_in,outputs=T_mul)\n",
        "\n",
        "    return T_model\n",
        "\n",
        "# transfer Time vector and PoI matrix to time-weighted PoI matrix\n",
        "def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n",
        "    if 1:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        if P_N>=2:\n",
        "            T_x0 =T_trans(T,T_F,H,W)(time_in)\n",
        "            T_x1 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=3:\n",
        "            T_x2 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=4:\n",
        "            T_x3 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=5:\n",
        "            T_x4 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=6:\n",
        "            T_x5 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=7:\n",
        "            T_x6 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=8:\n",
        "            T_x7 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=9:\n",
        "            T_x8 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=10:\n",
        "            T_x9 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=11:\n",
        "            T_x10=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=12:\n",
        "            T_x11=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=13:\n",
        "            T_x12=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=14:\n",
        "            T_x13=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=15:\n",
        "            T_x14=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=16:\n",
        "            T_x15=T_trans(T,T_F,H,W)(time_in)\n",
        "\n",
        "\n",
        "        if P_N==1:\n",
        "            T_x=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N==2:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1])\n",
        "        if P_N==3:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n",
        "        if P_N==4:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n",
        "        if P_N==5:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n",
        "        if P_N==6:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n",
        "        if P_N==7:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n",
        "        if P_N==8:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n",
        "        if P_N==9:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n",
        "        if P_N==10:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n",
        "        if P_N==11:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n",
        "        if P_N==12:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n",
        "        if P_N==13:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n",
        "        if P_N==14:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n",
        "        if P_N==15:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n",
        "        if P_N==16:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n",
        "\n",
        "        poi_time=Multiply()([poi_in,T_x])\n",
        "        if isPT_F:\n",
        "            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n",
        "            print('PT_F = YES')\n",
        "        else:\n",
        "            print('PT_F = NO')\n",
        "        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n",
        "\n",
        "        return PT_model\n",
        "\n",
        "# main model\n",
        "def DeepSTN(H=14,W=12,channel=1,      #H-map_height W-map_width channel-map_channel\n",
        "            c=3,p=4,t=4,              #c-closeness p-period t-trend\n",
        "            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n",
        "            is_plus=True,             #use ResPlus or mornal convolution\n",
        "            is_plus_efficient=False,  #use the efficient version of ResPlus\n",
        "            plus=8,rate=2,            #rate-pooling_rate\n",
        "            is_pt=False,               #use PoI and Time or not\n",
        "            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day\n",
        "            drop=0,\n",
        "            is_summary=False, #show detail\n",
        "            lr=0.0002,\n",
        "            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n",
        "            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n",
        "\n",
        "    all_channel = channel * (c+p+t)\n",
        "\n",
        "    cut0 = int( 0 )\n",
        "    cut1 = int( cut0 + channel*c )\n",
        "    cut2 = int( cut1 + channel*p )\n",
        "    cut3 = int( cut2 + channel*t )\n",
        "\n",
        "    cpt_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n",
        "    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n",
        "    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n",
        "\n",
        "    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n",
        "    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n",
        "    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n",
        "\n",
        "\n",
        "    cpt_global_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_global_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_global_input)\n",
        "    p_global_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_global_input)\n",
        "    t_global_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_global_input)\n",
        "\n",
        "    c_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_global_input)\n",
        "    p_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_global_input)\n",
        "    t_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_global_input)\n",
        "\n",
        "    if is_pt:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n",
        "\n",
        "        poi_time=PT_model([poi_in,time_in])\n",
        "\n",
        "        cpt_con1=Concatenate(axis=1)([c_global_out1,p_global_out1,t_global_out1,c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    else:\n",
        "        cpt_con1=Concatenate(axis=1)([c_global_out1,p_global_out1,t_global_out1,c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*6,conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*6,conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    if is_plus:\n",
        "        if is_plus_efficient:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "        else:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "\n",
        "    else:\n",
        "        for i in range(R_N):\n",
        "            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    cpt_conv2=Activation('relu')(cpt)\n",
        "    cpt_out2=BatchNormalization()(cpt_conv2)\n",
        "    cpt_conv1=Dropout(drop)(cpt_out2)\n",
        "    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n",
        "\n",
        "    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조\n",
        "\n",
        "    cpt_out1=Activation('tanh')(cpt_conv1)\n",
        "\n",
        "    if is_pt:\n",
        "        DeepSTN_model=Model(inputs=[cpt_global_input, cpt_input,poi_in,time_in],outputs=cpt_out1)\n",
        "    else:\n",
        "        DeepSTN_model=Model(inputs=[cpt_global_input, cpt_input],outputs=cpt_out1) # 여기에 글로벌 인풋도 추가\n",
        "\n",
        "    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[rmse,mae])\n",
        "\n",
        "    if is_summary:\n",
        "        DeepSTN_model.summary()\n",
        "\n",
        "    '''\n",
        "    print('***** pre_F : ',pre_F       )\n",
        "    print('***** conv_F: ',conv_F      )\n",
        "    print('***** R_N   : ',R_N         )\n",
        "\n",
        "    print('***** plus  : ',plus*is_plus)\n",
        "    print('***** rate  : ',rate*is_plus)\n",
        "\n",
        "    print('***** P_N   : ',P_N*is_pt   )\n",
        "    print('***** T_F   : ',T_F*is_pt   )\n",
        "    print('***** PT_F  : ',PT_F*is_pt*isPT_F )\n",
        "    print('***** T     : ',T           )\n",
        "\n",
        "    print('***** drop  : ',drop        )\n",
        "    '''\n",
        "    return DeepSTN_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02uqnK3dUwka",
        "outputId": "6875ffe1-4761-4c8e-991f-c844ee6c40fc"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=3\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "R_N=10\n",
        "\n",
        "is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 146.0  min= 0.0\n",
            "mean= -0.9895630507750447  variance= 0.04576444793426465\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 451.0  min= 0.0\n",
            "mean= -0.980999450611104  variance= 0.06442186147367623\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001032 0.030918 0.008642]\n",
            "Test  score: [0.000606 0.024617 0.006998]\n",
            "RMSE  MAE\n",
            "[[0.0246 0.007 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0082 0.0023]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001061 0.031417 0.008824]\n",
            "Test  score: [0.000643 0.025364 0.00729 ]\n",
            "RMSE  MAE\n",
            "[[0.0246 0.007 ]\n",
            " [0.0254 0.0073]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0167 0.0048]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001136 0.032448 0.009109]\n",
            "Test  score: [0.000695 0.026369 0.007611]\n",
            "RMSE  MAE\n",
            "[[0.0246 0.007 ]\n",
            " [0.0254 0.0073]\n",
            " [0.0264 0.0076]]\n",
            "RMSE  MAE\n",
            "[0.0255 0.0073]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTujgh28pvkw"
      },
      "source": [
        "# m0_Ent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m0_Ent.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaareiVrtfkE",
        "outputId": "ef5bdb7b-88ec-4193-8d32-c343d7ceecef"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 33.0  min= 0.0\n",
            "mean= -0.9878032883051704  variance= 0.06483403736343217\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 451.0  min= 0.0\n",
            "mean= -0.980999450611104  variance= 0.06442186147367623\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002299 0.045962 0.011704]\n",
            "Test  score: [0.001484 0.038521 0.01001 ]\n",
            "RMSE  MAE\n",
            "[[0.0385 0.01  ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0128 0.0033]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002432 0.04688  0.010801]\n",
            "Test  score: [0.001507 0.038819 0.008891]\n",
            "RMSE  MAE\n",
            "[[0.0385 0.01  ]\n",
            " [0.0388 0.0089]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0258 0.0063]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002242 0.045378 0.011109]\n",
            "Test  score: [0.001433 0.037853 0.009257]\n",
            "RMSE  MAE\n",
            "[[0.0385 0.01  ]\n",
            " [0.0388 0.0089]\n",
            " [0.0379 0.0093]]\n",
            "RMSE  MAE\n",
            "[0.0384 0.0094]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Pu97ettf0M"
      },
      "source": [
        "# m1_Col\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m1_Col.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBowoAPctgES",
        "outputId": "01d19846-7407-40bc-dcee-a9c32660f5cf"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 13.0  min= 0.0\n",
            "mean= -0.9942343311226624  variance= 0.04226600741012836\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 451.0  min= 0.0\n",
            "mean= -0.980999450611104  variance= 0.06442186147367623\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.0013   0.033676 0.006281]\n",
            "Test  score: [0.000726 0.02694  0.005014]\n",
            "RMSE  MAE\n",
            "[[0.0269 0.005 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.009  0.0017]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001259 0.033395 0.00648 ]\n",
            "Test  score: [0.000728 0.026986 0.005301]\n",
            "RMSE  MAE\n",
            "[[0.0269 0.005 ]\n",
            " [0.027  0.0053]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.018  0.0034]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001257 0.033341 0.006046]\n",
            "Test  score: [0.000723 0.026892 0.00481 ]\n",
            "RMSE  MAE\n",
            "[[0.0269 0.005 ]\n",
            " [0.027  0.0053]\n",
            " [0.0269 0.0048]]\n",
            "RMSE  MAE\n",
            "[0.0269 0.005 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYwKj5jjtgR_"
      },
      "source": [
        "# m2_ev\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m2_Ev.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhHEwFastgdY",
        "outputId": "c6822922-9f26-4a66-bff2-7d789c7f0115"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 1.0  min= 0.0\n",
            "mean= -0.9998356933739618  variance= 0.018126948320356582\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 451.0  min= 0.0\n",
            "mean= -0.980999450611104  variance= 0.06442186147367623\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000413 0.007972 0.000234]\n",
            "Test  score: [0.000212 0.014577 0.000136]\n",
            "RMSE  MAE\n",
            "[[0.0146 0.0001]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0049 0.    ]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000413 0.007959 0.000238]\n",
            "Test  score: [0.000213 0.014579 0.00014 ]\n",
            "RMSE  MAE\n",
            "[[0.0146 0.0001]\n",
            " [0.0146 0.0001]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0097 0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000413 0.00793  0.000217]\n",
            "Test  score: [0.000213 0.01458  0.000119]\n",
            "RMSE  MAE\n",
            "[[0.0146 0.0001]\n",
            " [0.0146 0.0001]\n",
            " [0.0146 0.0001]]\n",
            "RMSE  MAE\n",
            "[0.0146 0.0001]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwgQZLttgj7"
      },
      "source": [
        "# m4_Night\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m4_Night.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fUmWkrptgqb",
        "outputId": "e73358cf-e0a1-4e00-8ea7-ec896388af76"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 3.0  min= 0.0\n",
            "mean= -0.9968283842185973  variance= 0.048270752295892394\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 451.0  min= 0.0\n",
            "mean= -0.980999450611104  variance= 0.06442186147367623\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002188 0.045045 0.003134]\n",
            "Test  score: [0.00167  0.040867 0.002423]\n",
            "RMSE  MAE\n",
            "[[0.0409 0.0024]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0136 0.0008]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002185 0.045021 0.003201]\n",
            "Test  score: [0.001674 0.040912 0.002489]\n",
            "RMSE  MAE\n",
            "[[0.0409 0.0024]\n",
            " [0.0409 0.0025]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0273 0.0016]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002148 0.044699 0.003525]\n",
            "Test  score: [0.001667 0.040827 0.002837]\n",
            "RMSE  MAE\n",
            "[[0.0409 0.0024]\n",
            " [0.0409 0.0025]\n",
            " [0.0408 0.0028]]\n",
            "RMSE  MAE\n",
            "[0.0409 0.0026]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJBT50u6tgxG"
      },
      "source": [
        "# m5_Outdoor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m5_Outdoor.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDucuOaUtg4C",
        "outputId": "8b4e707c-2c7a-49de-da86-4785a0c952f5"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 130.0  min= 0.0\n",
            "mean= -0.9962962069298331  variance= 0.0168414251671113\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 451.0  min= 0.0\n",
            "mean= -0.980999450611104  variance= 0.06442186147367623\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00029  0.01368  0.003247]\n",
            "Test  score: [0.000103 0.010163 0.002581]\n",
            "RMSE  MAE\n",
            "[[0.0102 0.0026]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0034 0.0009]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000286 0.013371 0.00317 ]\n",
            "Test  score: [0.000096 0.009776 0.002506]\n",
            "RMSE  MAE\n",
            "[[0.0102 0.0026]\n",
            " [0.0098 0.0025]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0066 0.0017]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000307 0.014226 0.003257]\n",
            "Test  score: [0.000113 0.010639 0.002562]\n",
            "RMSE  MAE\n",
            "[[0.0102 0.0026]\n",
            " [0.0098 0.0025]\n",
            " [0.0106 0.0026]]\n",
            "RMSE  MAE\n",
            "[0.0102 0.0025]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxHVvubStg-d"
      },
      "source": [
        "# m6_Pro\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m6_Pro.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS2cSMb8thFU",
        "outputId": "ffdc40c5-903f-41b6-ec41-083ff9710af6"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 35.0  min= 0.0\n",
            "mean= -0.985686545634565  variance= 0.052230389053797795\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 451.0  min= 0.0\n",
            "mean= -0.980999450611104  variance= 0.06442186147367623\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001655 0.039847 0.012733]\n",
            "Test  score: [0.001184 0.034409 0.011075]\n",
            "RMSE  MAE\n",
            "[[0.0344 0.0111]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0115 0.0037]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001582 0.039034 0.013081]\n",
            "Test  score: [0.001161 0.034077 0.011577]\n",
            "RMSE  MAE\n",
            "[[0.0344 0.0111]\n",
            " [0.0341 0.0116]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0228 0.0076]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001604 0.039246 0.013004]\n",
            "Test  score: [0.001153 0.033951 0.011432]\n",
            "RMSE  MAE\n",
            "[[0.0344 0.0111]\n",
            " [0.0341 0.0116]\n",
            " [0.034  0.0114]]\n",
            "RMSE  MAE\n",
            "[0.0341 0.0114]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4_IdDnLthH3"
      },
      "source": [
        "# m7_Res\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m7_Res.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2aFCbbgthOl",
        "outputId": "7093f59b-bd64-4ec0-f1e3-2dca18a62349"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 4.0  min= 0.0\n",
            "mean= -0.9962097448766206  variance= 0.04684482697013769\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 451.0  min= 0.0\n",
            "mean= -0.980999450611104  variance= 0.06442186147367623\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001908 0.042206 0.003649]\n",
            "Test  score: [0.001755 0.041893 0.003243]\n",
            "RMSE  MAE\n",
            "[[0.0419 0.0032]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.014  0.0011]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001917 0.042302 0.003595]\n",
            "Test  score: [0.001765 0.042009 0.003194]\n",
            "RMSE  MAE\n",
            "[[0.0419 0.0032]\n",
            " [0.042  0.0032]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.028  0.0021]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00191  0.042229 0.003711]\n",
            "Test  score: [0.001761 0.04196  0.00331 ]\n",
            "RMSE  MAE\n",
            "[[0.0419 0.0032]\n",
            " [0.042  0.0032]\n",
            " [0.042  0.0033]]\n",
            "RMSE  MAE\n",
            "[0.042  0.0032]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMzzSXycthU3"
      },
      "source": [
        "# m3_Food\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m3_Food.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k9HNB1-thdM",
        "outputId": "dd1aee3b-4335-4d4e-d4b6-927fcdb76562"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 63.0  min= 0.0\n",
            "mean= -0.9811303442330509  variance= 0.07453364279577186\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 451.0  min= 0.0\n",
            "mean= -0.980999450611104  variance= 0.06442186147367623\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002019 0.043665 0.01407 ]\n",
            "Test  score: [0.001394 0.037335 0.011959]\n",
            "RMSE  MAE\n",
            "[[0.0373 0.012 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0124 0.004 ]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002184 0.045571 0.014382]\n",
            "Test  score: [0.001556 0.039442 0.012304]\n",
            "RMSE  MAE\n",
            "[[0.0373 0.012 ]\n",
            " [0.0394 0.0123]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0256 0.0081]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002088 0.044563 0.014441]\n",
            "Test  score: [0.001469 0.038325 0.0124  ]\n",
            "RMSE  MAE\n",
            "[[0.0373 0.012 ]\n",
            " [0.0394 0.0123]\n",
            " [0.0383 0.0124]]\n",
            "RMSE  MAE\n",
            "[0.0384 0.0122]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILgnq8L2pvvw"
      },
      "source": [
        "# m9_Tra\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m9_Tra.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOaxNvj2pv6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5954d4a2-f043-4bc8-a098-e070a9985622"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 132.0  min= 0.0\n",
            "mean= -0.9752139154585828  variance= 0.08477867360777425\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 451.0  min= 0.0\n",
            "mean= -0.980999450611104  variance= 0.06442186147367623\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002367 0.047492 0.016373]\n",
            "Test  score: [0.001752 0.041857 0.014446]\n",
            "RMSE  MAE\n",
            "[[0.0419 0.0144]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.014  0.0048]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002288 0.046769 0.016453]\n",
            "Test  score: [0.001688 0.041089 0.014441]\n",
            "RMSE  MAE\n",
            "[[0.0419 0.0144]\n",
            " [0.0411 0.0144]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0276 0.0096]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002459 0.048481 0.016694]\n",
            "Test  score: [0.001814 0.042596 0.014625]\n",
            "RMSE  MAE\n",
            "[[0.0419 0.0144]\n",
            " [0.0411 0.0144]\n",
            " [0.0426 0.0146]]\n",
            "RMSE  MAE\n",
            "[0.0418 0.0145]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aprWnDU5pwF7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5qRsgrtxzg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToZhxhT_tx_s"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69FLVj6KtyJj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBHYYmrtyTE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSrTiF06tycZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMSrD5sAtylz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_92N60ZtyvP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIUSHCm4ty4r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD_MEOuyty71"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4FxssZPtzNr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1x7kHVoG5d"
      },
      "source": [
        "# \"DeepSTN_net.py\"\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import numpy as np\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n",
        "from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "import PPT3_network.metrics as metrics\n",
        "\n",
        "# Relu-BN-Conv2D 3x3\n",
        "def conv_unit0(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    print('kernel=(3,3)')\n",
        "    return unit_model\n",
        "\n",
        "# Relu-BN-Conv2D 1x1\n",
        "def conv_unit1(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    print('kernel=(1,1)')\n",
        "    return unit_model\n",
        "\n",
        "# efficient version of Res_plus\n",
        "def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    #normal channels\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    #separated channels\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n",
        "    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n",
        "    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n",
        "\n",
        "    #merge\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# new resdual block\n",
        "def Res_plus(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n",
        "\n",
        "    cl_conv1B=plus_conv(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n",
        "\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# normal residual block\n",
        "def Res_normal(name,F,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "def cpt_slice(x, h1, h2):\n",
        "    return x[:,h1:h2,:,:]\n",
        "\n",
        "# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)\n",
        "def T_trans(T,T_F,H,W):\n",
        "\n",
        "    T_in=Input(shape=(T+7,H,W))\n",
        "    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n",
        "    T_act=Activation('relu')(T_mid)\n",
        "    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n",
        "    T_mul=Activation('relu')(T_fin)\n",
        "    T_model=Model(inputs=T_in,outputs=T_mul)\n",
        "\n",
        "    return T_model\n",
        "\n",
        "# transfer Time vector and PoI matrix to time-weighted PoI matrix\n",
        "def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n",
        "    if 1:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        if P_N>=2:\n",
        "            T_x0 =T_trans(T,T_F,H,W)(time_in)\n",
        "            T_x1 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=3:\n",
        "            T_x2 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=4:\n",
        "            T_x3 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=5:\n",
        "            T_x4 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=6:\n",
        "            T_x5 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=7:\n",
        "            T_x6 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=8:\n",
        "            T_x7 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=9:\n",
        "            T_x8 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=10:\n",
        "            T_x9 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=11:\n",
        "            T_x10=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=12:\n",
        "            T_x11=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=13:\n",
        "            T_x12=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=14:\n",
        "            T_x13=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=15:\n",
        "            T_x14=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=16:\n",
        "            T_x15=T_trans(T,T_F,H,W)(time_in)\n",
        "\n",
        "\n",
        "        if P_N==1:\n",
        "            T_x=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N==2:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1])\n",
        "        if P_N==3:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n",
        "        if P_N==4:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n",
        "        if P_N==5:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n",
        "        if P_N==6:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n",
        "        if P_N==7:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n",
        "        if P_N==8:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n",
        "        if P_N==9:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n",
        "        if P_N==10:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n",
        "        if P_N==11:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n",
        "        if P_N==12:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n",
        "        if P_N==13:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n",
        "        if P_N==14:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n",
        "        if P_N==15:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n",
        "        if P_N==16:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n",
        "\n",
        "        poi_time=Multiply()([poi_in,T_x])\n",
        "        if isPT_F:\n",
        "            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n",
        "            print('PT_F = YES')\n",
        "        else:\n",
        "            print('PT_F = NO')\n",
        "        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n",
        "\n",
        "        return PT_model\n",
        "\n",
        "# main model\n",
        "def DeepSTN(H=12,W=14,channel=1,      #H-map_height W-map_width channel-map_channel\n",
        "            c=3,p=4,t=4,              #c-closeness p-period t-trend\n",
        "            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n",
        "            is_plus=True,             #use ResPlus or mornal convolution\n",
        "            is_plus_efficient=False,  #use the efficient version of ResPlus\n",
        "            plus=8,rate=2,            #rate-pooling_rate\n",
        "            is_pt=True,               #use PoI and Time or not\n",
        "            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day\n",
        "            drop=0,\n",
        "            is_summary=True, #show detail\n",
        "            lr=0.0002,\n",
        "            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n",
        "            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n",
        "\n",
        "    all_channel = channel * (c+p+t)\n",
        "\n",
        "    cut0 = int( 0 )\n",
        "    cut1 = int( cut0 + channel*c )\n",
        "    cut2 = int( cut1 + channel*p )\n",
        "    cut3 = int( cut2 + channel*t )\n",
        "\n",
        "    cpt_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n",
        "    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n",
        "    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n",
        "\n",
        "    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n",
        "    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n",
        "    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n",
        "\n",
        "    if is_pt:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n",
        "\n",
        "        poi_time=PT_model([poi_in,time_in])\n",
        "\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    else:\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    if is_plus:\n",
        "        if is_plus_efficient:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "        else:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "\n",
        "    else:\n",
        "        for i in range(R_N):\n",
        "            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    cpt_conv2=Activation('relu')(cpt)\n",
        "    cpt_out2=BatchNormalization()(cpt_conv2)\n",
        "    cpt_conv1=Dropout(drop)(cpt_out2)\n",
        "    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n",
        "\n",
        "    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조\n",
        "\n",
        "    cpt_out1=Activation('tanh')(cpt_conv1)\n",
        "\n",
        "    if is_pt:\n",
        "        DeepSTN_model=Model(inputs=[cpt_input,poi_in,time_in],outputs=cpt_out1)\n",
        "    else:\n",
        "        DeepSTN_model=Model(inputs=cpt_input,outputs=cpt_out1)\n",
        "\n",
        "    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[metrics.rmse,metrics.mae])\n",
        "\n",
        "    if is_summary:\n",
        "        DeepSTN_model.summary()\n",
        "\n",
        "\n",
        "    print('***** pre_F : ',pre_F       )\n",
        "    print('***** conv_F: ',conv_F      )\n",
        "    print('***** R_N   : ',R_N         )\n",
        "\n",
        "    print('***** plus  : ',plus*is_plus)\n",
        "    print('***** rate  : ',rate*is_plus)\n",
        "\n",
        "    print('***** P_N   : ',P_N*is_pt   )\n",
        "    print('***** T_F   : ',T_F*is_pt   )\n",
        "    print('***** PT_F  : ',PT_F*is_pt*isPT_F )\n",
        "    print('***** T     : ',T           )\n",
        "\n",
        "    print('***** drop  : ',drop        )\n",
        "\n",
        "    return DeepSTN_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56kVKyKoUI5"
      },
      "source": [
        "# \"metrics.py\"\n",
        "\n",
        "# import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "# aliases\n",
        "mse = MSE = mean_squared_error\n",
        "# rmse = RMSE = root_mean_square_error\n",
        "\n",
        "\n",
        "def masked_mean_squared_error(y_true, y_pred):\n",
        "    idx = (y_true > 1e-6).nonzero()\n",
        "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "threshold=0.05\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-NOpElnMI"
      },
      "source": [
        "#ComparisionBikeNYC.py\n",
        "\n",
        "\n",
        "#from __future__ import print_function\n",
        "#from DATA.lzq_read_data_time_poi import lzq_load_data\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#import cPickle as pickle\n",
        "import numpy as np\n",
        "#import math\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "#from ipdb import set_trace\n",
        "#set_trace()\n",
        "\n",
        "#for GPU in Lab\n",
        "device=6\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(device)\n",
        "import tensorflow as tf  #from V1707\n",
        "config=tf.ConfigProto()  #from V1707\n",
        "config.gpu_options.allow_growth=True  #from V1707\n",
        "#config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
        "sess=tf.Session(config=config)  #from V1707\n",
        "#import keras.backend.tensorflow_backend as KTF\n",
        "#KTF._set_session(tf.Session(config=config))\n",
        "import setproctitle  #from V1707\n",
        "setproctitle.setproctitle('Comprison Start! @ ZiqianLin')  #from V1707\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "\n",
        "#hyperparameters\n",
        "epoch = 1#350  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0002  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 18,15,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 4*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=10\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "#DST result\n",
        "if XDST:\n",
        "    setproctitle.setproctitle('BJMobile DST @ ZiqianLin')  #from V1707\n",
        "\n",
        "    print(\"loading data...\")\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    R_N = 4   # number of residual units\n",
        "\n",
        "    from keras.optimizers import Adam\n",
        "    from DST_network.STResNet import stresnet\n",
        "    import DST_network.metrics as metrics\n",
        "\n",
        "    def build_model(external_dim,CFN):\n",
        "        c_conf = (len_closeness, channel, H, W) if len_closeness > 0 else None\n",
        "        p_conf = (len_period,    channel, H, W) if len_period    > 0 else None\n",
        "        t_conf = (len_trend,     channel, H, W) if len_trend     > 0 else None\n",
        "\n",
        "        model = stresnet(c_conf=c_conf, p_conf=p_conf, t_conf=t_conf,\n",
        "                         external_dim=external_dim, nb_residual_unit=R_N, CF=CFN)\n",
        "\n",
        "        adam = Adam(lr=lr)\n",
        "        model.compile(loss='mse', optimizer=adam, metrics=[metrics.rmse,metrics.mae])\n",
        "        model.summary()\n",
        "        #from keras.utils.visualize_util import plot\n",
        "        #plot(model, to_file='model.png', show_shapes=True)\n",
        "        return model\n",
        "\n",
        "\n",
        "    CF=64\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    import time\n",
        "\n",
        "    count=0\n",
        "\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        time_start=time.time()\n",
        "\n",
        "        F='DST_MODEL/dst_model_'+str(iterate)+'_.hdf5'\n",
        "\n",
        "        model = build_model(external_dim=False,CFN=CF)\n",
        "        if trainDST:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                filepath=F,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "                mode='min',\n",
        "                period=1)\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"training model...\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                nb_epoch=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('evaluating using the model that has the best loss on the valid set')\n",
        "        model.load_weights(F)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        score = model.evaluate(X_test, Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DST_SCORE/dst_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "\n",
        "        print('totally cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+ResPlus+PoI&Time\n",
        "if X11:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+ResPlus+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    index=np.arange(9)\n",
        "    P_train=P_train[:,index,:,:]\n",
        "    P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=True\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=True\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_11/MODEL/DeepSTN_11_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train11:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit([X_train,P_train,T_train], Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate([X_test ,P_test ,T_test ], Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_11/SCORE/DeepSTN_11_score3.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+ResPlus\n",
        "if X10:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+ResPlus @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    #index=np.arange(9)\n",
        "    #P_train=P_train[:,index,:,:]\n",
        "    #P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=True\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=False\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_10/MODEL/DeepSTN_10_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train10:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate(X_test,  Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_10/SCORE/DeepSTN_10_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+PoI&Time\n",
        "if X01:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    #index=np.arange(9)\n",
        "    #P_train=P_train[:,index,:,:]\n",
        "    #P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=False\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=True\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_01/MODEL/DeepSTN_01_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train01:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit([X_train,P_train,T_train], Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate([X_test, P_test, T_test ],  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_01/SCORE/DeepSTN_01_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN\n",
        "if X00:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    index=np.arange(9)\n",
        "    P_train=P_train[:,index,:,:]\n",
        "    P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=False\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=False\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train00:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#Comparison\n",
        "X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "print('MODEL                     RMSE  MAE')\n",
        "if 0:\n",
        "    print('ResNet                  :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DST_SCORE/dst_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    print(for_show)\n",
        "if 0:\n",
        "    print('DeepSTN                 :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_00/SCORE/DeepSTN_00_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 1:\n",
        "    print('DeepSTN+ResPlus         :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_10/SCORE/DeepSTN_10_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 0:\n",
        "    print('DeepSTN+PoI&Time        :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_01/SCORE/DeepSTN_01_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 1:\n",
        "    print('DeepSTN+ResPlus+PoI&Time:',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_11/SCORE/DeepSTN_11_score3.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}