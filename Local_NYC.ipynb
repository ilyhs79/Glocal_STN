{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV_EdNWClAq9",
        "outputId": "349cfe55-bb92-4dd2-fc59-08a0a8c796d0"
      },
      "source": [
        "# google drive connect\n",
        "Copied_path = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN' # Paste target directory here\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(Copied_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AauHopRUnGYv"
      },
      "source": [
        "# m8_shop\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m8_Shop.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgFX7E7fo_a8"
      },
      "source": [
        "# DST Network/ilayer.py\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "# from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class iLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        # self.output_dim = output_dim\n",
        "        super(iLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        initial_weight_value = np.random.random(input_shape[1:])\n",
        "        self.W = K.variable(initial_weight_value)\n",
        "        #self.trainable_weights = [self.W]\n",
        "        self.trainable_weight = [self.W]\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return x * self.W\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k925xkl8Uynj"
      },
      "source": [
        "# \"metrics.py\"\n",
        "\n",
        "# import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "# aliases\n",
        "mse = MSE = mean_squared_error\n",
        "# rmse = RMSE = root_mean_square_error\n",
        "\n",
        "\n",
        "def masked_mean_squared_error(y_true, y_pred):\n",
        "    idx = (y_true > 1e-6).nonzero()\n",
        "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "threshold=0.05\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8M9judzxQIB"
      },
      "source": [
        "# \"DeepSTN_net.py\"\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import numpy as np\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n",
        "from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "#import PPT3_network.metrics as metrics\n",
        "\n",
        "# Relu-BN-Conv2D 3x3\n",
        "def conv_unit0(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    #print('kernel=(3,3)')\n",
        "    return unit_model\n",
        "\n",
        "# Relu-BN-Conv2D 1x1\n",
        "def conv_unit1(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    #print('kernel=(1,1)')\n",
        "    return unit_model\n",
        "\n",
        "# efficient version of Res_plus\n",
        "def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    #normal channels\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    #separated channels\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n",
        "    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n",
        "    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n",
        "\n",
        "    #merge\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# new resdual block\n",
        "def Res_plus(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n",
        "\n",
        "    cl_conv1B=plus_conv(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n",
        "\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# normal residual block\n",
        "def Res_normal(name,F,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "def cpt_slice(x, h1, h2):\n",
        "    return x[:,h1:h2,:,:]\n",
        "\n",
        "# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)\n",
        "def T_trans(T,T_F,H,W):\n",
        "\n",
        "    T_in=Input(shape=(T+7,H,W))\n",
        "    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n",
        "    T_act=Activation('relu')(T_mid)\n",
        "    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n",
        "    T_mul=Activation('relu')(T_fin)\n",
        "    T_model=Model(inputs=T_in,outputs=T_mul)\n",
        "\n",
        "    return T_model\n",
        "\n",
        "# transfer Time vector and PoI matrix to time-weighted PoI matrix\n",
        "def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n",
        "    if 1:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        if P_N>=2:\n",
        "            T_x0 =T_trans(T,T_F,H,W)(time_in)\n",
        "            T_x1 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=3:\n",
        "            T_x2 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=4:\n",
        "            T_x3 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=5:\n",
        "            T_x4 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=6:\n",
        "            T_x5 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=7:\n",
        "            T_x6 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=8:\n",
        "            T_x7 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=9:\n",
        "            T_x8 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=10:\n",
        "            T_x9 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=11:\n",
        "            T_x10=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=12:\n",
        "            T_x11=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=13:\n",
        "            T_x12=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=14:\n",
        "            T_x13=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=15:\n",
        "            T_x14=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=16:\n",
        "            T_x15=T_trans(T,T_F,H,W)(time_in)\n",
        "\n",
        "\n",
        "        if P_N==1:\n",
        "            T_x=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N==2:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1])\n",
        "        if P_N==3:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n",
        "        if P_N==4:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n",
        "        if P_N==5:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n",
        "        if P_N==6:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n",
        "        if P_N==7:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n",
        "        if P_N==8:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n",
        "        if P_N==9:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n",
        "        if P_N==10:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n",
        "        if P_N==11:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n",
        "        if P_N==12:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n",
        "        if P_N==13:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n",
        "        if P_N==14:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n",
        "        if P_N==15:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n",
        "        if P_N==16:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n",
        "\n",
        "        poi_time=Multiply()([poi_in,T_x])\n",
        "        if isPT_F:\n",
        "            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n",
        "            print('PT_F = YES')\n",
        "        else:\n",
        "            print('PT_F = NO')\n",
        "        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n",
        "\n",
        "        return PT_model\n",
        "\n",
        "# main model\n",
        "def DeepSTN(H=14,W=12,channel=1,      #H-map_height W-map_width channel-map_channel\n",
        "            c=3,p=4,t=4,              #c-closeness p-period t-trend\n",
        "            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n",
        "            is_plus=True,             #use ResPlus or mornal convolution\n",
        "            is_plus_efficient=False,  #use the efficient version of ResPlus\n",
        "            plus=8,rate=2,            #rate-pooling_rate\n",
        "            is_pt=False,               #use PoI and Time or not\n",
        "            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day\n",
        "            drop=0,\n",
        "            is_summary=False, #show detail\n",
        "            lr=0.0001,\n",
        "            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n",
        "            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n",
        "\n",
        "    all_channel = channel * (c+p+t)\n",
        "\n",
        "    cut0 = int( 0 )\n",
        "    cut1 = int( cut0 + channel*c )\n",
        "    cut2 = int( cut1 + channel*p )\n",
        "    cut3 = int( cut2 + channel*t )\n",
        "\n",
        "    cpt_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n",
        "    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n",
        "    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n",
        "\n",
        "    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n",
        "    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n",
        "    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n",
        "\n",
        "    if is_pt:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n",
        "\n",
        "        poi_time=PT_model([poi_in,time_in])\n",
        "\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    else:\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    if is_plus:\n",
        "        if is_plus_efficient:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "        else:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    else:\n",
        "        for i in range(R_N):\n",
        "            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    cpt_conv2=Activation('relu')(cpt)\n",
        "    cpt_out2=BatchNormalization()(cpt_conv2)\n",
        "    cpt_conv1=Dropout(drop)(cpt_out2)\n",
        "    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n",
        "    print('cpt_conv1 shape is', cpt_conv1.shape)\n",
        "    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조\n",
        "\n",
        "    cpt_out1=Activation('tanh')(cpt_conv1)\n",
        "\n",
        "    if is_pt:\n",
        "        DeepSTN_model=Model(inputs=[cpt_input,poi_in,time_in],outputs=cpt_out1)\n",
        "    else:\n",
        "        DeepSTN_model=Model(inputs=cpt_input,outputs=cpt_out1)\n",
        "\n",
        "    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[rmse,mae])\n",
        "\n",
        "    if is_summary:\n",
        "        DeepSTN_model.summary()\n",
        "\n",
        "    '''\n",
        "    print('***** pre_F : ',pre_F       )\n",
        "    print('***** conv_F: ',conv_F      )\n",
        "    print('***** R_N   : ',R_N         )\n",
        "\n",
        "    print('***** plus  : ',plus*is_plus)\n",
        "    print('***** rate  : ',rate*is_plus)\n",
        "\n",
        "    print('***** P_N   : ',P_N*is_pt   )\n",
        "    print('***** T_F   : ',T_F*is_pt   )\n",
        "    print('***** PT_F  : ',PT_F*is_pt*isPT_F )\n",
        "    print('***** T     : ',T           )\n",
        "\n",
        "    print('***** drop  : ',drop        )\n",
        "    '''\n",
        "    return DeepSTN_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02uqnK3dUwka",
        "outputId": "ce661514-8144-402d-cb55-bd2a05120cc9"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 16,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=3\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "R_N=10\n",
        "\n",
        "is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''\n",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 60.0  min= 0.0\n",
            "mean= -0.9908684895833333  variance= 0.04447277708632065\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000667 0.025101 0.006785]\n",
            "Test  score: [0.00036  0.018962 0.005078]\n",
            "RMSE  MAE\n",
            "[[0.019  0.0051]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0063 0.0017]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000704 0.025801 0.006653]\n",
            "Test  score: [0.000372 0.019288 0.004806]\n",
            "RMSE  MAE\n",
            "[[0.019  0.0051]\n",
            " [0.0193 0.0048]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0127 0.0033]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000699 0.02568  0.006696]\n",
            "Test  score: [0.000369 0.019196 0.004858]\n",
            "RMSE  MAE\n",
            "[[0.019  0.0051]\n",
            " [0.0193 0.0048]\n",
            " [0.0192 0.0049]]\n",
            "RMSE  MAE\n",
            "[0.0191 0.0049]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTujgh28pvkw"
      },
      "source": [
        "# m0_Ent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m0_Ent.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaareiVrtfkE",
        "outputId": "d463d45a-562e-4ee2-d348-95f0492a9b3a"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 49.0  min= 0.0\n",
            "mean= -0.9962797619047619  variance= 0.02770920949471694\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000296 0.016416 0.002599]\n",
            "Test  score: [0.000202 0.014212 0.002054]\n",
            "RMSE  MAE\n",
            "[[0.0142 0.0021]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0047 0.0007]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000301 0.016542 0.002581]\n",
            "Test  score: [0.000206 0.014336 0.00203 ]\n",
            "RMSE  MAE\n",
            "[[0.0142 0.0021]\n",
            " [0.0143 0.002 ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0095 0.0014]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000363 0.018156 0.002777]\n",
            "Test  score: [0.000241 0.015509 0.002153]\n",
            "RMSE  MAE\n",
            "[[0.0142 0.0021]\n",
            " [0.0143 0.002 ]\n",
            " [0.0155 0.0022]]\n",
            "RMSE  MAE\n",
            "[0.0147 0.0021]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Pu97ettf0M"
      },
      "source": [
        "# m1_Col\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m1_Col.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBowoAPctgES",
        "outputId": "71a8daa4-c416-4853-a9dd-223cc0bedaa3"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 7.0  min= 0.0\n",
            "mean= -0.9955078124999998  variance= 0.04561460128187354\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001172 0.032246 0.003828]\n",
            "Test  score: [0.000788 0.028068 0.002995]\n",
            "RMSE  MAE\n",
            "[[0.0281 0.003 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0094 0.001 ]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001202 0.032603 0.003604]\n",
            "Test  score: [0.000819 0.028612 0.002769]\n",
            "RMSE  MAE\n",
            "[[0.0281 0.003 ]\n",
            " [0.0286 0.0028]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0189 0.0019]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001172 0.032255 0.003812]\n",
            "Test  score: [0.000797 0.028239 0.003005]\n",
            "RMSE  MAE\n",
            "[[0.0281 0.003 ]\n",
            " [0.0286 0.0028]\n",
            " [0.0282 0.003 ]]\n",
            "RMSE  MAE\n",
            "[0.0283 0.0029]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYwKj5jjtgR_"
      },
      "source": [
        "# m2_mv\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m2_Ev.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhHEwFastgdY",
        "outputId": "506ebea1-6ddc-4071-c664-e14d5c1cfd08"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 2.0  min= 0.0\n",
            "mean= -0.9995963541666667  variance= 0.020408423180341476\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.010018 0.000357]\n",
            "Test  score: [0.000232 0.015246 0.000289]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0051 0.0001]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.009992 0.000328]\n",
            "Test  score: [0.000233 0.015248 0.00026 ]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0102 0.0002]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.010064 0.000343]\n",
            "Test  score: [0.000232 0.015243 0.000274]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]]\n",
            "RMSE  MAE\n",
            "[0.0152 0.0003]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwgQZLttgj7"
      },
      "source": [
        "# m4_Night\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m4_Night.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fUmWkrptgqb",
        "outputId": "3ba10b64-3928-401f-dd6e-488f9d5e9b28"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 13.0  min= 0.0\n",
            "mean= -0.9996183894230768  variance= 0.010054515338544382\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000116 0.007154 0.000336]\n",
            "Test  score: [0.00004  0.006294 0.000244]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0002]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0021 0.0001]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000115 0.007132 0.000397]\n",
            "Test  score: [0.000039 0.006254 0.000307]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0002]\n",
            " [0.0063 0.0003]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0042 0.0002]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000115 0.007135 0.000363]\n",
            "Test  score: [0.000039 0.006253 0.000272]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0002]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]]\n",
            "RMSE  MAE\n",
            "[0.0063 0.0003]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJBT50u6tgxG"
      },
      "source": [
        "# m5_Outdoor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m5_Outdoor.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDucuOaUtg4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50726ab0-5d84-4737-af75-ddc7cdf64b7c"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 48.0  min= 0.0\n",
            "mean= -0.9933797200520835  variance= 0.03452679972215169\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000646 0.022816 0.004677]\n",
            "Test  score: [0.000224 0.01495  0.002947]\n",
            "RMSE  MAE\n",
            "[[0.015  0.0029]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.005 0.001]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000825 0.026212 0.005041]\n",
            "Test  score: [0.000311 0.017637 0.003201]\n",
            "RMSE  MAE\n",
            "[[0.015  0.0029]\n",
            " [0.0176 0.0032]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0109 0.002 ]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000621 0.022309 0.004681]\n",
            "Test  score: [0.000217 0.014746 0.002999]\n",
            "RMSE  MAE\n",
            "[[0.015  0.0029]\n",
            " [0.0176 0.0032]\n",
            " [0.0147 0.003 ]]\n",
            "RMSE  MAE\n",
            "[0.0158 0.003 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxHVvubStg-d"
      },
      "source": [
        "# m6_Pro\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m6_Pro.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS2cSMb8thFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41feecfe-3d6e-472f-c7b5-2122b61c6864"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 68.0  min= 0.0\n",
            "mean= -0.9916000306372551  variance= 0.04181876512912886\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.0009   0.027648 0.006297]\n",
            "Test  score: [0.000406 0.020144 0.004604]\n",
            "RMSE  MAE\n",
            "[[0.0201 0.0046]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0067 0.0015]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000727 0.025054 0.006258]\n",
            "Test  score: [0.000329 0.018128 0.004726]\n",
            "RMSE  MAE\n",
            "[[0.0201 0.0046]\n",
            " [0.0181 0.0047]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0128 0.0031]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000818 0.026523 0.006253]\n",
            "Test  score: [0.000374 0.019336 0.004631]\n",
            "RMSE  MAE\n",
            "[[0.0201 0.0046]\n",
            " [0.0181 0.0047]\n",
            " [0.0193 0.0046]]\n",
            "RMSE  MAE\n",
            "[0.0192 0.0047]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4_IdDnLthH3"
      },
      "source": [
        "# m7_Res\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m7_Res.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2aFCbbgthOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3cfb36c-470e-4b54-8f82-0eff15430565"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 4.0  min= 0.0\n",
            "mean= -0.997197265625  variance= 0.03973737211395832\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001218 0.033212 0.002278]\n",
            "Test  score: [0.000635 0.025195 0.001384]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0014]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0084 0.0005]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00122  0.033247 0.002185]\n",
            "Test  score: [0.000639 0.025282 0.001297]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0014]\n",
            " [0.0253 0.0013]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0168 0.0009]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00122  0.033244 0.002211]\n",
            "Test  score: [0.000639 0.025281 0.001323]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0014]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0013]]\n",
            "RMSE  MAE\n",
            "[0.0253 0.0013]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMzzSXycthU3"
      },
      "source": [
        "# m3_Food\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m3_Food.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k9HNB1-thdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ad5dd9-881e-4d89-bbbc-448c33d49a0f"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 98.0  min= 0.0\n",
            "mean= -0.9913027476615647  variance= 0.054585593962263146\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001029 0.029983 0.005875]\n",
            "Test  score: [0.000488 0.022097 0.003977]\n",
            "RMSE  MAE\n",
            "[[0.0221 0.004 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0074 0.0013]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000773 0.026363 0.005634]\n",
            "Test  score: [0.000389 0.019733 0.004034]\n",
            "RMSE  MAE\n",
            "[[0.0221 0.004 ]\n",
            " [0.0197 0.004 ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0139 0.0027]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000811 0.026949 0.005769]\n",
            "Test  score: [0.00039  0.019737 0.003987]\n",
            "RMSE  MAE\n",
            "[[0.0221 0.004 ]\n",
            " [0.0197 0.004 ]\n",
            " [0.0197 0.004 ]]\n",
            "RMSE  MAE\n",
            "[0.0205 0.004 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILgnq8L2pvvw"
      },
      "source": [
        "# m9_Tra\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m9_Tra.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOaxNvj2pv6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d25952-248e-43ab-f7cd-fee4893a0e1b"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "print(R_N, is_plus)\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 37.0  min= 0.0\n",
            "mean= -0.9933016610360359  variance= 0.037008620307463456\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000525 0.021563 0.004615]\n",
            "Test  score: [0.000277 0.016644 0.003395]\n",
            "RMSE  MAE\n",
            "[[0.0166 0.0034]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0055 0.0011]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000533 0.021737 0.004544]\n",
            "Test  score: [0.000277 0.016657 0.003304]\n",
            "RMSE  MAE\n",
            "[[0.0166 0.0034]\n",
            " [0.0167 0.0033]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0111 0.0022]\n",
            "cpt_conv1 shape is (None, 1, 16, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000514 0.021345 0.004889]\n",
            "Test  score: [0.00027  0.016445 0.003718]\n",
            "RMSE  MAE\n",
            "[[0.0166 0.0034]\n",
            " [0.0167 0.0033]\n",
            " [0.0164 0.0037]]\n",
            "RMSE  MAE\n",
            "[0.0166 0.0035]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aprWnDU5pwF7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5qRsgrtxzg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToZhxhT_tx_s"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69FLVj6KtyJj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBHYYmrtyTE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSrTiF06tycZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMSrD5sAtylz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_92N60ZtyvP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIUSHCm4ty4r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD_MEOuyty71"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4FxssZPtzNr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1x7kHVoG5d"
      },
      "source": [
        "# \"DeepSTN_net.py\"\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import numpy as np\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n",
        "from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "import PPT3_network.metrics as metrics\n",
        "\n",
        "# Relu-BN-Conv2D 3x3\n",
        "def conv_unit0(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    print('kernel=(3,3)')\n",
        "    return unit_model\n",
        "\n",
        "# Relu-BN-Conv2D 1x1\n",
        "def conv_unit1(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    print('kernel=(1,1)')\n",
        "    return unit_model\n",
        "\n",
        "# efficient version of Res_plus\n",
        "def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    #normal channels\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    #separated channels\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n",
        "    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n",
        "    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n",
        "\n",
        "    #merge\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# new resdual block\n",
        "def Res_plus(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n",
        "\n",
        "    cl_conv1B=plus_conv(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n",
        "\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# normal residual block\n",
        "def Res_normal(name,F,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "def cpt_slice(x, h1, h2):\n",
        "    return x[:,h1:h2,:,:]\n",
        "\n",
        "# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)\n",
        "def T_trans(T,T_F,H,W):\n",
        "\n",
        "    T_in=Input(shape=(T+7,H,W))\n",
        "    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n",
        "    T_act=Activation('relu')(T_mid)\n",
        "    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n",
        "    T_mul=Activation('relu')(T_fin)\n",
        "    T_model=Model(inputs=T_in,outputs=T_mul)\n",
        "\n",
        "    return T_model\n",
        "\n",
        "# transfer Time vector and PoI matrix to time-weighted PoI matrix\n",
        "def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n",
        "    if 1:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        if P_N>=2:\n",
        "            T_x0 =T_trans(T,T_F,H,W)(time_in)\n",
        "            T_x1 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=3:\n",
        "            T_x2 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=4:\n",
        "            T_x3 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=5:\n",
        "            T_x4 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=6:\n",
        "            T_x5 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=7:\n",
        "            T_x6 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=8:\n",
        "            T_x7 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=9:\n",
        "            T_x8 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=10:\n",
        "            T_x9 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=11:\n",
        "            T_x10=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=12:\n",
        "            T_x11=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=13:\n",
        "            T_x12=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=14:\n",
        "            T_x13=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=15:\n",
        "            T_x14=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=16:\n",
        "            T_x15=T_trans(T,T_F,H,W)(time_in)\n",
        "\n",
        "\n",
        "        if P_N==1:\n",
        "            T_x=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N==2:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1])\n",
        "        if P_N==3:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n",
        "        if P_N==4:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n",
        "        if P_N==5:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n",
        "        if P_N==6:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n",
        "        if P_N==7:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n",
        "        if P_N==8:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n",
        "        if P_N==9:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n",
        "        if P_N==10:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n",
        "        if P_N==11:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n",
        "        if P_N==12:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n",
        "        if P_N==13:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n",
        "        if P_N==14:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n",
        "        if P_N==15:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n",
        "        if P_N==16:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n",
        "\n",
        "        poi_time=Multiply()([poi_in,T_x])\n",
        "        if isPT_F:\n",
        "            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n",
        "            print('PT_F = YES')\n",
        "        else:\n",
        "            print('PT_F = NO')\n",
        "        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n",
        "\n",
        "        return PT_model\n",
        "\n",
        "# main model\n",
        "def DeepSTN(H=12,W=14,channel=1,      #H-map_height W-map_width channel-map_channel\n",
        "            c=3,p=4,t=4,              #c-closeness p-period t-trend\n",
        "            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n",
        "            is_plus=True,             #use ResPlus or mornal convolution\n",
        "            is_plus_efficient=False,  #use the efficient version of ResPlus\n",
        "            plus=8,rate=2,            #rate-pooling_rate\n",
        "            is_pt=True,               #use PoI and Time or not\n",
        "            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day\n",
        "            drop=0,\n",
        "            is_summary=True, #show detail\n",
        "            lr=0.0002,\n",
        "            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n",
        "            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n",
        "\n",
        "    all_channel = channel * (c+p+t)\n",
        "\n",
        "    cut0 = int( 0 )\n",
        "    cut1 = int( cut0 + channel*c )\n",
        "    cut2 = int( cut1 + channel*p )\n",
        "    cut3 = int( cut2 + channel*t )\n",
        "\n",
        "    cpt_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n",
        "    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n",
        "    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n",
        "\n",
        "    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n",
        "    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n",
        "    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n",
        "\n",
        "    if is_pt:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n",
        "\n",
        "        poi_time=PT_model([poi_in,time_in])\n",
        "\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    else:\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    if is_plus:\n",
        "        if is_plus_efficient:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "        else:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "\n",
        "    else:\n",
        "        for i in range(R_N):\n",
        "            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    cpt_conv2=Activation('relu')(cpt)\n",
        "    cpt_out2=BatchNormalization()(cpt_conv2)\n",
        "    cpt_conv1=Dropout(drop)(cpt_out2)\n",
        "    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n",
        "\n",
        "    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조\n",
        "\n",
        "    cpt_out1=Activation('tanh')(cpt_conv1)\n",
        "\n",
        "    if is_pt:\n",
        "        DeepSTN_model=Model(inputs=[cpt_input,poi_in,time_in],outputs=cpt_out1)\n",
        "    else:\n",
        "        DeepSTN_model=Model(inputs=cpt_input,outputs=cpt_out1)\n",
        "\n",
        "    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[metrics.rmse,metrics.mae])\n",
        "\n",
        "    if is_summary:\n",
        "        DeepSTN_model.summary()\n",
        "\n",
        "\n",
        "    print('***** pre_F : ',pre_F       )\n",
        "    print('***** conv_F: ',conv_F      )\n",
        "    print('***** R_N   : ',R_N         )\n",
        "\n",
        "    print('***** plus  : ',plus*is_plus)\n",
        "    print('***** rate  : ',rate*is_plus)\n",
        "\n",
        "    print('***** P_N   : ',P_N*is_pt   )\n",
        "    print('***** T_F   : ',T_F*is_pt   )\n",
        "    print('***** PT_F  : ',PT_F*is_pt*isPT_F )\n",
        "    print('***** T     : ',T           )\n",
        "\n",
        "    print('***** drop  : ',drop        )\n",
        "\n",
        "    return DeepSTN_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56kVKyKoUI5"
      },
      "source": [
        "# \"metrics.py\"\n",
        "\n",
        "# import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "# aliases\n",
        "mse = MSE = mean_squared_error\n",
        "# rmse = RMSE = root_mean_square_error\n",
        "\n",
        "\n",
        "def masked_mean_squared_error(y_true, y_pred):\n",
        "    idx = (y_true > 1e-6).nonzero()\n",
        "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "threshold=0.05\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-NOpElnMI"
      },
      "source": [
        "#ComparisionBikeNYC.py\n",
        "\n",
        "\n",
        "#from __future__ import print_function\n",
        "#from DATA.lzq_read_data_time_poi import lzq_load_data\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#import cPickle as pickle\n",
        "import numpy as np\n",
        "#import math\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "#from ipdb import set_trace\n",
        "#set_trace()\n",
        "\n",
        "#for GPU in Lab\n",
        "device=6\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(device)\n",
        "import tensorflow as tf  #from V1707\n",
        "config=tf.ConfigProto()  #from V1707\n",
        "config.gpu_options.allow_growth=True  #from V1707\n",
        "#config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
        "sess=tf.Session(config=config)  #from V1707\n",
        "#import keras.backend.tensorflow_backend as KTF\n",
        "#KTF._set_session(tf.Session(config=config))\n",
        "import setproctitle  #from V1707\n",
        "setproctitle.setproctitle('Comprison Start! @ ZiqianLin')  #from V1707\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "\n",
        "#hyperparameters\n",
        "epoch = 1#350  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0002  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 18,15,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 4*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=10\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "#DST result\n",
        "if XDST:\n",
        "    setproctitle.setproctitle('BJMobile DST @ ZiqianLin')  #from V1707\n",
        "\n",
        "    print(\"loading data...\")\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    R_N = 4   # number of residual units\n",
        "\n",
        "    from keras.optimizers import Adam\n",
        "    from DST_network.STResNet import stresnet\n",
        "    import DST_network.metrics as metrics\n",
        "\n",
        "    def build_model(external_dim,CFN):\n",
        "        c_conf = (len_closeness, channel, H, W) if len_closeness > 0 else None\n",
        "        p_conf = (len_period,    channel, H, W) if len_period    > 0 else None\n",
        "        t_conf = (len_trend,     channel, H, W) if len_trend     > 0 else None\n",
        "\n",
        "        model = stresnet(c_conf=c_conf, p_conf=p_conf, t_conf=t_conf,\n",
        "                         external_dim=external_dim, nb_residual_unit=R_N, CF=CFN)\n",
        "\n",
        "        adam = Adam(lr=lr)\n",
        "        model.compile(loss='mse', optimizer=adam, metrics=[metrics.rmse,metrics.mae])\n",
        "        model.summary()\n",
        "        #from keras.utils.visualize_util import plot\n",
        "        #plot(model, to_file='model.png', show_shapes=True)\n",
        "        return model\n",
        "\n",
        "\n",
        "    CF=64\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    import time\n",
        "\n",
        "    count=0\n",
        "\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        time_start=time.time()\n",
        "\n",
        "        F='DST_MODEL/dst_model_'+str(iterate)+'_.hdf5'\n",
        "\n",
        "        model = build_model(external_dim=False,CFN=CF)\n",
        "        if trainDST:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                filepath=F,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "                mode='min',\n",
        "                period=1)\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"training model...\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                nb_epoch=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('evaluating using the model that has the best loss on the valid set')\n",
        "        model.load_weights(F)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        score = model.evaluate(X_test, Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DST_SCORE/dst_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "\n",
        "        print('totally cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+ResPlus+PoI&Time\n",
        "if X11:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+ResPlus+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    index=np.arange(9)\n",
        "    P_train=P_train[:,index,:,:]\n",
        "    P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=True\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=True\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_11/MODEL/DeepSTN_11_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train11:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit([X_train,P_train,T_train], Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate([X_test ,P_test ,T_test ], Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_11/SCORE/DeepSTN_11_score3.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+ResPlus\n",
        "if X10:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+ResPlus @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    #index=np.arange(9)\n",
        "    #P_train=P_train[:,index,:,:]\n",
        "    #P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=True\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=False\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_10/MODEL/DeepSTN_10_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train10:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate(X_test,  Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_10/SCORE/DeepSTN_10_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+PoI&Time\n",
        "if X01:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    #index=np.arange(9)\n",
        "    #P_train=P_train[:,index,:,:]\n",
        "    #P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=False\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=True\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_01/MODEL/DeepSTN_01_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train01:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit([X_train,P_train,T_train], Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate([X_test, P_test, T_test ],  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_01/SCORE/DeepSTN_01_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN\n",
        "if X00:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    index=np.arange(9)\n",
        "    P_train=P_train[:,index,:,:]\n",
        "    P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=False\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=False\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train00:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#Comparison\n",
        "X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "print('MODEL                     RMSE  MAE')\n",
        "if 0:\n",
        "    print('ResNet                  :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DST_SCORE/dst_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    print(for_show)\n",
        "if 0:\n",
        "    print('DeepSTN                 :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_00/SCORE/DeepSTN_00_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 1:\n",
        "    print('DeepSTN+ResPlus         :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_10/SCORE/DeepSTN_10_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 0:\n",
        "    print('DeepSTN+PoI&Time        :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_01/SCORE/DeepSTN_01_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 1:\n",
        "    print('DeepSTN+ResPlus+PoI&Time:',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_11/SCORE/DeepSTN_11_score3.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}