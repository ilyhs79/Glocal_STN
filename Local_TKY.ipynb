{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV_EdNWClAq9",
        "outputId": "62fc86fb-810e-4a42-ef95-c31433488bd1"
      },
      "source": [
        "# google drive connect\n",
        "Copied_path = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN' # Paste target directory here\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(Copied_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AauHopRUnGYv"
      },
      "source": [
        "# m8_shop\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m8_Shop.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgFX7E7fo_a8"
      },
      "source": [
        "# DST Network/ilayer.py\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "# from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class iLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        # self.output_dim = output_dim\n",
        "        super(iLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        initial_weight_value = np.random.random(input_shape[1:])\n",
        "        self.W = K.variable(initial_weight_value)\n",
        "        #self.trainable_weights = [self.W]\n",
        "        self.trainable_weight = [self.W]\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return x * self.W\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k925xkl8Uynj"
      },
      "source": [
        "# \"metrics.py\"\n",
        "\n",
        "# import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "# aliases\n",
        "mse = MSE = mean_squared_error\n",
        "# rmse = RMSE = root_mean_square_error\n",
        "\n",
        "\n",
        "def masked_mean_squared_error(y_true, y_pred):\n",
        "    idx = (y_true > 1e-6).nonzero()\n",
        "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "threshold=0.05\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8M9judzxQIB"
      },
      "source": [
        "# \"DeepSTN_net.py\"\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import numpy as np\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n",
        "from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "#import PPT3_network.metrics as metrics\n",
        "\n",
        "# Relu-BN-Conv2D 3x3\n",
        "def conv_unit0(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    #print('kernel=(3,3)')\n",
        "    return unit_model\n",
        "\n",
        "# Relu-BN-Conv2D 1x1\n",
        "def conv_unit1(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    #print('kernel=(1,1)')\n",
        "    return unit_model\n",
        "\n",
        "# efficient version of Res_plus\n",
        "def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    #normal channels\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    #separated channels\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n",
        "    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n",
        "    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n",
        "\n",
        "    #merge\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# new resdual block\n",
        "def Res_plus(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n",
        "\n",
        "    cl_conv1B=plus_conv(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n",
        "\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# normal residual block\n",
        "def Res_normal(name,F,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "def cpt_slice(x, h1, h2):\n",
        "    return x[:,h1:h2,:,:]\n",
        "\n",
        "# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)\n",
        "def T_trans(T,T_F,H,W):\n",
        "\n",
        "    T_in=Input(shape=(T+7,H,W))\n",
        "    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n",
        "    T_act=Activation('relu')(T_mid)\n",
        "    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n",
        "    T_mul=Activation('relu')(T_fin)\n",
        "    T_model=Model(inputs=T_in,outputs=T_mul)\n",
        "\n",
        "    return T_model\n",
        "\n",
        "# transfer Time vector and PoI matrix to time-weighted PoI matrix\n",
        "def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n",
        "    if 1:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        if P_N>=2:\n",
        "            T_x0 =T_trans(T,T_F,H,W)(time_in)\n",
        "            T_x1 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=3:\n",
        "            T_x2 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=4:\n",
        "            T_x3 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=5:\n",
        "            T_x4 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=6:\n",
        "            T_x5 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=7:\n",
        "            T_x6 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=8:\n",
        "            T_x7 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=9:\n",
        "            T_x8 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=10:\n",
        "            T_x9 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=11:\n",
        "            T_x10=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=12:\n",
        "            T_x11=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=13:\n",
        "            T_x12=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=14:\n",
        "            T_x13=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=15:\n",
        "            T_x14=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=16:\n",
        "            T_x15=T_trans(T,T_F,H,W)(time_in)\n",
        "\n",
        "\n",
        "        if P_N==1:\n",
        "            T_x=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N==2:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1])\n",
        "        if P_N==3:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n",
        "        if P_N==4:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n",
        "        if P_N==5:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n",
        "        if P_N==6:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n",
        "        if P_N==7:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n",
        "        if P_N==8:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n",
        "        if P_N==9:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n",
        "        if P_N==10:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n",
        "        if P_N==11:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n",
        "        if P_N==12:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n",
        "        if P_N==13:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n",
        "        if P_N==14:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n",
        "        if P_N==15:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n",
        "        if P_N==16:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n",
        "\n",
        "        poi_time=Multiply()([poi_in,T_x])\n",
        "        if isPT_F:\n",
        "            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n",
        "            print('PT_F = YES')\n",
        "        else:\n",
        "            print('PT_F = NO')\n",
        "        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n",
        "\n",
        "        return PT_model\n",
        "\n",
        "# main model\n",
        "def DeepSTN(H=14,W=12,channel=1,      #H-map_height W-map_width channel-map_channel\n",
        "            c=3,p=4,t=4,              #c-closeness p-period t-trend\n",
        "            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n",
        "            is_plus=True,             #use ResPlus or mornal convolution\n",
        "            is_plus_efficient=False,  #use the efficient version of ResPlus\n",
        "            plus=8,rate=2,            #rate-pooling_rate\n",
        "            is_pt=False,               #use PoI and Time or not\n",
        "            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day\n",
        "            drop=0,\n",
        "            is_summary=False, #show detail\n",
        "            lr=0.0001,\n",
        "            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n",
        "            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n",
        "\n",
        "    all_channel = channel * (c+p+t)\n",
        "\n",
        "    cut0 = int( 0 )\n",
        "    cut1 = int( cut0 + channel*c )\n",
        "    cut2 = int( cut1 + channel*p )\n",
        "    cut3 = int( cut2 + channel*t )\n",
        "\n",
        "    cpt_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n",
        "    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n",
        "    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n",
        "\n",
        "    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n",
        "    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n",
        "    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n",
        "\n",
        "    if is_pt:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n",
        "\n",
        "        poi_time=PT_model([poi_in,time_in])\n",
        "\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    else:\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    if is_plus:\n",
        "        if is_plus_efficient:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "        else:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    else:\n",
        "        for i in range(R_N):\n",
        "            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    cpt_conv2=Activation('relu')(cpt)\n",
        "    cpt_out2=BatchNormalization()(cpt_conv2)\n",
        "    cpt_conv1=Dropout(drop)(cpt_out2)\n",
        "    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n",
        "    print('cpt_conv1 shape is', cpt_conv1.shape)\n",
        "    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조\n",
        "\n",
        "    cpt_out1=Activation('tanh')(cpt_conv1)\n",
        "\n",
        "    if is_pt:\n",
        "        DeepSTN_model=Model(inputs=[cpt_input,poi_in,time_in],outputs=cpt_out1)\n",
        "    else:\n",
        "        DeepSTN_model=Model(inputs=cpt_input,outputs=cpt_out1)\n",
        "\n",
        "    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[rmse,mae])\n",
        "\n",
        "    if is_summary:\n",
        "        DeepSTN_model.summary()\n",
        "\n",
        "    '''\n",
        "    print('***** pre_F : ',pre_F       )\n",
        "    print('***** conv_F: ',conv_F      )\n",
        "    print('***** R_N   : ',R_N         )\n",
        "\n",
        "    print('***** plus  : ',plus*is_plus)\n",
        "    print('***** rate  : ',rate*is_plus)\n",
        "\n",
        "    print('***** P_N   : ',P_N*is_pt   )\n",
        "    print('***** T_F   : ',T_F*is_pt   )\n",
        "    print('***** PT_F  : ',PT_F*is_pt*isPT_F )\n",
        "    print('***** T     : ',T           )\n",
        "\n",
        "    print('***** drop  : ',drop        )\n",
        "    '''\n",
        "    return DeepSTN_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02uqnK3dUwka",
        "outputId": "282404d2-f14e-4859-9661-fafe586d1734"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=3\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "R_N=10\n",
        "\n",
        "is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=1,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''\n",
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 146.0  min= 0.0\n",
            "mean= -0.9895630507750447  variance= 0.04576444793426465\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 45s 433ms/step - loss: 0.3784 - rmse: 0.3634 - mae: 0.2766 - val_loss: 0.0088 - val_rmse: 0.0938 - val_mae: 0.0732\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0019 - rmse: 0.0437 - mae: 0.0104 - val_loss: 0.0032 - val_rmse: 0.0557 - val_mae: 0.0393\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0022 - rmse: 0.0467 - mae: 0.0102 - val_loss: 0.0025 - val_rmse: 0.0498 - val_mae: 0.0354\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0018 - rmse: 0.0418 - mae: 0.0099 - val_loss: 0.0024 - val_rmse: 0.0486 - val_mae: 0.0347\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0021 - rmse: 0.0459 - mae: 0.0102 - val_loss: 0.0027 - val_rmse: 0.0523 - val_mae: 0.0352\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0023 - rmse: 0.0474 - mae: 0.0109 - val_loss: 0.0022 - val_rmse: 0.0461 - val_mae: 0.0321\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0020 - rmse: 0.0441 - mae: 0.0102 - val_loss: 0.0021 - val_rmse: 0.0462 - val_mae: 0.0316\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0018 - rmse: 0.0415 - mae: 0.0098 - val_loss: 0.0019 - val_rmse: 0.0427 - val_mae: 0.0285\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0021 - rmse: 0.0453 - mae: 0.0101 - val_loss: 0.0017 - val_rmse: 0.0412 - val_mae: 0.0262\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0021 - rmse: 0.0453 - mae: 0.0101 - val_loss: 0.0016 - val_rmse: 0.0388 - val_mae: 0.0239\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0018 - rmse: 0.0415 - mae: 0.0092 - val_loss: 0.0013 - val_rmse: 0.0362 - val_mae: 0.0220\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0018 - rmse: 0.0422 - mae: 0.0095 - val_loss: 0.0022 - val_rmse: 0.0467 - val_mae: 0.0216\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0022 - rmse: 0.0462 - mae: 0.0106 - val_loss: 0.0015 - val_rmse: 0.0383 - val_mae: 0.0187\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0019 - rmse: 0.0422 - mae: 0.0100 - val_loss: 0.0013 - val_rmse: 0.0359 - val_mae: 0.0164\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0018 - rmse: 0.0418 - mae: 0.0099 - val_loss: 0.0014 - val_rmse: 0.0370 - val_mae: 0.0153\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0019 - rmse: 0.0430 - mae: 0.0102 - val_loss: 9.8119e-04 - val_rmse: 0.0308 - val_mae: 0.0131\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0019 - rmse: 0.0427 - mae: 0.0102 - val_loss: 0.0011 - val_rmse: 0.0322 - val_mae: 0.0125\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0018 - rmse: 0.0422 - mae: 0.0101 - val_loss: 0.0011 - val_rmse: 0.0322 - val_mae: 0.0115\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0399 - mae: 0.0099 - val_loss: 9.8203e-04 - val_rmse: 0.0308 - val_mae: 0.0107\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0396 - mae: 0.0098 - val_loss: 8.8837e-04 - val_rmse: 0.0290 - val_mae: 0.0096\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0017 - rmse: 0.0405 - mae: 0.0099 - val_loss: 9.2636e-04 - val_rmse: 0.0296 - val_mae: 0.0093\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0017 - rmse: 0.0402 - mae: 0.0097 - val_loss: 9.0040e-04 - val_rmse: 0.0292 - val_mae: 0.0088\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0015 - rmse: 0.0376 - mae: 0.0096 - val_loss: 8.5518e-04 - val_rmse: 0.0285 - val_mae: 0.0086\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0015 - rmse: 0.0385 - mae: 0.0098 - val_loss: 8.6081e-04 - val_rmse: 0.0286 - val_mae: 0.0083\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0404 - mae: 0.0102 - val_loss: 8.9839e-04 - val_rmse: 0.0292 - val_mae: 0.0081\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0401 - mae: 0.0100 - val_loss: 8.8584e-04 - val_rmse: 0.0290 - val_mae: 0.0080\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0017 - rmse: 0.0411 - mae: 0.0101 - val_loss: 8.7381e-04 - val_rmse: 0.0288 - val_mae: 0.0079\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0015 - rmse: 0.0385 - mae: 0.0097 - val_loss: 8.6685e-04 - val_rmse: 0.0287 - val_mae: 0.0079\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0399 - mae: 0.0101 - val_loss: 8.6394e-04 - val_rmse: 0.0287 - val_mae: 0.0078\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0017 - rmse: 0.0404 - mae: 0.0101 - val_loss: 8.8237e-04 - val_rmse: 0.0289 - val_mae: 0.0078\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0016 - rmse: 0.0401 - mae: 0.0100 - val_loss: 8.8075e-04 - val_rmse: 0.0289 - val_mae: 0.0076\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0015 - rmse: 0.0391 - mae: 0.0095 - val_loss: 9.0343e-04 - val_rmse: 0.0292 - val_mae: 0.0076\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0016 - rmse: 0.0399 - mae: 0.0100 - val_loss: 8.5125e-04 - val_rmse: 0.0284 - val_mae: 0.0076\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0017 - rmse: 0.0406 - mae: 0.0102 - val_loss: 8.7752e-04 - val_rmse: 0.0287 - val_mae: 0.0075\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0015 - rmse: 0.0382 - mae: 0.0098 - val_loss: 8.2108e-04 - val_rmse: 0.0278 - val_mae: 0.0074\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0017 - rmse: 0.0404 - mae: 0.0099 - val_loss: 9.0438e-04 - val_rmse: 0.0292 - val_mae: 0.0073\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0015 - rmse: 0.0381 - mae: 0.0102 - val_loss: 8.7101e-04 - val_rmse: 0.0286 - val_mae: 0.0073\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0015 - rmse: 0.0378 - mae: 0.0096 - val_loss: 8.0636e-04 - val_rmse: 0.0276 - val_mae: 0.0074\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0017 - rmse: 0.0406 - mae: 0.0108 - val_loss: 9.0952e-04 - val_rmse: 0.0292 - val_mae: 0.0073\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0014 - rmse: 0.0371 - mae: 0.0096 - val_loss: 8.3648e-04 - val_rmse: 0.0280 - val_mae: 0.0073\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0013 - rmse: 0.0358 - mae: 0.0094 - val_loss: 8.7347e-04 - val_rmse: 0.0286 - val_mae: 0.0073\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0015 - rmse: 0.0385 - mae: 0.0099 - val_loss: 8.5477e-04 - val_rmse: 0.0284 - val_mae: 0.0073\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0394 - mae: 0.0104 - val_loss: 8.5076e-04 - val_rmse: 0.0284 - val_mae: 0.0073\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0392 - mae: 0.0102 - val_loss: 8.8367e-04 - val_rmse: 0.0288 - val_mae: 0.0073\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0393 - mae: 0.0100 - val_loss: 8.2457e-04 - val_rmse: 0.0279 - val_mae: 0.0073\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0015 - rmse: 0.0389 - mae: 0.0099 - val_loss: 8.5864e-04 - val_rmse: 0.0284 - val_mae: 0.0073\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0015 - rmse: 0.0377 - mae: 0.0099 - val_loss: 8.3741e-04 - val_rmse: 0.0281 - val_mae: 0.0073\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0017 - rmse: 0.0405 - mae: 0.0102 - val_loss: 8.6078e-04 - val_rmse: 0.0285 - val_mae: 0.0073\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0401 - mae: 0.0100 - val_loss: 8.4291e-04 - val_rmse: 0.0283 - val_mae: 0.0073\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0017 - rmse: 0.0405 - mae: 0.0101 - val_loss: 8.5729e-04 - val_rmse: 0.0284 - val_mae: 0.0072\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0401 - mae: 0.0102 - val_loss: 8.3545e-04 - val_rmse: 0.0281 - val_mae: 0.0073\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0014 - rmse: 0.0366 - mae: 0.0095 - val_loss: 8.1154e-04 - val_rmse: 0.0277 - val_mae: 0.0073\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0015 - rmse: 0.0385 - mae: 0.0103 - val_loss: 8.7214e-04 - val_rmse: 0.0286 - val_mae: 0.0072\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0015 - rmse: 0.0390 - mae: 0.0102 - val_loss: 8.0329e-04 - val_rmse: 0.0276 - val_mae: 0.0073\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0014 - rmse: 0.0366 - mae: 0.0099 - val_loss: 8.3545e-04 - val_rmse: 0.0280 - val_mae: 0.0073\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0016 - rmse: 0.0398 - mae: 0.0102 - val_loss: 8.1117e-04 - val_rmse: 0.0276 - val_mae: 0.0073\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0389 - mae: 0.0100 - val_loss: 8.6750e-04 - val_rmse: 0.0286 - val_mae: 0.0072\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0393 - mae: 0.0099 - val_loss: 8.0195e-04 - val_rmse: 0.0275 - val_mae: 0.0073\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0015 - rmse: 0.0378 - mae: 0.0099 - val_loss: 8.0674e-04 - val_rmse: 0.0275 - val_mae: 0.0073\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0398 - mae: 0.0102 - val_loss: 8.1056e-04 - val_rmse: 0.0275 - val_mae: 0.0072\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0013 - rmse: 0.0358 - mae: 0.0093 - val_loss: 7.8357e-04 - val_rmse: 0.0272 - val_mae: 0.0073\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0015 - rmse: 0.0382 - mae: 0.0101 - val_loss: 8.5078e-04 - val_rmse: 0.0281 - val_mae: 0.0072\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0016 - rmse: 0.0393 - mae: 0.0100 - val_loss: 8.0465e-04 - val_rmse: 0.0275 - val_mae: 0.0073\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0013 - rmse: 0.0364 - mae: 0.0100 - val_loss: 7.7334e-04 - val_rmse: 0.0270 - val_mae: 0.0073\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0014 - rmse: 0.0373 - mae: 0.0100 - val_loss: 8.0466e-04 - val_rmse: 0.0275 - val_mae: 0.0072\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0014 - rmse: 0.0368 - mae: 0.0097 - val_loss: 7.6661e-04 - val_rmse: 0.0268 - val_mae: 0.0072\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0017 - rmse: 0.0410 - mae: 0.0107 - val_loss: 8.1476e-04 - val_rmse: 0.0276 - val_mae: 0.0072\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0014 - rmse: 0.0362 - mae: 0.0095 - val_loss: 7.6207e-04 - val_rmse: 0.0267 - val_mae: 0.0073\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0014 - rmse: 0.0372 - mae: 0.0098 - val_loss: 8.0985e-04 - val_rmse: 0.0275 - val_mae: 0.0072\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0013 - rmse: 0.0358 - mae: 0.0095 - val_loss: 7.7075e-04 - val_rmse: 0.0270 - val_mae: 0.0073\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0016 - rmse: 0.0397 - mae: 0.0105 - val_loss: 7.8964e-04 - val_rmse: 0.0273 - val_mae: 0.0072\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0015 - rmse: 0.0378 - mae: 0.0102 - val_loss: 7.7554e-04 - val_rmse: 0.0271 - val_mae: 0.0072\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 2s 120ms/step - loss: 0.0015 - rmse: 0.0379 - mae: 0.0101 - val_loss: 7.4863e-04 - val_rmse: 0.0266 - val_mae: 0.0073\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 2s 121ms/step - loss: 0.0015 - rmse: 0.0390 - mae: 0.0101 - val_loss: 7.6128e-04 - val_rmse: 0.0267 - val_mae: 0.0072\n",
            "Epoch 75/100\n",
            " 1/13 [=>............................] - ETA: 1s - loss: 0.0015 - rmse: 0.0392 - mae: 0.0106"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTujgh28pvkw"
      },
      "source": [
        "# m0_Ent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m0_Ent.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaareiVrtfkE",
        "outputId": "422c3825-fcac-45bd-c3e9-720cd56b5e1b"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 33.0  min= 0.0\n",
            "mean= -0.9878032883051704  variance= 0.06483403736343217\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002489 0.04768  0.011148]\n",
            "Test  score: [0.00158  0.039752 0.009248]\n",
            "RMSE  MAE\n",
            "[[0.0398 0.0092]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0133 0.0031]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002796 0.050246 0.01058 ]\n",
            "Test  score: [0.001767 0.042034 0.00859 ]\n",
            "RMSE  MAE\n",
            "[[0.0398 0.0092]\n",
            " [0.042  0.0086]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0273 0.0059]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002556 0.048007 0.011177]\n",
            "Test  score: [0.001606 0.040074 0.009304]\n",
            "RMSE  MAE\n",
            "[[0.0398 0.0092]\n",
            " [0.042  0.0086]\n",
            " [0.0401 0.0093]]\n",
            "RMSE  MAE\n",
            "[0.0406 0.009 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Pu97ettf0M"
      },
      "source": [
        "# m1_Col\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m1_Col.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBowoAPctgES",
        "outputId": "601be8d0-d61a-4575-863e-741e3ce904ba"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 13.0  min= 0.0\n",
            "mean= -0.9942343311226624  variance= 0.04226600741012836\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001362 0.03434  0.005665]\n",
            "Test  score: [0.000747 0.027337 0.004318]\n",
            "RMSE  MAE\n",
            "[[0.0273 0.0043]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0091 0.0014]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001371 0.034349 0.005448]\n",
            "Test  score: [0.000753 0.02744  0.004097]\n",
            "RMSE  MAE\n",
            "[[0.0273 0.0043]\n",
            " [0.0274 0.0041]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0183 0.0028]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001358 0.034337 0.005527]\n",
            "Test  score: [0.000751 0.027408 0.004191]\n",
            "RMSE  MAE\n",
            "[[0.0273 0.0043]\n",
            " [0.0274 0.0041]\n",
            " [0.0274 0.0042]]\n",
            "RMSE  MAE\n",
            "[0.0274 0.0042]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYwKj5jjtgR_"
      },
      "source": [
        "# m2_mv\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m2_Ev.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhHEwFastgdY",
        "outputId": "97afceee-cb6e-4415-a25c-46e29de01cdc"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 1.0  min= 0.0\n",
            "mean= -0.9998356933739618  variance= 0.018126948320356582\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000413 0.007952 0.000237]\n",
            "Test  score: [0.000213 0.01458  0.000139]\n",
            "RMSE  MAE\n",
            "[[0.0146 0.0001]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0049 0.    ]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000413 0.007953 0.000232]\n",
            "Test  score: [0.000213 0.014579 0.000135]\n",
            "RMSE  MAE\n",
            "[[0.0146 0.0001]\n",
            " [0.0146 0.0001]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0097 0.0001]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000413 0.007947 0.000229]\n",
            "Test  score: [0.000213 0.01458  0.000132]\n",
            "RMSE  MAE\n",
            "[[0.0146 0.0001]\n",
            " [0.0146 0.0001]\n",
            " [0.0146 0.0001]]\n",
            "RMSE  MAE\n",
            "[0.0146 0.0001]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwgQZLttgj7"
      },
      "source": [
        "# m4_Night\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m4_Night.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fUmWkrptgqb",
        "outputId": "31b0e3fc-c4b4-4818-ca01-818f52d6bb27"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 3.0  min= 0.0\n",
            "mean= -0.9968283842185973  variance= 0.048270752295892394\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002168 0.044863 0.003358]\n",
            "Test  score: [0.001665 0.040805 0.002662]\n",
            "RMSE  MAE\n",
            "[[0.0408 0.0027]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0136 0.0009]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002153 0.044752 0.00343 ]\n",
            "Test  score: [0.001669 0.040853 0.002749]\n",
            "RMSE  MAE\n",
            "[[0.0408 0.0027]\n",
            " [0.0409 0.0027]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0272 0.0018]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002167 0.044851 0.003365]\n",
            "Test  score: [0.001666 0.040819 0.002672]\n",
            "RMSE  MAE\n",
            "[[0.0408 0.0027]\n",
            " [0.0409 0.0027]\n",
            " [0.0408 0.0027]]\n",
            "RMSE  MAE\n",
            "[0.0408 0.0027]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJBT50u6tgxG"
      },
      "source": [
        "# m5_Outdoor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m5_Outdoor.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDucuOaUtg4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fde6a97-6ad5-48a6-c7b3-b6849b8b8c53"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 130.0  min= 0.0\n",
            "mean= -0.9962962069298331  variance= 0.0168414251671113\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000295 0.013743 0.003217]\n",
            "Test  score: [0.000102 0.01008  0.002512]\n",
            "RMSE  MAE\n",
            "[[0.0101 0.0025]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0034 0.0008]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000281 0.013337 0.003215]\n",
            "Test  score: [0.000095 0.009742 0.002536]\n",
            "RMSE  MAE\n",
            "[[0.0101 0.0025]\n",
            " [0.0097 0.0025]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0066 0.0017]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000296 0.013779 0.00321 ]\n",
            "Test  score: [0.000102 0.010115 0.002504]\n",
            "RMSE  MAE\n",
            "[[0.0101 0.0025]\n",
            " [0.0097 0.0025]\n",
            " [0.0101 0.0025]]\n",
            "RMSE  MAE\n",
            "[0.01   0.0025]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxHVvubStg-d"
      },
      "source": [
        "# m6_Pro\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m6_Pro.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS2cSMb8thFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a644a9c4-e5db-4608-f212-18a685f9eada"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 35.0  min= 0.0\n",
            "mean= -0.985686545634565  variance= 0.052230389053797795\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001687 0.040239 0.013136]\n",
            "Test  score: [0.001229 0.035053 0.011595]\n",
            "RMSE  MAE\n",
            "[[0.0351 0.0116]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0117 0.0039]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001767 0.041172 0.012495]\n",
            "Test  score: [0.001288 0.035893 0.0108  ]\n",
            "RMSE  MAE\n",
            "[[0.0351 0.0116]\n",
            " [0.0359 0.0108]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0236 0.0075]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001907 0.042734 0.012302]\n",
            "Test  score: [0.00137  0.037019 0.010482]\n",
            "RMSE  MAE\n",
            "[[0.0351 0.0116]\n",
            " [0.0359 0.0108]\n",
            " [0.037  0.0105]]\n",
            "RMSE  MAE\n",
            "[0.036 0.011]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4_IdDnLthH3"
      },
      "source": [
        "# m7_Res\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m7_Res.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2aFCbbgthOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8283c25f-994a-4953-a76f-d37ce3d57c5e"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 4.0  min= 0.0\n",
            "mean= -0.9962097448766206  variance= 0.04684482697013769\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001908 0.042197 0.003683]\n",
            "Test  score: [0.001753 0.041867 0.003283]\n",
            "RMSE  MAE\n",
            "[[0.0419 0.0033]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.014  0.0011]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001918 0.04231  0.00351 ]\n",
            "Test  score: [0.001764 0.042001 0.003109]\n",
            "RMSE  MAE\n",
            "[[0.0419 0.0033]\n",
            " [0.042  0.0031]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.028  0.0021]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001905 0.042171 0.003765]\n",
            "Test  score: [0.001752 0.041857 0.003367]\n",
            "RMSE  MAE\n",
            "[[0.0419 0.0033]\n",
            " [0.042  0.0031]\n",
            " [0.0419 0.0034]]\n",
            "RMSE  MAE\n",
            "[0.0419 0.0033]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMzzSXycthU3"
      },
      "source": [
        "# m3_Food\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m3_Food.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k9HNB1-thdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af694f5e-1a9d-454d-be69-a741fe30db58"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 63.0  min= 0.0\n",
            "mean= -0.9811303442330509  variance= 0.07453364279577186\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002352 0.047242 0.014739]\n",
            "Test  score: [0.00164  0.040496 0.012638]\n",
            "RMSE  MAE\n",
            "[[0.0405 0.0126]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0135 0.0042]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002341 0.047149 0.015101]\n",
            "Test  score: [0.001673 0.040901 0.013068]\n",
            "RMSE  MAE\n",
            "[[0.0405 0.0126]\n",
            " [0.0409 0.0131]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0271 0.0086]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002532 0.048989 0.015004]\n",
            "Test  score: [0.00171  0.041355 0.012695]\n",
            "RMSE  MAE\n",
            "[[0.0405 0.0126]\n",
            " [0.0409 0.0131]\n",
            " [0.0414 0.0127]]\n",
            "RMSE  MAE\n",
            "[0.0409 0.0128]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILgnq8L2pvvw"
      },
      "source": [
        "# m9_Tra\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data2/m9_Tra.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOaxNvj2pv6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c7d0fe-8a89-4128-d02f-6d54aaaa1f2f"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=2\n",
        "\n",
        "#is_plus=False\n",
        "print(R_N, is_plus)\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit(X_train, Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=(X_test, Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_data shape:  (797, 1, 14, 12)\n",
            "max= 132.0  min= 0.0\n",
            "mean= -0.9752139154585828  variance= 0.08477867360777425\n",
            "number_of_skip_hours: 224\n",
            "len_train=461\n",
            "len_test =112\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002612 0.050072 0.017011]\n",
            "Test  score: [0.001954 0.044203 0.015137]\n",
            "RMSE  MAE\n",
            "[[0.0442 0.0151]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0147 0.005 ]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002751 0.051285 0.017065]\n",
            "Test  score: [0.002077 0.045572 0.014938]\n",
            "RMSE  MAE\n",
            "[[0.0442 0.0151]\n",
            " [0.0456 0.0149]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0299 0.01  ]\n",
            "cpt_conv1 shape is (None, 1, 14, 12)\n",
            "              mse     rmse    mae\n",
            "Train score: [0.002678 0.050687 0.016953]\n",
            "Test  score: [0.00203  0.045051 0.014911]\n",
            "RMSE  MAE\n",
            "[[0.0442 0.0151]\n",
            " [0.0456 0.0149]\n",
            " [0.0451 0.0149]]\n",
            "RMSE  MAE\n",
            "[0.0449 0.015 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aprWnDU5pwF7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5qRsgrtxzg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToZhxhT_tx_s"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69FLVj6KtyJj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBHYYmrtyTE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSrTiF06tycZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMSrD5sAtylz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_92N60ZtyvP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIUSHCm4ty4r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD_MEOuyty71"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4FxssZPtzNr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1x7kHVoG5d"
      },
      "source": [
        "# \"DeepSTN_net.py\"\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import numpy as np\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n",
        "from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "import PPT3_network.metrics as metrics\n",
        "\n",
        "# Relu-BN-Conv2D 3x3\n",
        "def conv_unit0(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    print('kernel=(3,3)')\n",
        "    return unit_model\n",
        "\n",
        "# Relu-BN-Conv2D 1x1\n",
        "def conv_unit1(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    print('kernel=(1,1)')\n",
        "    return unit_model\n",
        "\n",
        "# efficient version of Res_plus\n",
        "def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    #normal channels\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    #separated channels\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n",
        "    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n",
        "    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n",
        "\n",
        "    #merge\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# new resdual block\n",
        "def Res_plus(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n",
        "\n",
        "    cl_conv1B=plus_conv(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n",
        "\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# normal residual block\n",
        "def Res_normal(name,F,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "def cpt_slice(x, h1, h2):\n",
        "    return x[:,h1:h2,:,:]\n",
        "\n",
        "# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)\n",
        "def T_trans(T,T_F,H,W):\n",
        "\n",
        "    T_in=Input(shape=(T+7,H,W))\n",
        "    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n",
        "    T_act=Activation('relu')(T_mid)\n",
        "    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n",
        "    T_mul=Activation('relu')(T_fin)\n",
        "    T_model=Model(inputs=T_in,outputs=T_mul)\n",
        "\n",
        "    return T_model\n",
        "\n",
        "# transfer Time vector and PoI matrix to time-weighted PoI matrix\n",
        "def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n",
        "    if 1:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        if P_N>=2:\n",
        "            T_x0 =T_trans(T,T_F,H,W)(time_in)\n",
        "            T_x1 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=3:\n",
        "            T_x2 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=4:\n",
        "            T_x3 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=5:\n",
        "            T_x4 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=6:\n",
        "            T_x5 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=7:\n",
        "            T_x6 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=8:\n",
        "            T_x7 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=9:\n",
        "            T_x8 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=10:\n",
        "            T_x9 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=11:\n",
        "            T_x10=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=12:\n",
        "            T_x11=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=13:\n",
        "            T_x12=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=14:\n",
        "            T_x13=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=15:\n",
        "            T_x14=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=16:\n",
        "            T_x15=T_trans(T,T_F,H,W)(time_in)\n",
        "\n",
        "\n",
        "        if P_N==1:\n",
        "            T_x=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N==2:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1])\n",
        "        if P_N==3:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n",
        "        if P_N==4:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n",
        "        if P_N==5:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n",
        "        if P_N==6:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n",
        "        if P_N==7:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n",
        "        if P_N==8:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n",
        "        if P_N==9:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n",
        "        if P_N==10:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n",
        "        if P_N==11:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n",
        "        if P_N==12:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n",
        "        if P_N==13:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n",
        "        if P_N==14:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n",
        "        if P_N==15:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n",
        "        if P_N==16:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n",
        "\n",
        "        poi_time=Multiply()([poi_in,T_x])\n",
        "        if isPT_F:\n",
        "            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n",
        "            print('PT_F = YES')\n",
        "        else:\n",
        "            print('PT_F = NO')\n",
        "        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n",
        "\n",
        "        return PT_model\n",
        "\n",
        "# main model\n",
        "def DeepSTN(H=12,W=14,channel=1,      #H-map_height W-map_width channel-map_channel\n",
        "            c=3,p=4,t=4,              #c-closeness p-period t-trend\n",
        "            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n",
        "            is_plus=True,             #use ResPlus or mornal convolution\n",
        "            is_plus_efficient=False,  #use the efficient version of ResPlus\n",
        "            plus=8,rate=2,            #rate-pooling_rate\n",
        "            is_pt=True,               #use PoI and Time or not\n",
        "            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day\n",
        "            drop=0,\n",
        "            is_summary=True, #show detail\n",
        "            lr=0.0002,\n",
        "            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n",
        "            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n",
        "\n",
        "    all_channel = channel * (c+p+t)\n",
        "\n",
        "    cut0 = int( 0 )\n",
        "    cut1 = int( cut0 + channel*c )\n",
        "    cut2 = int( cut1 + channel*p )\n",
        "    cut3 = int( cut2 + channel*t )\n",
        "\n",
        "    cpt_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n",
        "    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n",
        "    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n",
        "\n",
        "    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n",
        "    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n",
        "    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n",
        "\n",
        "    if is_pt:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n",
        "\n",
        "        poi_time=PT_model([poi_in,time_in])\n",
        "\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    else:\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    if is_plus:\n",
        "        if is_plus_efficient:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "        else:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "\n",
        "    else:\n",
        "        for i in range(R_N):\n",
        "            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    cpt_conv2=Activation('relu')(cpt)\n",
        "    cpt_out2=BatchNormalization()(cpt_conv2)\n",
        "    cpt_conv1=Dropout(drop)(cpt_out2)\n",
        "    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n",
        "\n",
        "    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조\n",
        "\n",
        "    cpt_out1=Activation('tanh')(cpt_conv1)\n",
        "\n",
        "    if is_pt:\n",
        "        DeepSTN_model=Model(inputs=[cpt_input,poi_in,time_in],outputs=cpt_out1)\n",
        "    else:\n",
        "        DeepSTN_model=Model(inputs=cpt_input,outputs=cpt_out1)\n",
        "\n",
        "    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[metrics.rmse,metrics.mae])\n",
        "\n",
        "    if is_summary:\n",
        "        DeepSTN_model.summary()\n",
        "\n",
        "\n",
        "    print('***** pre_F : ',pre_F       )\n",
        "    print('***** conv_F: ',conv_F      )\n",
        "    print('***** R_N   : ',R_N         )\n",
        "\n",
        "    print('***** plus  : ',plus*is_plus)\n",
        "    print('***** rate  : ',rate*is_plus)\n",
        "\n",
        "    print('***** P_N   : ',P_N*is_pt   )\n",
        "    print('***** T_F   : ',T_F*is_pt   )\n",
        "    print('***** PT_F  : ',PT_F*is_pt*isPT_F )\n",
        "    print('***** T     : ',T           )\n",
        "\n",
        "    print('***** drop  : ',drop        )\n",
        "\n",
        "    return DeepSTN_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56kVKyKoUI5"
      },
      "source": [
        "# \"metrics.py\"\n",
        "\n",
        "# import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "# aliases\n",
        "mse = MSE = mean_squared_error\n",
        "# rmse = RMSE = root_mean_square_error\n",
        "\n",
        "\n",
        "def masked_mean_squared_error(y_true, y_pred):\n",
        "    idx = (y_true > 1e-6).nonzero()\n",
        "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "threshold=0.05\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-NOpElnMI"
      },
      "source": [
        "#ComparisionBikeNYC.py\n",
        "\n",
        "\n",
        "#from __future__ import print_function\n",
        "#from DATA.lzq_read_data_time_poi import lzq_load_data\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#import cPickle as pickle\n",
        "import numpy as np\n",
        "#import math\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "#from ipdb import set_trace\n",
        "#set_trace()\n",
        "\n",
        "#for GPU in Lab\n",
        "device=6\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(device)\n",
        "import tensorflow as tf  #from V1707\n",
        "config=tf.ConfigProto()  #from V1707\n",
        "config.gpu_options.allow_growth=True  #from V1707\n",
        "#config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
        "sess=tf.Session(config=config)  #from V1707\n",
        "#import keras.backend.tensorflow_backend as KTF\n",
        "#KTF._set_session(tf.Session(config=config))\n",
        "import setproctitle  #from V1707\n",
        "setproctitle.setproctitle('Comprison Start! @ ZiqianLin')  #from V1707\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "\n",
        "#hyperparameters\n",
        "epoch = 1#350  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0002  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 18,15,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 4*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=10\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "#DST result\n",
        "if XDST:\n",
        "    setproctitle.setproctitle('BJMobile DST @ ZiqianLin')  #from V1707\n",
        "\n",
        "    print(\"loading data...\")\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    R_N = 4   # number of residual units\n",
        "\n",
        "    from keras.optimizers import Adam\n",
        "    from DST_network.STResNet import stresnet\n",
        "    import DST_network.metrics as metrics\n",
        "\n",
        "    def build_model(external_dim,CFN):\n",
        "        c_conf = (len_closeness, channel, H, W) if len_closeness > 0 else None\n",
        "        p_conf = (len_period,    channel, H, W) if len_period    > 0 else None\n",
        "        t_conf = (len_trend,     channel, H, W) if len_trend     > 0 else None\n",
        "\n",
        "        model = stresnet(c_conf=c_conf, p_conf=p_conf, t_conf=t_conf,\n",
        "                         external_dim=external_dim, nb_residual_unit=R_N, CF=CFN)\n",
        "\n",
        "        adam = Adam(lr=lr)\n",
        "        model.compile(loss='mse', optimizer=adam, metrics=[metrics.rmse,metrics.mae])\n",
        "        model.summary()\n",
        "        #from keras.utils.visualize_util import plot\n",
        "        #plot(model, to_file='model.png', show_shapes=True)\n",
        "        return model\n",
        "\n",
        "\n",
        "    CF=64\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    import time\n",
        "\n",
        "    count=0\n",
        "\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        time_start=time.time()\n",
        "\n",
        "        F='DST_MODEL/dst_model_'+str(iterate)+'_.hdf5'\n",
        "\n",
        "        model = build_model(external_dim=False,CFN=CF)\n",
        "        if trainDST:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                filepath=F,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "                mode='min',\n",
        "                period=1)\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"training model...\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                nb_epoch=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('evaluating using the model that has the best loss on the valid set')\n",
        "        model.load_weights(F)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        score = model.evaluate(X_test, Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DST_SCORE/dst_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "\n",
        "        print('totally cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+ResPlus+PoI&Time\n",
        "if X11:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+ResPlus+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    index=np.arange(9)\n",
        "    P_train=P_train[:,index,:,:]\n",
        "    P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=True\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=True\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_11/MODEL/DeepSTN_11_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train11:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit([X_train,P_train,T_train], Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate([X_test ,P_test ,T_test ], Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_11/SCORE/DeepSTN_11_score3.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+ResPlus\n",
        "if X10:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+ResPlus @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    #index=np.arange(9)\n",
        "    #P_train=P_train[:,index,:,:]\n",
        "    #P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=True\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=False\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_10/MODEL/DeepSTN_10_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train10:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate(X_test,  Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_10/SCORE/DeepSTN_10_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+PoI&Time\n",
        "if X01:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    #index=np.arange(9)\n",
        "    #P_train=P_train[:,index,:,:]\n",
        "    #P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=False\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=True\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_01/MODEL/DeepSTN_01_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train01:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit([X_train,P_train,T_train], Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate([X_test, P_test, T_test ],  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_01/SCORE/DeepSTN_01_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN\n",
        "if X00:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    index=np.arange(9)\n",
        "    P_train=P_train[:,index,:,:]\n",
        "    P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=False\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=False\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train00:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#Comparison\n",
        "X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "print('MODEL                     RMSE  MAE')\n",
        "if 0:\n",
        "    print('ResNet                  :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DST_SCORE/dst_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    print(for_show)\n",
        "if 0:\n",
        "    print('DeepSTN                 :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_00/SCORE/DeepSTN_00_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 1:\n",
        "    print('DeepSTN+ResPlus         :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_10/SCORE/DeepSTN_10_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 0:\n",
        "    print('DeepSTN+PoI&Time        :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_01/SCORE/DeepSTN_01_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 1:\n",
        "    print('DeepSTN+ResPlus+PoI&Time:',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_11/SCORE/DeepSTN_11_score3.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}