{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV_EdNWClAq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55590aa6-30d4-419b-b088-57e35c6ad4de"
      },
      "source": [
        "# google drive connect\n",
        "Copied_path = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN' # Paste target directory here\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(Copied_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS2vPiaBYItm"
      },
      "source": [
        "# global\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "destination = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/data3/'\n",
        "file_list = [destination+'m0_Ent', destination+'m1_Col',destination+'m2_Ev', destination+'m3_Food', destination+'m4_Night', destination+'m5_Outdoor', destination+'m6_Pro', destination+'m7_Res', destination+'m8_Shop', destination+'m9_Tra']\n",
        "\n",
        "new_global_matrix = np.zeros_like(np.load(destination+'global_ori.npy'))\n",
        "\n",
        "#exclude_index = []\n",
        "exclude_index = [1,2,4,7]\n",
        "for i, file_name in enumerate(file_list) :\n",
        "  if i not in exclude_index :\n",
        "    matrix = np.load(file_name+'.npy')\n",
        "    new_global_matrix = new_global_matrix + matrix\n",
        "\n",
        "np.save(destination+'global', new_global_matrix)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/global.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AauHopRUnGYv"
      },
      "source": [
        "# m0_Ent\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m0_Ent.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgFX7E7fo_a8"
      },
      "source": [
        "# DST Network/ilayer.py\n",
        "\n",
        "from keras import backend as K\n",
        "#from keras.engine.topology import Layer\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "# from keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class iLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        # self.output_dim = output_dim\n",
        "        super(iLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        initial_weight_value = np.random.random(input_shape[1:])\n",
        "        self.W = K.variable(initial_weight_value)\n",
        "        #self.trainable_weights = [self.W]\n",
        "        self.trainable_weight = [self.W]\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        return x * self.W\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k925xkl8Uynj"
      },
      "source": [
        "# \"metrics.py\"\n",
        "\n",
        "# import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "# aliases\n",
        "mse = MSE = mean_squared_error\n",
        "# rmse = RMSE = root_mean_square_error\n",
        "\n",
        "\n",
        "def masked_mean_squared_error(y_true, y_pred):\n",
        "    idx = (y_true > 1e-6).nonzero()\n",
        "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "threshold=0.05\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8M9judzxQIB"
      },
      "source": [
        "# \"DeepSTN_net.py\"\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n",
        "from tensorflow.keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "#import PPT3_network.metrics as metrics\n",
        "\n",
        "# Relu-BN-Conv2D 3x3\n",
        "def conv_unit0(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    #print('kernel=(3,3)')\n",
        "    return unit_model\n",
        "\n",
        "# Relu-BN-Conv2D 1x1\n",
        "def conv_unit1(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    #print('kernel=(1,1)')\n",
        "    return unit_model\n",
        "\n",
        "# efficient version of Res_plus\n",
        "def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    #normal channels\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    #separated channels\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n",
        "    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n",
        "    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n",
        "\n",
        "    #merge\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# new resdual block\n",
        "def Res_plus(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n",
        "\n",
        "    cl_conv1B=plus_conv(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n",
        "\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# normal residual block\n",
        "def Res_normal(name,F,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "def cpt_slice(x, h1, h2):\n",
        "    return x[:,h1:h2,:,:]\n",
        "\n",
        "# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)\n",
        "def T_trans(T,T_F,H,W):\n",
        "\n",
        "    T_in=Input(shape=(T+7,H,W))\n",
        "    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n",
        "    T_act=Activation('relu')(T_mid)\n",
        "    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n",
        "    T_mul=Activation('relu')(T_fin)\n",
        "    T_model=Model(inputs=T_in,outputs=T_mul)\n",
        "\n",
        "    return T_model\n",
        "\n",
        "# transfer Time vector and PoI matrix to time-weighted PoI matrix\n",
        "def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n",
        "    if 1:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        if P_N>=2:\n",
        "            T_x0 =T_trans(T,T_F,H,W)(time_in)\n",
        "            T_x1 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=3:\n",
        "            T_x2 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=4:\n",
        "            T_x3 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=5:\n",
        "            T_x4 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=6:\n",
        "            T_x5 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=7:\n",
        "            T_x6 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=8:\n",
        "            T_x7 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=9:\n",
        "            T_x8 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=10:\n",
        "            T_x9 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=11:\n",
        "            T_x10=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=12:\n",
        "            T_x11=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=13:\n",
        "            T_x12=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=14:\n",
        "            T_x13=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=15:\n",
        "            T_x14=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=16:\n",
        "            T_x15=T_trans(T,T_F,H,W)(time_in)\n",
        "\n",
        "\n",
        "        if P_N==1:\n",
        "            T_x=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N==2:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1])\n",
        "        if P_N==3:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n",
        "        if P_N==4:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n",
        "        if P_N==5:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n",
        "        if P_N==6:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n",
        "        if P_N==7:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n",
        "        if P_N==8:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n",
        "        if P_N==9:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n",
        "        if P_N==10:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n",
        "        if P_N==11:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n",
        "        if P_N==12:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n",
        "        if P_N==13:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n",
        "        if P_N==14:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n",
        "        if P_N==15:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n",
        "        if P_N==16:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n",
        "\n",
        "        poi_time=Multiply()([poi_in,T_x])\n",
        "        if isPT_F:\n",
        "            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n",
        "            print('PT_F = YES')\n",
        "        else:\n",
        "            print('PT_F = NO')\n",
        "        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n",
        "\n",
        "        return PT_model\n",
        "\n",
        "# main model\n",
        "def DeepSTN(H=14,W=12,channel=1,      #H-map_height W-map_width channel-map_channel\n",
        "            c=3,p=4,t=4,              #c-closeness p-period t-trend\n",
        "            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n",
        "            is_plus=True,             #use ResPlus or mornal convolution\n",
        "            is_plus_efficient=False,  #use the efficient version of ResPlus\n",
        "            plus=8,rate=2,            #rate-pooling_rate\n",
        "            is_pt=False,               #use PoI and Time or not\n",
        "            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day\n",
        "            drop=0,\n",
        "            is_summary=False, #show detail\n",
        "            lr=0.0002,\n",
        "            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n",
        "            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n",
        "\n",
        "    all_channel = channel * (c+p+t)\n",
        "\n",
        "    cut0 = int( 0 )\n",
        "    cut1 = int( cut0 + channel*c )\n",
        "    cut2 = int( cut1 + channel*p )\n",
        "    cut3 = int( cut2 + channel*t )\n",
        "\n",
        "    cpt_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n",
        "    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n",
        "    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n",
        "\n",
        "    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n",
        "    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n",
        "    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n",
        "\n",
        "\n",
        "    cpt_global_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_global_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_global_input)\n",
        "    p_global_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_global_input)\n",
        "    t_global_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_global_input)\n",
        "\n",
        "    c_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_global_input)\n",
        "    p_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_global_input)\n",
        "    t_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_global_input)\n",
        "\n",
        "    if is_pt:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n",
        "\n",
        "        poi_time=PT_model([poi_in,time_in])\n",
        "\n",
        "        cpt_con1=Concatenate(axis=1)([c_global_out1,p_global_out1,t_global_out1,c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    else:\n",
        "        cpt_con1=Concatenate(axis=1)([c_global_out1,p_global_out1,t_global_out1,c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*6,conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*6,conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    if is_plus:\n",
        "        if is_plus_efficient:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "        else:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "\n",
        "    else:\n",
        "        for i in range(R_N):\n",
        "            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    cpt_conv2=Activation('relu')(cpt)\n",
        "    cpt_out2=BatchNormalization()(cpt_conv2)\n",
        "    cpt_conv1=Dropout(drop)(cpt_out2)\n",
        "    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n",
        "\n",
        "    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조\n",
        "\n",
        "    cpt_out1=Activation('tanh')(cpt_conv1)\n",
        "\n",
        "    if is_pt:\n",
        "        DeepSTN_model=Model(inputs=[cpt_global_input, cpt_input,poi_in,time_in],outputs=cpt_out1)\n",
        "    else:\n",
        "        DeepSTN_model=Model(inputs=[cpt_global_input, cpt_input],outputs=cpt_out1) # 여기에 글로벌 인풋도 추가\n",
        "\n",
        "    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[rmse,mae])\n",
        "\n",
        "    if is_summary:\n",
        "        DeepSTN_model.summary()\n",
        "\n",
        "    '''\n",
        "    print('***** pre_F : ',pre_F       )\n",
        "    print('***** conv_F: ',conv_F      )\n",
        "    print('***** R_N   : ',R_N         )\n",
        "\n",
        "    print('***** plus  : ',plus*is_plus)\n",
        "    print('***** rate  : ',rate*is_plus)\n",
        "\n",
        "    print('***** P_N   : ',P_N*is_pt   )\n",
        "    print('***** T_F   : ',T_F*is_pt   )\n",
        "    print('***** PT_F  : ',PT_F*is_pt*isPT_F )\n",
        "    print('***** T     : ',T           )\n",
        "\n",
        "    print('***** drop  : ',drop        )\n",
        "    '''\n",
        "    return DeepSTN_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02uqnK3dUwka",
        "outputId": "b5b6593e-42cd-4074-8e45-e5e512d25231"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 16,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=10\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "R_N=10\n",
        "\n",
        "is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 49.0  min= 0.0\n",
            "mean= -0.9962797619047619  variance= 0.02770920949471694\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 216.0  min= 0.0\n",
            "mean= -0.9874105420524694  variance= 0.059394225259970096\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000395 0.019006 0.002771]\n",
            "Test  score: [0.000267 0.016347 0.002122]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0021]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0016 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000271 0.015725 0.002694]\n",
            "Test  score: [0.000188 0.013706 0.00219 ]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0021]\n",
            " [0.0137 0.0022]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.003  0.0004]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000317 0.016987 0.002742]\n",
            "Test  score: [0.000222 0.014899 0.002202]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0021]\n",
            " [0.0137 0.0022]\n",
            " [0.0149 0.0022]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0045 0.0007]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000395 0.01901  0.002781]\n",
            "Test  score: [0.000268 0.016358 0.002132]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0021]\n",
            " [0.0137 0.0022]\n",
            " [0.0149 0.0022]\n",
            " [0.0164 0.0021]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0061 0.0009]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000312 0.016844 0.002683]\n",
            "Test  score: [0.000216 0.01471  0.002138]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0021]\n",
            " [0.0137 0.0022]\n",
            " [0.0149 0.0022]\n",
            " [0.0164 0.0021]\n",
            " [0.0147 0.0021]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0076 0.0011]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000274 0.015801 0.002775]\n",
            "Test  score: [0.000187 0.013665 0.002261]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0021]\n",
            " [0.0137 0.0022]\n",
            " [0.0149 0.0022]\n",
            " [0.0164 0.0021]\n",
            " [0.0147 0.0021]\n",
            " [0.0137 0.0023]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.009  0.0013]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000272 0.015722 0.002773]\n",
            "Test  score: [0.000186 0.013638 0.002264]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0021]\n",
            " [0.0137 0.0022]\n",
            " [0.0149 0.0022]\n",
            " [0.0164 0.0021]\n",
            " [0.0147 0.0021]\n",
            " [0.0137 0.0023]\n",
            " [0.0136 0.0023]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0103 0.0015]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000272 0.015739 0.002652]\n",
            "Test  score: [0.000184 0.013578 0.002128]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0021]\n",
            " [0.0137 0.0022]\n",
            " [0.0149 0.0022]\n",
            " [0.0164 0.0021]\n",
            " [0.0147 0.0021]\n",
            " [0.0137 0.0023]\n",
            " [0.0136 0.0023]\n",
            " [0.0136 0.0021]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0117 0.0017]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000286 0.016107 0.002702]\n",
            "Test  score: [0.000194 0.013929 0.002163]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0021]\n",
            " [0.0137 0.0022]\n",
            " [0.0149 0.0022]\n",
            " [0.0164 0.0021]\n",
            " [0.0147 0.0021]\n",
            " [0.0137 0.0023]\n",
            " [0.0136 0.0023]\n",
            " [0.0136 0.0021]\n",
            " [0.0139 0.0022]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0131 0.002 ]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000265 0.015568 0.002625]\n",
            "Test  score: [0.000183 0.013521 0.002119]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0021]\n",
            " [0.0137 0.0022]\n",
            " [0.0149 0.0022]\n",
            " [0.0164 0.0021]\n",
            " [0.0147 0.0021]\n",
            " [0.0137 0.0023]\n",
            " [0.0136 0.0023]\n",
            " [0.0136 0.0021]\n",
            " [0.0139 0.0022]\n",
            " [0.0135 0.0021]]\n",
            "RMSE  MAE\n",
            "[0.0144 0.0022]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UWmAFVqQOmT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DAlvdMfzOmQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "urDlDV9POmND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xaj-X8WDOmJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqA4u_X2OmAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTujgh28pvkw"
      },
      "source": [
        "# m1_Col\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "destination = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/data3/'\n",
        "file_list = [destination+'m0_Ent', destination+'m1_Col',destination+'m2_Ev', destination+'m3_Food', destination+'m4_Night', destination+'m5_Outdoor', destination+'m6_Pro', destination+'m7_Res', destination+'m8_Shop', destination+'m9_Tra']\n",
        "\n",
        "new_global_matrix = np.zeros_like(np.load(destination+'global_ori.npy'))\n",
        "\n",
        "#exclude_index = []\n",
        "exclude_index = [2, 4, 7]\n",
        "for i, file_name in enumerate(file_list) :\n",
        "  if i not in exclude_index :\n",
        "    matrix = np.load(file_name+'.npy')\n",
        "    new_global_matrix = new_global_matrix + matrix\n",
        "\n",
        "np.save(destination+'global', new_global_matrix)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m1_Col.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaareiVrtfkE",
        "outputId": "0dae86a6-5c6c-4bb6-eab0-8d6c1c5c4485"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 7.0  min= 0.0\n",
            "mean= -0.9955078124999998  variance= 0.04561460128187354\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 217.0  min= 0.0\n",
            "mean= -0.9873236487135179  variance= 0.05967185299694305\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001172 0.032245 0.003951]\n",
            "Test  score: [0.000799 0.028269 0.003149]\n",
            "RMSE  MAE\n",
            "[[0.0283 0.0031]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0028 0.0003]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00121  0.032706 0.003566]\n",
            "Test  score: [0.000825 0.028727 0.002751]\n",
            "RMSE  MAE\n",
            "[[0.0283 0.0031]\n",
            " [0.0287 0.0028]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0057 0.0006]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001243 0.033114 0.003376]\n",
            "Test  score: [0.000842 0.029014 0.002531]\n",
            "RMSE  MAE\n",
            "[[0.0283 0.0031]\n",
            " [0.0287 0.0028]\n",
            " [0.029  0.0025]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0086 0.0008]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001207 0.032635 0.003558]\n",
            "Test  score: [0.00082  0.028635 0.002726]\n",
            "RMSE  MAE\n",
            "[[0.0283 0.0031]\n",
            " [0.0287 0.0028]\n",
            " [0.029  0.0025]\n",
            " [0.0286 0.0027]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0115 0.0011]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001213 0.03274  0.003611]\n",
            "Test  score: [0.000811 0.028475 0.002743]\n",
            "RMSE  MAE\n",
            "[[0.0283 0.0031]\n",
            " [0.0287 0.0028]\n",
            " [0.029  0.0025]\n",
            " [0.0286 0.0027]\n",
            " [0.0285 0.0027]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0143 0.0014]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001207 0.032654 0.003602]\n",
            "Test  score: [0.000823 0.028684 0.002786]\n",
            "RMSE  MAE\n",
            "[[0.0283 0.0031]\n",
            " [0.0287 0.0028]\n",
            " [0.029  0.0025]\n",
            " [0.0286 0.0027]\n",
            " [0.0285 0.0027]\n",
            " [0.0287 0.0028]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0172 0.0017]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001153 0.032111 0.004145]\n",
            "Test  score: [0.0008   0.028276 0.003385]\n",
            "RMSE  MAE\n",
            "[[0.0283 0.0031]\n",
            " [0.0287 0.0028]\n",
            " [0.029  0.0025]\n",
            " [0.0286 0.0027]\n",
            " [0.0285 0.0027]\n",
            " [0.0287 0.0028]\n",
            " [0.0283 0.0034]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.02  0.002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001217 0.032795 0.003688]\n",
            "Test  score: [0.00083  0.028813 0.002861]\n",
            "RMSE  MAE\n",
            "[[0.0283 0.0031]\n",
            " [0.0287 0.0028]\n",
            " [0.029  0.0025]\n",
            " [0.0286 0.0027]\n",
            " [0.0285 0.0027]\n",
            " [0.0287 0.0028]\n",
            " [0.0283 0.0034]\n",
            " [0.0288 0.0029]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0229 0.0023]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001126 0.031817 0.004888]\n",
            "Test  score: [0.000776 0.027848 0.004118]\n",
            "RMSE  MAE\n",
            "[[0.0283 0.0031]\n",
            " [0.0287 0.0028]\n",
            " [0.029  0.0025]\n",
            " [0.0286 0.0027]\n",
            " [0.0285 0.0027]\n",
            " [0.0287 0.0028]\n",
            " [0.0283 0.0034]\n",
            " [0.0288 0.0029]\n",
            " [0.0278 0.0041]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0257 0.0027]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001239 0.033067 0.003381]\n",
            "Test  score: [0.000842 0.029015 0.002539]\n",
            "RMSE  MAE\n",
            "[[0.0283 0.0031]\n",
            " [0.0287 0.0028]\n",
            " [0.029  0.0025]\n",
            " [0.0286 0.0027]\n",
            " [0.0285 0.0027]\n",
            " [0.0287 0.0028]\n",
            " [0.0283 0.0034]\n",
            " [0.0288 0.0029]\n",
            " [0.0278 0.0041]\n",
            " [0.029  0.0025]]\n",
            "RMSE  MAE\n",
            "[0.0286 0.003 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Pu97ettf0M"
      },
      "source": [
        "# m2_Ev\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "destination = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/data3/'\n",
        "file_list = [destination+'m0_Ent', destination+'m1_Col',destination+'m2_Ev', destination+'m3_Food', destination+'m4_Night', destination+'m5_Outdoor', destination+'m6_Pro', destination+'m7_Res', destination+'m8_Shop', destination+'m9_Tra']\n",
        "\n",
        "new_global_matrix = np.zeros_like(np.load(destination+'global_ori.npy'))\n",
        "\n",
        "#exclude_index = []\n",
        "exclude_index = [1, 4, 7]\n",
        "for i, file_name in enumerate(file_list) :\n",
        "  if i not in exclude_index :\n",
        "    matrix = np.load(file_name+'.npy')\n",
        "    new_global_matrix = new_global_matrix + matrix\n",
        "\n",
        "np.save(destination+'global', new_global_matrix)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m2_Ev.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBowoAPctgES",
        "outputId": "6cc4bd0e-4ac1-4af2-c38c-ec67feab620a"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 2.0  min= 0.0\n",
            "mean= -0.9995963541666667  variance= 0.020408423180341476\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 216.0  min= 0.0\n",
            "mean= -0.9874068045910492  variance= 0.05941044977518067\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.009993 0.000331]\n",
            "Test  score: [0.000232 0.015246 0.000263]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0015 0.    ]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.010049 0.000342]\n",
            "Test  score: [0.000232 0.015243 0.000273]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.003  0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.010003 0.000327]\n",
            "Test  score: [0.000233 0.015248 0.000259]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0046 0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000302 0.010132 0.000365]\n",
            "Test  score: [0.000232 0.015241 0.000291]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0061 0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.009996 0.000325]\n",
            "Test  score: [0.000232 0.015247 0.000257]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0076 0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.010015 0.000321]\n",
            "Test  score: [0.000232 0.015246 0.000252]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0091 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.00999  0.000326]\n",
            "Test  score: [0.000232 0.015247 0.000257]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0107 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.009989 0.00032 ]\n",
            "Test  score: [0.000232 0.015247 0.000252]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0122 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000303 0.010059 0.000374]\n",
            "Test  score: [0.000233 0.015248 0.000304]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0137 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000302 0.010079 0.000358]\n",
            "Test  score: [0.000232 0.015243 0.000285]\n",
            "RMSE  MAE\n",
            "[[0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]\n",
            " [0.0152 0.0003]]\n",
            "RMSE  MAE\n",
            "[0.0152 0.0003]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYwKj5jjtgR_"
      },
      "source": [
        "# m3_Food\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "destination = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/data3/'\n",
        "file_list = [destination+'m0_Ent', destination+'m1_Col',destination+'m2_Ev', destination+'m3_Food', destination+'m4_Night', destination+'m5_Outdoor', destination+'m6_Pro', destination+'m7_Res', destination+'m8_Shop', destination+'m9_Tra']\n",
        "\n",
        "new_global_matrix = np.zeros_like(np.load(destination+'global_ori.npy'))\n",
        "\n",
        "#exclude_index = []\n",
        "exclude_index = [1, 2, 4, 7]\n",
        "for i, file_name in enumerate(file_list) :\n",
        "  if i not in exclude_index :\n",
        "    matrix = np.load(file_name+'.npy')\n",
        "    new_global_matrix = new_global_matrix + matrix\n",
        "\n",
        "np.save(destination+'global', new_global_matrix)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m3_Food.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhHEwFastgdY",
        "outputId": "304482cd-8a78-45f2-b938-b546a1447dde"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 98.0  min= 0.0\n",
            "mean= -0.9913027476615647  variance= 0.054585593962263146\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 216.0  min= 0.0\n",
            "mean= -0.9874105420524694  variance= 0.059394225259970096\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000707 0.025165 0.00551 ]\n",
            "Test  score: [0.000355 0.018835 0.003963]\n",
            "RMSE  MAE\n",
            "[[0.0188 0.004 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0019 0.0004]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000825 0.026976 0.005764]\n",
            "Test  score: [0.000375 0.019362 0.003996]\n",
            "RMSE  MAE\n",
            "[[0.0188 0.004 ]\n",
            " [0.0194 0.004 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0038 0.0008]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000733 0.025551 0.005452]\n",
            "Test  score: [0.000372 0.019298 0.003811]\n",
            "RMSE  MAE\n",
            "[[0.0188 0.004 ]\n",
            " [0.0194 0.004 ]\n",
            " [0.0193 0.0038]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0057 0.0012]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000737 0.025539 0.005587]\n",
            "Test  score: [0.000353 0.018794 0.003952]\n",
            "RMSE  MAE\n",
            "[[0.0188 0.004 ]\n",
            " [0.0194 0.004 ]\n",
            " [0.0193 0.0038]\n",
            " [0.0188 0.004 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0076 0.0016]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001107 0.031438 0.006019]\n",
            "Test  score: [0.000557 0.023611 0.004203]\n",
            "RMSE  MAE\n",
            "[[0.0188 0.004 ]\n",
            " [0.0194 0.004 ]\n",
            " [0.0193 0.0038]\n",
            " [0.0188 0.004 ]\n",
            " [0.0236 0.0042]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.01  0.002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000712 0.025337 0.005549]\n",
            "Test  score: [0.000368 0.019188 0.003978]\n",
            "RMSE  MAE\n",
            "[[0.0188 0.004 ]\n",
            " [0.0194 0.004 ]\n",
            " [0.0193 0.0038]\n",
            " [0.0188 0.004 ]\n",
            " [0.0236 0.0042]\n",
            " [0.0192 0.004 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0119 0.0024]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000687 0.02492  0.005444]\n",
            "Test  score: [0.000357 0.018898 0.003932]\n",
            "RMSE  MAE\n",
            "[[0.0188 0.004 ]\n",
            " [0.0194 0.004 ]\n",
            " [0.0193 0.0038]\n",
            " [0.0188 0.004 ]\n",
            " [0.0236 0.0042]\n",
            " [0.0192 0.004 ]\n",
            " [0.0189 0.0039]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0138 0.0028]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000744 0.026255 0.005769]\n",
            "Test  score: [0.000426 0.020645 0.004325]\n",
            "RMSE  MAE\n",
            "[[0.0188 0.004 ]\n",
            " [0.0194 0.004 ]\n",
            " [0.0193 0.0038]\n",
            " [0.0188 0.004 ]\n",
            " [0.0236 0.0042]\n",
            " [0.0192 0.004 ]\n",
            " [0.0189 0.0039]\n",
            " [0.0206 0.0043]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0159 0.0032]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000714 0.025273 0.005631]\n",
            "Test  score: [0.000359 0.018951 0.004054]\n",
            "RMSE  MAE\n",
            "[[0.0188 0.004 ]\n",
            " [0.0194 0.004 ]\n",
            " [0.0193 0.0038]\n",
            " [0.0188 0.004 ]\n",
            " [0.0236 0.0042]\n",
            " [0.0192 0.004 ]\n",
            " [0.0189 0.0039]\n",
            " [0.0206 0.0043]\n",
            " [0.019  0.0041]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0178 0.0036]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000736 0.025553 0.005645]\n",
            "Test  score: [0.000369 0.019205 0.004047]\n",
            "RMSE  MAE\n",
            "[[0.0188 0.004 ]\n",
            " [0.0194 0.004 ]\n",
            " [0.0193 0.0038]\n",
            " [0.0188 0.004 ]\n",
            " [0.0236 0.0042]\n",
            " [0.0192 0.004 ]\n",
            " [0.0189 0.0039]\n",
            " [0.0206 0.0043]\n",
            " [0.019  0.0041]\n",
            " [0.0192 0.004 ]]\n",
            "RMSE  MAE\n",
            "[0.0197 0.004 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwgQZLttgj7"
      },
      "source": [
        "# m4_Night\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "destination = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/data3/'\n",
        "file_list = [destination+'m0_Ent', destination+'m1_Col',destination+'m2_Ev', destination+'m3_Food', destination+'m4_Night', destination+'m5_Outdoor', destination+'m6_Pro', destination+'m7_Res', destination+'m8_Shop', destination+'m9_Tra']\n",
        "\n",
        "new_global_matrix = np.zeros_like(np.load(destination+'global_ori.npy'))\n",
        "\n",
        "#exclude_index = []\n",
        "exclude_index = [1, 2, 7]\n",
        "for i, file_name in enumerate(file_list) :\n",
        "  if i not in exclude_index :\n",
        "    matrix = np.load(file_name+'.npy')\n",
        "    new_global_matrix = new_global_matrix + matrix\n",
        "\n",
        "np.save(destination+'global', new_global_matrix)\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m4_Night.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fUmWkrptgqb",
        "outputId": "76586db7-1bd0-460b-d8b9-7747b3746912"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 13.0  min= 0.0\n",
            "mean= -0.9996183894230768  variance= 0.010054515338544382\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 217.0  min= 0.0\n",
            "mean= -0.9874456965245777  variance= 0.059223009482505286\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000116 0.007136 0.000392]\n",
            "Test  score: [0.000039 0.006256 0.0003  ]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0006 0.    ]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000116 0.007153 0.000328]\n",
            "Test  score: [0.00004  0.006294 0.000237]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0013 0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000116 0.007151 0.000342]\n",
            "Test  score: [0.00004  0.006285 0.000251]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.0063 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0019 0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000116 0.007153 0.000348]\n",
            "Test  score: [0.00004  0.006291 0.000256]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0025 0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000115 0.007152 0.000385]\n",
            "Test  score: [0.000039 0.006269 0.000291]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0031 0.0001]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000115 0.007138 0.000359]\n",
            "Test  score: [0.000039 0.006274 0.000265]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0038 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000116 0.007157 0.000351]\n",
            "Test  score: [0.00004  0.006294 0.000259]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0044 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000115 0.007149 0.000359]\n",
            "Test  score: [0.000039 0.006279 0.000267]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.005  0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000115 0.007133 0.000363]\n",
            "Test  score: [0.000039 0.006239 0.000272]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0062 0.0003]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0056 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000115 0.00712  0.000411]\n",
            "Test  score: [0.000039 0.006229 0.00032 ]\n",
            "RMSE  MAE\n",
            "[[0.0063 0.0003]\n",
            " [0.0063 0.0002]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0063 0.0003]\n",
            " [0.0062 0.0003]\n",
            " [0.0062 0.0003]]\n",
            "RMSE  MAE\n",
            "[0.0063 0.0003]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJBT50u6tgxG"
      },
      "source": [
        "# m5_Outdoor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "destination = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/data3/'\n",
        "file_list = [destination+'m0_Ent', destination+'m1_Col',destination+'m2_Ev', destination+'m3_Food', destination+'m4_Night', destination+'m5_Outdoor', destination+'m6_Pro', destination+'m7_Res', destination+'m8_Shop', destination+'m9_Tra']\n",
        "\n",
        "new_global_matrix = np.zeros_like(np.load(destination+'global_ori.npy'))\n",
        "\n",
        "#exclude_index = []\n",
        "exclude_index = [1, 2, 4, 7]\n",
        "for i, file_name in enumerate(file_list) :\n",
        "  if i not in exclude_index :\n",
        "    matrix = np.load(file_name+'.npy')\n",
        "    new_global_matrix = new_global_matrix + matrix\n",
        "\n",
        "np.save(destination+'global', new_global_matrix)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m5_Outdoor.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDucuOaUtg4C",
        "outputId": "615fbe43-93f9-4b62-c150-dc11522c013d"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 48.0  min= 0.0\n",
            "mean= -0.9933797200520835  variance= 0.03452679972215169\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 216.0  min= 0.0\n",
            "mean= -0.9874105420524694  variance= 0.059394225259970096\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00057  0.021446 0.004903]\n",
            "Test  score: [0.000209 0.014464 0.003322]\n",
            "RMSE  MAE\n",
            "[[0.0145 0.0033]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0014 0.0003]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000611 0.022171 0.004895]\n",
            "Test  score: [0.000215 0.014659 0.003251]\n",
            "RMSE  MAE\n",
            "[[0.0145 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0029 0.0007]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000611 0.022167 0.004947]\n",
            "Test  score: [0.000215 0.014668 0.003298]\n",
            "RMSE  MAE\n",
            "[[0.0145 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0044 0.001 ]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000579 0.02154  0.004914]\n",
            "Test  score: [0.000208 0.014421 0.003334]\n",
            "RMSE  MAE\n",
            "[[0.0145 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0144 0.0033]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0058 0.0013]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000608 0.02215  0.004775]\n",
            "Test  score: [0.000215 0.014671 0.003082]\n",
            "RMSE  MAE\n",
            "[[0.0145 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0144 0.0033]\n",
            " [0.0147 0.0031]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0073 0.0016]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000718 0.024223 0.00492 ]\n",
            "Test  score: [0.000261 0.016148 0.003163]\n",
            "RMSE  MAE\n",
            "[[0.0145 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0144 0.0033]\n",
            " [0.0147 0.0031]\n",
            " [0.0161 0.0032]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0089 0.0019]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00061  0.022221 0.004748]\n",
            "Test  score: [0.000218 0.014776 0.003063]\n",
            "RMSE  MAE\n",
            "[[0.0145 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0144 0.0033]\n",
            " [0.0147 0.0031]\n",
            " [0.0161 0.0032]\n",
            " [0.0148 0.0031]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0104 0.0023]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000569 0.021356 0.004951]\n",
            "Test  score: [0.000205 0.014301 0.003379]\n",
            "RMSE  MAE\n",
            "[[0.0145 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0144 0.0033]\n",
            " [0.0147 0.0031]\n",
            " [0.0161 0.0032]\n",
            " [0.0148 0.0031]\n",
            " [0.0143 0.0034]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0118 0.0026]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000562 0.021354 0.004951]\n",
            "Test  score: [0.000206 0.014358 0.00339 ]\n",
            "RMSE  MAE\n",
            "[[0.0145 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0144 0.0033]\n",
            " [0.0147 0.0031]\n",
            " [0.0161 0.0032]\n",
            " [0.0148 0.0031]\n",
            " [0.0143 0.0034]\n",
            " [0.0144 0.0034]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0132 0.0029]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00059  0.021716 0.004932]\n",
            "Test  score: [0.000207 0.014404 0.003323]\n",
            "RMSE  MAE\n",
            "[[0.0145 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0147 0.0033]\n",
            " [0.0144 0.0033]\n",
            " [0.0147 0.0031]\n",
            " [0.0161 0.0032]\n",
            " [0.0148 0.0031]\n",
            " [0.0143 0.0034]\n",
            " [0.0144 0.0034]\n",
            " [0.0144 0.0033]]\n",
            "RMSE  MAE\n",
            "[0.0147 0.0033]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxHVvubStg-d"
      },
      "source": [
        "# m6_Pro\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "destination = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/data3/'\n",
        "file_list = [destination+'m0_Ent', destination+'m1_Col',destination+'m2_Ev', destination+'m3_Food', destination+'m4_Night', destination+'m5_Outdoor', destination+'m6_Pro', destination+'m7_Res', destination+'m8_Shop', destination+'m9_Tra']\n",
        "\n",
        "new_global_matrix = np.zeros_like(np.load(destination+'global_ori.npy'))\n",
        "\n",
        "#exclude_index = []\n",
        "exclude_index = [1, 2, 4, 7]\n",
        "for i, file_name in enumerate(file_list) :\n",
        "  if i not in exclude_index :\n",
        "    matrix = np.load(file_name+'.npy')\n",
        "    new_global_matrix = new_global_matrix + matrix\n",
        "\n",
        "np.save(destination+'global', new_global_matrix)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m6_Pro.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS2cSMb8thFU",
        "outputId": "04d426dd-55c6-4c9a-8ddd-decbd0fe5240"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "'''\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 68.0  min= 0.0\n",
            "mean= -0.9916000306372551  variance= 0.04181876512912886\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 216.0  min= 0.0\n",
            "mean= -0.9874105420524694  variance= 0.059394225259970096\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000709 0.024766 0.005987]\n",
            "Test  score: [0.000332 0.018217 0.004458]\n",
            "RMSE  MAE\n",
            "[[0.0182 0.0045]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0018 0.0004]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000677 0.024355 0.006139]\n",
            "Test  score: [0.000319 0.017868 0.004629]\n",
            "RMSE  MAE\n",
            "[[0.0182 0.0045]\n",
            " [0.0179 0.0046]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0036 0.0009]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000672 0.024269 0.006134]\n",
            "Test  score: [0.000323 0.017984 0.004648]\n",
            "RMSE  MAE\n",
            "[[0.0182 0.0045]\n",
            " [0.0179 0.0046]\n",
            " [0.018  0.0046]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0054 0.0014]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000686 0.024514 0.006139]\n",
            "Test  score: [0.000322 0.017949 0.00465 ]\n",
            "RMSE  MAE\n",
            "[[0.0182 0.0045]\n",
            " [0.0179 0.0046]\n",
            " [0.018  0.0046]\n",
            " [0.0179 0.0046]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0072 0.0018]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000715 0.024627 0.006376]\n",
            "Test  score: [0.000317 0.017796 0.004993]\n",
            "RMSE  MAE\n",
            "[[0.0182 0.0045]\n",
            " [0.0179 0.0046]\n",
            " [0.018  0.0046]\n",
            " [0.0179 0.0046]\n",
            " [0.0178 0.005 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.009  0.0023]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000689 0.024488 0.006192]\n",
            "Test  score: [0.000317 0.017809 0.004712]\n",
            "RMSE  MAE\n",
            "[[0.0182 0.0045]\n",
            " [0.0179 0.0046]\n",
            " [0.018  0.0046]\n",
            " [0.0179 0.0046]\n",
            " [0.0178 0.005 ]\n",
            " [0.0178 0.0047]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0108 0.0028]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000755 0.025407 0.00608 ]\n",
            "Test  score: [0.000365 0.019104 0.00464 ]\n",
            "RMSE  MAE\n",
            "[[0.0182 0.0045]\n",
            " [0.0179 0.0046]\n",
            " [0.018  0.0046]\n",
            " [0.0179 0.0046]\n",
            " [0.0178 0.005 ]\n",
            " [0.0178 0.0047]\n",
            " [0.0191 0.0046]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0127 0.0033]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000696 0.024487 0.006402]\n",
            "Test  score: [0.000319 0.017862 0.005003]\n",
            "RMSE  MAE\n",
            "[[0.0182 0.0045]\n",
            " [0.0179 0.0046]\n",
            " [0.018  0.0046]\n",
            " [0.0179 0.0046]\n",
            " [0.0178 0.005 ]\n",
            " [0.0178 0.0047]\n",
            " [0.0191 0.0046]\n",
            " [0.0179 0.005 ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0145 0.0038]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000682 0.024247 0.006336]\n",
            "Test  score: [0.000315 0.017744 0.004914]\n",
            "RMSE  MAE\n",
            "[[0.0182 0.0045]\n",
            " [0.0179 0.0046]\n",
            " [0.018  0.0046]\n",
            " [0.0179 0.0046]\n",
            " [0.0178 0.005 ]\n",
            " [0.0178 0.0047]\n",
            " [0.0191 0.0046]\n",
            " [0.0179 0.005 ]\n",
            " [0.0177 0.0049]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0162 0.0043]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000653 0.024031 0.006244]\n",
            "Test  score: [0.000305 0.017466 0.004775]\n",
            "RMSE  MAE\n",
            "[[0.0182 0.0045]\n",
            " [0.0179 0.0046]\n",
            " [0.018  0.0046]\n",
            " [0.0179 0.0046]\n",
            " [0.0178 0.005 ]\n",
            " [0.0178 0.0047]\n",
            " [0.0191 0.0046]\n",
            " [0.0179 0.005 ]\n",
            " [0.0177 0.0049]\n",
            " [0.0175 0.0048]]\n",
            "RMSE  MAE\n",
            "[0.018  0.0047]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4_IdDnLthH3"
      },
      "source": [
        "# m7_Res\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "destination = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/data3/'\n",
        "file_list = [destination+'m0_Ent', destination+'m1_Col',destination+'m2_Ev', destination+'m3_Food', destination+'m4_Night', destination+'m5_Outdoor', destination+'m6_Pro', destination+'m7_Res', destination+'m8_Shop', destination+'m9_Tra']\n",
        "\n",
        "new_global_matrix = np.zeros_like(np.load(destination+'global_ori.npy'))\n",
        "\n",
        "#exclude_index = []\n",
        "exclude_index = [1, 2, 4]\n",
        "for i, file_name in enumerate(file_list) :\n",
        "  if i not in exclude_index :\n",
        "    matrix = np.load(file_name+'.npy')\n",
        "    new_global_matrix = new_global_matrix + matrix\n",
        "\n",
        "np.save(destination+'global', new_global_matrix)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m7_Res.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2aFCbbgthOl",
        "outputId": "348ce658-7e39-45c0-b88f-1bd694c712ee"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 4.0  min= 0.0\n",
            "mean= -0.997197265625  variance= 0.03973737211395832\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 216.0  min= 0.0\n",
            "mean= -0.9873586395640431  variance= 0.059495167636171145\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001204 0.033025 0.002492]\n",
            "Test  score: [0.000634 0.025171 0.001613]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0016]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0025 0.0002]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00122  0.033238 0.002214]\n",
            "Test  score: [0.000639 0.025283 0.001327]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0016]\n",
            " [0.0253 0.0013]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.005  0.0003]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001218 0.033209 0.002249]\n",
            "Test  score: [0.000638 0.02525  0.00136 ]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0016]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0014]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0076 0.0004]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001203 0.033009 0.002616]\n",
            "Test  score: [0.000631 0.025121 0.00173 ]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0016]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0014]\n",
            " [0.0251 0.0017]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0101 0.0006]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001219 0.033234 0.002251]\n",
            "Test  score: [0.000639 0.025284 0.001361]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0016]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0014]\n",
            " [0.0251 0.0017]\n",
            " [0.0253 0.0014]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0126 0.0007]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001213 0.033147 0.002458]\n",
            "Test  score: [0.00064  0.025306 0.001583]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0016]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0014]\n",
            " [0.0251 0.0017]\n",
            " [0.0253 0.0014]\n",
            " [0.0253 0.0016]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0151 0.0009]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001214 0.033153 0.002316]\n",
            "Test  score: [0.000635 0.025191 0.001424]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0016]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0014]\n",
            " [0.0251 0.0017]\n",
            " [0.0253 0.0014]\n",
            " [0.0253 0.0016]\n",
            " [0.0252 0.0014]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0177 0.001 ]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.00122  0.033241 0.002233]\n",
            "Test  score: [0.000639 0.025283 0.001345]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0016]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0014]\n",
            " [0.0251 0.0017]\n",
            " [0.0253 0.0014]\n",
            " [0.0253 0.0016]\n",
            " [0.0252 0.0014]\n",
            " [0.0253 0.0013]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0202 0.0012]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001216 0.033198 0.002253]\n",
            "Test  score: [0.00064  0.025292 0.00137 ]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0016]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0014]\n",
            " [0.0251 0.0017]\n",
            " [0.0253 0.0014]\n",
            " [0.0253 0.0016]\n",
            " [0.0252 0.0014]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0014]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0227 0.0013]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.001209 0.033096 0.002429]\n",
            "Test  score: [0.000633 0.025164 0.001542]\n",
            "RMSE  MAE\n",
            "[[0.0252 0.0016]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0014]\n",
            " [0.0251 0.0017]\n",
            " [0.0253 0.0014]\n",
            " [0.0253 0.0016]\n",
            " [0.0252 0.0014]\n",
            " [0.0253 0.0013]\n",
            " [0.0253 0.0014]\n",
            " [0.0252 0.0015]]\n",
            "RMSE  MAE\n",
            "[0.0252 0.0015]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMzzSXycthU3"
      },
      "source": [
        "# m8_Shop\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "destination = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/data3/'\n",
        "file_list = [destination+'m0_Ent', destination+'m1_Col',destination+'m2_Ev', destination+'m3_Food', destination+'m4_Night', destination+'m5_Outdoor', destination+'m6_Pro', destination+'m7_Res', destination+'m8_Shop', destination+'m9_Tra']\n",
        "\n",
        "new_global_matrix = np.zeros_like(np.load(destination+'global_ori.npy'))\n",
        "\n",
        "#exclude_index = []\n",
        "exclude_index = [1, 2, 4, 7]\n",
        "for i, file_name in enumerate(file_list) :\n",
        "  if i not in exclude_index :\n",
        "    matrix = np.load(file_name+'.npy')\n",
        "    new_global_matrix = new_global_matrix + matrix\n",
        "\n",
        "np.save(destination+'global', new_global_matrix)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m8_Shop.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k9HNB1-thdM",
        "outputId": "fa7fe9c9-7a5e-4e71-9533-b77c01efa789"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 60.0  min= 0.0\n",
            "mean= -0.9908684895833333  variance= 0.04447277708632065\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 216.0  min= 0.0\n",
            "mean= -0.9874105420524694  variance= 0.059394225259970096\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000715 0.025959 0.007104]\n",
            "Test  score: [0.000388 0.019694 0.005365]\n",
            "RMSE  MAE\n",
            "[[0.0197 0.0054]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.002  0.0005]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000649 0.024794 0.006951]\n",
            "Test  score: [0.000355 0.018846 0.005321]\n",
            "RMSE  MAE\n",
            "[[0.0197 0.0054]\n",
            " [0.0188 0.0053]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0039 0.0011]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000728 0.026233 0.006659]\n",
            "Test  score: [0.000413 0.020335 0.004911]\n",
            "RMSE  MAE\n",
            "[[0.0197 0.0054]\n",
            " [0.0188 0.0053]\n",
            " [0.0203 0.0049]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0059 0.0016]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000662 0.025071 0.00668 ]\n",
            "Test  score: [0.000366 0.01914  0.004937]\n",
            "RMSE  MAE\n",
            "[[0.0197 0.0054]\n",
            " [0.0188 0.0053]\n",
            " [0.0203 0.0049]\n",
            " [0.0191 0.0049]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0078 0.0021]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000645 0.024801 0.006935]\n",
            "Test  score: [0.000363 0.019056 0.005306]\n",
            "RMSE  MAE\n",
            "[[0.0197 0.0054]\n",
            " [0.0188 0.0053]\n",
            " [0.0203 0.0049]\n",
            " [0.0191 0.0049]\n",
            " [0.0191 0.0053]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0097 0.0026]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000643 0.024746 0.006848]\n",
            "Test  score: [0.000348 0.018662 0.005099]\n",
            "RMSE  MAE\n",
            "[[0.0197 0.0054]\n",
            " [0.0188 0.0053]\n",
            " [0.0203 0.0049]\n",
            " [0.0191 0.0049]\n",
            " [0.0191 0.0053]\n",
            " [0.0187 0.0051]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0116 0.0031]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000678 0.025377 0.006571]\n",
            "Test  score: [0.000355 0.01884  0.004711]\n",
            "RMSE  MAE\n",
            "[[0.0197 0.0054]\n",
            " [0.0188 0.0053]\n",
            " [0.0203 0.0049]\n",
            " [0.0191 0.0049]\n",
            " [0.0191 0.0053]\n",
            " [0.0187 0.0051]\n",
            " [0.0188 0.0047]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0135 0.0036]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000686 0.025421 0.006953]\n",
            "Test  score: [0.000371 0.019273 0.00523 ]\n",
            "RMSE  MAE\n",
            "[[0.0197 0.0054]\n",
            " [0.0188 0.0053]\n",
            " [0.0203 0.0049]\n",
            " [0.0191 0.0049]\n",
            " [0.0191 0.0053]\n",
            " [0.0187 0.0051]\n",
            " [0.0188 0.0047]\n",
            " [0.0193 0.0052]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0154 0.0041]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000667 0.025132 0.007048]\n",
            "Test  score: [0.000362 0.019024 0.00535 ]\n",
            "RMSE  MAE\n",
            "[[0.0197 0.0054]\n",
            " [0.0188 0.0053]\n",
            " [0.0203 0.0049]\n",
            " [0.0191 0.0049]\n",
            " [0.0191 0.0053]\n",
            " [0.0187 0.0051]\n",
            " [0.0188 0.0047]\n",
            " [0.0193 0.0052]\n",
            " [0.019  0.0053]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0173 0.0046]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000655 0.02493  0.007104]\n",
            "Test  score: [0.000363 0.019065 0.005427]\n",
            "RMSE  MAE\n",
            "[[0.0197 0.0054]\n",
            " [0.0188 0.0053]\n",
            " [0.0203 0.0049]\n",
            " [0.0191 0.0049]\n",
            " [0.0191 0.0053]\n",
            " [0.0187 0.0051]\n",
            " [0.0188 0.0047]\n",
            " [0.0193 0.0052]\n",
            " [0.019  0.0053]\n",
            " [0.0191 0.0054]]\n",
            "RMSE  MAE\n",
            "[0.0192 0.0052]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILgnq8L2pvvw"
      },
      "source": [
        "# m9_Tra\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "destination = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/data3/'\n",
        "file_list = [destination+'m0_Ent', destination+'m1_Col',destination+'m2_Ev', destination+'m3_Food', destination+'m4_Night', destination+'m5_Outdoor', destination+'m6_Pro', destination+'m7_Res', destination+'m8_Shop', destination+'m9_Tra']\n",
        "\n",
        "new_global_matrix = np.zeros_like(np.load(destination+'global_ori.npy'))\n",
        "\n",
        "#exclude_index = []\n",
        "exclude_index = [1, 2, 4, 7]\n",
        "for i, file_name in enumerate(file_list) :\n",
        "  if i not in exclude_index :\n",
        "    matrix = np.load(file_name+'.npy')\n",
        "    new_global_matrix = new_global_matrix + matrix\n",
        "\n",
        "np.save(destination+'global', new_global_matrix)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MM:\n",
        "    def __init__(self,MM_max,MM_min):\n",
        "        self.max=MM_max\n",
        "        self.min=MM_min\n",
        "\n",
        "# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n",
        "def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n",
        "\n",
        "    all_data=np.load('data3/m9_Tra.npy')\n",
        "    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n",
        "    len_total,feature,map_height,map_width=all_data.shape\n",
        "    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n",
        "    #len_total,feature,map_height,map_width=all_data.shape\n",
        "    print('all_data shape: ',all_data.shape)\n",
        "    #mm=MM(np.max(all_data),np.min(all_data))\n",
        "    print('max=',np.max(all_data),' min=',np.min(all_data))\n",
        "\n",
        "    #for time\n",
        "    time=np.arange(len_total,dtype=int)\n",
        "    #hour\n",
        "    time_hour=time%T_period\n",
        "    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n",
        "    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n",
        "    for i in range(len_total):\n",
        "        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #day\n",
        "    time_day=(time//T_period)%7\n",
        "    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n",
        "    for i in range(len_total):\n",
        "        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n",
        "    #con\n",
        "    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n",
        "\n",
        "    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n",
        "    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n",
        "\n",
        "    if len_trend>0:\n",
        "        number_of_skip_hours=T_trend*len_trend\n",
        "    elif len_period>0:\n",
        "        number_of_skip_hours=T_period*len_period\n",
        "    elif len_closeness>0:\n",
        "        number_of_skip_hours=T_closeness*len_closeness\n",
        "    else:\n",
        "        print(\"wrong\")\n",
        "    print('number_of_skip_hours:',number_of_skip_hours)\n",
        "\n",
        "    Y=all_data[number_of_skip_hours:len_total]\n",
        "\n",
        "    if len_closeness>0:\n",
        "        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n",
        "        for i in range(len_closeness-1):\n",
        "            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n",
        "    if len_period>0:\n",
        "        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n",
        "        for i in range(len_period-1):\n",
        "            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n",
        "    if len_trend>0:\n",
        "        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n",
        "        for i in range(len_trend-1):\n",
        "            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n",
        "\n",
        "    matrix_T=matrix_T[number_of_skip_hours:]\n",
        "\n",
        "    X_closeness_train=X_closeness[:-len_test]\n",
        "    X_period_train=X_period[:-len_test]\n",
        "    X_trend_train=X_trend[:-len_test]\n",
        "    T_train=matrix_T[:-len_test]\n",
        "    X_closeness_test=X_closeness[-len_test:]\n",
        "    X_period_test=X_period[-len_test:]\n",
        "    X_trend_test=X_trend[-len_test:]\n",
        "    T_test=matrix_T[-len_test:]\n",
        "\n",
        "    X_train=[X_closeness_train,X_period_train,X_trend_train]\n",
        "    X_test=[X_closeness_test,X_period_test,X_trend_test]\n",
        "    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n",
        "    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n",
        "    Y_train=Y[:-len_test]\n",
        "    Y_test=Y[-len_test:]\n",
        "\n",
        "    len_train=X_closeness_train.shape[0]\n",
        "    len_test=X_closeness_test.shape[0]\n",
        "    print('len_train='+str(len_train))\n",
        "    print('len_test ='+str(len_test ))\n",
        "\n",
        "    '''\n",
        "    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n",
        "    for i in range(poi.shape[0]):\n",
        "        poi[i]=poi[i]/np.max(poi[i])\n",
        "    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n",
        "    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n",
        "\n",
        "    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n",
        "    '''\n",
        "    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOaxNvj2pv6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd1d850-0d86-4a53-92e9-e2fd8ea40150"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf  #from V1707\n",
        "from keras import backend as K\n",
        "\n",
        "'''\n",
        "#hyperparameters\n",
        "epoch = 100  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0001  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 14,12,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 8*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=1\n",
        "\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n",
        "X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n",
        "\n",
        "index=np.arange(9)\n",
        "#P_train=P_train[:,index,:,:]\n",
        "#P_test =P_test [:,index,:,:]\n",
        "'''\n",
        "pre_F=64\n",
        "conv_F=64\n",
        "#R_N=10\n",
        "\n",
        "#is_plus=True\n",
        "plus=8\n",
        "rate=1\n",
        "\n",
        "is_pt=False\n",
        "P_N=9\n",
        "T_F=7*8\n",
        "PT_F=9\n",
        "\n",
        "drop=0.1\n",
        "'''\n",
        "import time\n",
        "count=0\n",
        "count_sum=iterate_num\n",
        "\n",
        "iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "RMSE=np.zeros([iterate_num,1])\n",
        "MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "for iterate_index in range(iterate_num):\n",
        "    count=count+1\n",
        "    time_start=time.time()\n",
        "    iterate=iterate_loop[iterate_index]\n",
        "\n",
        "    #print(\"***** conv_model *****\")\n",
        "    model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                  c=len_closeness,p=len_period,\n",
        "                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                  is_plus=is_plus,\n",
        "                  plus=plus,rate=rate,\n",
        "                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                  drop=drop)\n",
        "\n",
        "    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "    #train conv_model\n",
        "\n",
        "    if train00:\n",
        "        '''\n",
        "        model_checkpoint=ModelCheckpoint(\n",
        "                filepath=file_conv,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                mode='min',\n",
        "                period=1\n",
        "            )\n",
        "        '''\n",
        "        #print('=' * 10)\n",
        "        #print(\"***** training conv_model *****\")\n",
        "        history = model.fit((X_train_global, X_train), Y_train,\n",
        "                            epochs=epoch,\n",
        "                            batch_size=batch_size,\n",
        "                            validation_split=0.1,\n",
        "                            #callbacks=[model_checkpoint],\n",
        "                            verbose=0,\n",
        "                            #validation_data=((X_test_global, X_test), Y_test)\n",
        "                            )\n",
        "\n",
        "    #print('=' * 10)\n",
        "    #print('***** evaluate *****')\n",
        "    #model.load_weights(file_conv)\n",
        "\n",
        "    #print(X_train_global.shape, X_train.shape)\n",
        "\n",
        "    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "\n",
        "    print('              mse     rmse    mae')\n",
        "    print('Train score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "    #print(X_test_global.shape, X_test.shape)\n",
        "    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "    print('Test  score:',end=' ')\n",
        "    np.set_printoptions(precision=6, suppress=True)\n",
        "    print(np.array(score))\n",
        "\n",
        "\n",
        "\n",
        "    RMSE[iterate_index,0]=score[1]\n",
        "    MAE [iterate_index,0]=score[2]\n",
        "\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "    np.set_printoptions(precision=4, suppress=True)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print('RMSE  MAE')\n",
        "    print(for_show)\n",
        "\n",
        "    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "    '''\n",
        "    time_end=time.time()\n",
        "    print('iterate cost',time_end-time_start)\n",
        "    print(str(count)+'/'+str(count_sum))\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 37.0  min= 0.0\n",
            "mean= -0.9933016610360359  variance= 0.037008620307463456\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "all_data shape:  (800, 1, 16, 12)\n",
            "max= 216.0  min= 0.0\n",
            "mean= -0.9874105420524694  variance= 0.059394225259970096\n",
            "number_of_skip_hours: 224\n",
            "len_train=464\n",
            "len_test =112\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000491 0.02094  0.005171]\n",
            "Test  score: [0.000266 0.01632  0.004056]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0041]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0016 0.0004]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000501 0.021174 0.004638]\n",
            "Test  score: [0.00028  0.016725 0.003445]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0041]\n",
            " [0.0167 0.0034]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0033 0.0008]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000515 0.021402 0.004688]\n",
            "Test  score: [0.000277 0.016647 0.003491]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0041]\n",
            " [0.0167 0.0034]\n",
            " [0.0166 0.0035]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.005  0.0011]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000501 0.021177 0.004849]\n",
            "Test  score: [0.00027  0.016444 0.003686]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0041]\n",
            " [0.0167 0.0034]\n",
            " [0.0166 0.0035]\n",
            " [0.0164 0.0037]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0066 0.0015]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000508 0.021297 0.004522]\n",
            "Test  score: [0.000282 0.016785 0.003306]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0041]\n",
            " [0.0167 0.0034]\n",
            " [0.0166 0.0035]\n",
            " [0.0164 0.0037]\n",
            " [0.0168 0.0033]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0083 0.0018]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000497 0.02112  0.004621]\n",
            "Test  score: [0.000272 0.016507 0.003454]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0041]\n",
            " [0.0167 0.0034]\n",
            " [0.0166 0.0035]\n",
            " [0.0164 0.0037]\n",
            " [0.0168 0.0033]\n",
            " [0.0165 0.0035]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0099 0.0021]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000487 0.020922 0.004672]\n",
            "Test  score: [0.000271 0.01645  0.00352 ]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0041]\n",
            " [0.0167 0.0034]\n",
            " [0.0166 0.0035]\n",
            " [0.0164 0.0037]\n",
            " [0.0168 0.0033]\n",
            " [0.0165 0.0035]\n",
            " [0.0164 0.0035]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0116 0.0025]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000518 0.021419 0.00474 ]\n",
            "Test  score: [0.00028  0.016721 0.003532]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0041]\n",
            " [0.0167 0.0034]\n",
            " [0.0166 0.0035]\n",
            " [0.0164 0.0037]\n",
            " [0.0168 0.0033]\n",
            " [0.0165 0.0035]\n",
            " [0.0164 0.0035]\n",
            " [0.0167 0.0035]\n",
            " [0.     0.    ]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0133 0.0028]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000524 0.02157  0.004575]\n",
            "Test  score: [0.000279 0.016698 0.003376]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0041]\n",
            " [0.0167 0.0034]\n",
            " [0.0166 0.0035]\n",
            " [0.0164 0.0037]\n",
            " [0.0168 0.0033]\n",
            " [0.0165 0.0035]\n",
            " [0.0164 0.0035]\n",
            " [0.0167 0.0035]\n",
            " [0.0167 0.0034]\n",
            " [0.     0.    ]]\n",
            "RMSE  MAE\n",
            "[0.0149 0.0032]\n",
            "              mse     rmse    mae\n",
            "Train score: [0.000795 0.026358 0.004917]\n",
            "Test  score: [0.000409 0.020217 0.00354 ]\n",
            "RMSE  MAE\n",
            "[[0.0163 0.0041]\n",
            " [0.0167 0.0034]\n",
            " [0.0166 0.0035]\n",
            " [0.0164 0.0037]\n",
            " [0.0168 0.0033]\n",
            " [0.0165 0.0035]\n",
            " [0.0164 0.0035]\n",
            " [0.0167 0.0035]\n",
            " [0.0167 0.0034]\n",
            " [0.0202 0.0035]]\n",
            "RMSE  MAE\n",
            "[0.017  0.0035]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aprWnDU5pwF7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5qRsgrtxzg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToZhxhT_tx_s"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69FLVj6KtyJj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBHYYmrtyTE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSrTiF06tycZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMSrD5sAtylz"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_92N60ZtyvP"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIUSHCm4ty4r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD_MEOuyty71"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4FxssZPtzNr"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1x7kHVoG5d"
      },
      "source": [
        "# \"DeepSTN_net.py\"\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import numpy as np\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n",
        "from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv2D\n",
        "import PPT3_network.metrics as metrics\n",
        "\n",
        "# Relu-BN-Conv2D 3x3\n",
        "def conv_unit0(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    print('kernel=(3,3)')\n",
        "    return unit_model\n",
        "\n",
        "# Relu-BN-Conv2D 1x1\n",
        "def conv_unit1(Fin,Fout,drop,H,W):\n",
        "    unit_input=Input(shape=(Fin,H,W))\n",
        "\n",
        "    unit_conv=Activation('relu')(unit_input)\n",
        "    unit_conv=BatchNormalization()(unit_conv)\n",
        "    unit_conv=Dropout(drop)(unit_conv)\n",
        "    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n",
        "    unit_model=Model(inputs=unit_input,outputs=unit_output)\n",
        "    print('kernel=(1,1)')\n",
        "    return unit_model\n",
        "\n",
        "# efficient version of Res_plus\n",
        "def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    #normal channels\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    #separated channels\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n",
        "    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n",
        "    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n",
        "    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n",
        "\n",
        "    #merge\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# new resdual block\n",
        "def Res_plus(name,F,Fplus,rate,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n",
        "\n",
        "    if rate == 1:\n",
        "        cl_conv1B=cl_input\n",
        "    if rate !=1:\n",
        "        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n",
        "\n",
        "    cl_conv1B=Activation('relu')(cl_conv1B)\n",
        "    cl_conv1B=BatchNormalization()(cl_conv1B)\n",
        "\n",
        "    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n",
        "\n",
        "    cl_conv1B=plus_conv(cl_conv1B)\n",
        "\n",
        "    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n",
        "\n",
        "    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "# normal residual block\n",
        "def Res_normal(name,F,drop,H,W):\n",
        "    cl_input=Input(shape=(F,H,W))\n",
        "\n",
        "    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n",
        "\n",
        "    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n",
        "\n",
        "    cl_out=Add()([cl_input,cl_conv2])\n",
        "\n",
        "    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n",
        "\n",
        "    return cl_model\n",
        "\n",
        "def cpt_slice(x, h1, h2):\n",
        "    return x[:,h1:h2,:,:]\n",
        "\n",
        "# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)\n",
        "def T_trans(T,T_F,H,W):\n",
        "\n",
        "    T_in=Input(shape=(T+7,H,W))\n",
        "    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n",
        "    T_act=Activation('relu')(T_mid)\n",
        "    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n",
        "    T_mul=Activation('relu')(T_fin)\n",
        "    T_model=Model(inputs=T_in,outputs=T_mul)\n",
        "\n",
        "    return T_model\n",
        "\n",
        "# transfer Time vector and PoI matrix to time-weighted PoI matrix\n",
        "def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n",
        "    if 1:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        if P_N>=2:\n",
        "            T_x0 =T_trans(T,T_F,H,W)(time_in)\n",
        "            T_x1 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=3:\n",
        "            T_x2 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=4:\n",
        "            T_x3 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=5:\n",
        "            T_x4 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=6:\n",
        "            T_x5 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=7:\n",
        "            T_x6 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=8:\n",
        "            T_x7 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=9:\n",
        "            T_x8 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=10:\n",
        "            T_x9 =T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=11:\n",
        "            T_x10=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=12:\n",
        "            T_x11=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=13:\n",
        "            T_x12=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=14:\n",
        "            T_x13=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=15:\n",
        "            T_x14=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N>=16:\n",
        "            T_x15=T_trans(T,T_F,H,W)(time_in)\n",
        "\n",
        "\n",
        "        if P_N==1:\n",
        "            T_x=T_trans(T,T_F,H,W)(time_in)\n",
        "        if P_N==2:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1])\n",
        "        if P_N==3:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n",
        "        if P_N==4:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n",
        "        if P_N==5:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n",
        "        if P_N==6:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n",
        "        if P_N==7:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n",
        "        if P_N==8:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n",
        "        if P_N==9:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n",
        "        if P_N==10:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n",
        "        if P_N==11:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n",
        "        if P_N==12:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n",
        "        if P_N==13:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n",
        "        if P_N==14:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n",
        "        if P_N==15:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n",
        "        if P_N==16:\n",
        "            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n",
        "\n",
        "        poi_time=Multiply()([poi_in,T_x])\n",
        "        if isPT_F:\n",
        "            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n",
        "            print('PT_F = YES')\n",
        "        else:\n",
        "            print('PT_F = NO')\n",
        "        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n",
        "\n",
        "        return PT_model\n",
        "\n",
        "# main model\n",
        "def DeepSTN(H=12,W=14,channel=1,      #H-map_height W-map_width channel-map_channel\n",
        "            c=3,p=4,t=4,              #c-closeness p-period t-trend\n",
        "            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n",
        "            is_plus=True,             #use ResPlus or mornal convolution\n",
        "            is_plus_efficient=False,  #use the efficient version of ResPlus\n",
        "            plus=8,rate=2,            #rate-pooling_rate\n",
        "            is_pt=True,               #use PoI and Time or not\n",
        "            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day\n",
        "            drop=0,\n",
        "            is_summary=True, #show detail\n",
        "            lr=0.0002,\n",
        "            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n",
        "            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n",
        "\n",
        "    all_channel = channel * (c+p+t)\n",
        "\n",
        "    cut0 = int( 0 )\n",
        "    cut1 = int( cut0 + channel*c )\n",
        "    cut2 = int( cut1 + channel*p )\n",
        "    cut3 = int( cut2 + channel*t )\n",
        "\n",
        "    cpt_input=Input(shape=(all_channel,H,W))\n",
        "\n",
        "    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n",
        "    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n",
        "    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n",
        "\n",
        "    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n",
        "    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n",
        "    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n",
        "\n",
        "    if is_pt:\n",
        "        poi_in=Input(shape=(P_N,H,W))\n",
        "        # T_times/day + 7days/week\n",
        "        time_in=Input(shape=(T+7,H,W))\n",
        "\n",
        "        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n",
        "\n",
        "        poi_time=PT_model([poi_in,time_in])\n",
        "\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    else:\n",
        "        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n",
        "        if kernel1:\n",
        "            cpt=conv_unit1(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "        else:\n",
        "            cpt=conv_unit0(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n",
        "\n",
        "    if is_plus:\n",
        "        if is_plus_efficient:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "        else:\n",
        "            for i in range(R_N):\n",
        "                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n",
        "\n",
        "    else:\n",
        "        for i in range(R_N):\n",
        "            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n",
        "\n",
        "\n",
        "    cpt_conv2=Activation('relu')(cpt)\n",
        "    cpt_out2=BatchNormalization()(cpt_conv2)\n",
        "    cpt_conv1=Dropout(drop)(cpt_out2)\n",
        "    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n",
        "\n",
        "    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조\n",
        "\n",
        "    cpt_out1=Activation('tanh')(cpt_conv1)\n",
        "\n",
        "    if is_pt:\n",
        "        DeepSTN_model=Model(inputs=[cpt_input,poi_in,time_in],outputs=cpt_out1)\n",
        "    else:\n",
        "        DeepSTN_model=Model(inputs=cpt_input,outputs=cpt_out1)\n",
        "\n",
        "    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[metrics.rmse,metrics.mae])\n",
        "\n",
        "    if is_summary:\n",
        "        DeepSTN_model.summary()\n",
        "\n",
        "\n",
        "    print('***** pre_F : ',pre_F       )\n",
        "    print('***** conv_F: ',conv_F      )\n",
        "    print('***** R_N   : ',R_N         )\n",
        "\n",
        "    print('***** plus  : ',plus*is_plus)\n",
        "    print('***** rate  : ',rate*is_plus)\n",
        "\n",
        "    print('***** P_N   : ',P_N*is_pt   )\n",
        "    print('***** T_F   : ',T_F*is_pt   )\n",
        "    print('***** PT_F  : ',PT_F*is_pt*isPT_F )\n",
        "    print('***** T     : ',T           )\n",
        "\n",
        "    print('***** drop  : ',drop        )\n",
        "\n",
        "    return DeepSTN_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56kVKyKoUI5"
      },
      "source": [
        "# \"metrics.py\"\n",
        "\n",
        "# import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "def root_mean_square_error(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "# aliases\n",
        "mse = MSE = mean_squared_error\n",
        "# rmse = RMSE = root_mean_square_error\n",
        "\n",
        "\n",
        "def masked_mean_squared_error(y_true, y_pred):\n",
        "    idx = (y_true > 1e-6).nonzero()\n",
        "    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n",
        "\n",
        "def masked_rmse(y_true, y_pred):\n",
        "    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_pred - y_true))\n",
        "\n",
        "\n",
        "threshold=0.05\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-NOpElnMI"
      },
      "source": [
        "#ComparisionBikeNYC.py\n",
        "\n",
        "\n",
        "#from __future__ import print_function\n",
        "#from DATA.lzq_read_data_time_poi import lzq_load_data\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#import cPickle as pickle\n",
        "import numpy as np\n",
        "#import math\n",
        "\n",
        "NO=4\n",
        "#for reproduction\n",
        "seed=1\n",
        "for i in range(NO):\n",
        "    seed=seed*10+7\n",
        "seed=seed*10+7\n",
        "np.random.seed(seed)\n",
        "#from ipdb import set_trace\n",
        "#set_trace()\n",
        "\n",
        "#for GPU in Lab\n",
        "device=6\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(device)\n",
        "import tensorflow as tf  #from V1707\n",
        "config=tf.ConfigProto()  #from V1707\n",
        "config.gpu_options.allow_growth=True  #from V1707\n",
        "#config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
        "sess=tf.Session(config=config)  #from V1707\n",
        "#import keras.backend.tensorflow_backend as KTF\n",
        "#KTF._set_session(tf.Session(config=config))\n",
        "import setproctitle  #from V1707\n",
        "setproctitle.setproctitle('Comprison Start! @ ZiqianLin')  #from V1707\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "\n",
        "#hyperparameters\n",
        "epoch = 1#350  # number of epoch at training stage\n",
        "batch_size = 32  # batch size\n",
        "lr = 0.0002  # learning rate\n",
        "\n",
        "#H,W,channel = 21,12,2   # grid size\n",
        "H,W,channel = 18,15,1   # grid size\n",
        "\n",
        "#T = 24*1  # number of time intervals in one day\n",
        "T = 4*1  # number of time intervals in one day\n",
        "\n",
        "len_closeness = 3  # length of closeness dependent sequence\n",
        "len_period = 4  # length of peroid dependent sequence\n",
        "len_trend = 4  # length of trend dependent sequence\n",
        "\n",
        "T_closeness,T_period,T_trend=1,T,T*7\n",
        "\n",
        "# last 7 days for testing data\n",
        "days_test = 14\n",
        "len_test = T * days_test\n",
        "\n",
        "#the number of repetition and if retrain the model\n",
        "iterate_num=10\n",
        "\n",
        "XDST=0  #DST\n",
        "X11=1   #DSTN+ResPlus+PoI&Time\n",
        "X10=1   #DSTN+ResPlus\n",
        "X01=0   #DSTN+PoI&Time\n",
        "X00=0   #DSTN\n",
        "\n",
        "trainDST=1  #DST\n",
        "train11=1   #DSTN+ResPlus+PoI&Time\n",
        "train10=1   #DSTN+ResPlus\n",
        "train01=1   #DSTN+PoI&Time\n",
        "train00=1   #DSTN\n",
        "\n",
        "\n",
        "\n",
        "#DST result\n",
        "if XDST:\n",
        "    setproctitle.setproctitle('BJMobile DST @ ZiqianLin')  #from V1707\n",
        "\n",
        "    print(\"loading data...\")\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    R_N = 4   # number of residual units\n",
        "\n",
        "    from keras.optimizers import Adam\n",
        "    from DST_network.STResNet import stresnet\n",
        "    import DST_network.metrics as metrics\n",
        "\n",
        "    def build_model(external_dim,CFN):\n",
        "        c_conf = (len_closeness, channel, H, W) if len_closeness > 0 else None\n",
        "        p_conf = (len_period,    channel, H, W) if len_period    > 0 else None\n",
        "        t_conf = (len_trend,     channel, H, W) if len_trend     > 0 else None\n",
        "\n",
        "        model = stresnet(c_conf=c_conf, p_conf=p_conf, t_conf=t_conf,\n",
        "                         external_dim=external_dim, nb_residual_unit=R_N, CF=CFN)\n",
        "\n",
        "        adam = Adam(lr=lr)\n",
        "        model.compile(loss='mse', optimizer=adam, metrics=[metrics.rmse,metrics.mae])\n",
        "        model.summary()\n",
        "        #from keras.utils.visualize_util import plot\n",
        "        #plot(model, to_file='model.png', show_shapes=True)\n",
        "        return model\n",
        "\n",
        "\n",
        "    CF=64\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    import time\n",
        "\n",
        "    count=0\n",
        "\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        time_start=time.time()\n",
        "\n",
        "        F='DST_MODEL/dst_model_'+str(iterate)+'_.hdf5'\n",
        "\n",
        "        model = build_model(external_dim=False,CFN=CF)\n",
        "        if trainDST:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                filepath=F,\n",
        "                monitor='val_rmse',\n",
        "                verbose=1,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "                mode='min',\n",
        "                period=1)\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"training model...\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                nb_epoch=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('evaluating using the model that has the best loss on the valid set')\n",
        "        model.load_weights(F)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        score = model.evaluate(X_test, Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DST_SCORE/dst_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "\n",
        "        print('totally cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+ResPlus+PoI&Time\n",
        "if X11:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+ResPlus+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    index=np.arange(9)\n",
        "    P_train=P_train[:,index,:,:]\n",
        "    P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=True\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=True\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_11/MODEL/DeepSTN_11_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train11:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit([X_train,P_train,T_train], Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate([X_test ,P_test ,T_test ], Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_11/SCORE/DeepSTN_11_score3.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+ResPlus\n",
        "if X10:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+ResPlus @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    #index=np.arange(9)\n",
        "    #P_train=P_train[:,index,:,:]\n",
        "    #P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=True\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=False\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_10/MODEL/DeepSTN_10_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train10:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate(X_test,  Y_test, batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_10/SCORE/DeepSTN_10_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN+PoI&Time\n",
        "if X01:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    #index=np.arange(9)\n",
        "    #P_train=P_train[:,index,:,:]\n",
        "    #P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=False\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=True\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_01/MODEL/DeepSTN_01_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train01:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit([X_train,P_train,T_train], Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate([X_test, P_test, T_test ],  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_01/SCORE/DeepSTN_01_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#DSTN\n",
        "if X00:\n",
        "    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n",
        "    from DeepSTN_network.DeepSTN_net import DeepSTN\n",
        "\n",
        "    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n",
        "    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n",
        "\n",
        "    index=np.arange(9)\n",
        "    P_train=P_train[:,index,:,:]\n",
        "    P_test =P_test [:,index,:,:]\n",
        "\n",
        "    pre_F=64\n",
        "    conv_F=64\n",
        "    R_N=2\n",
        "\n",
        "    is_plus=False\n",
        "    plus=8\n",
        "    rate=1\n",
        "\n",
        "    is_pt=False\n",
        "    P_N=9\n",
        "    T_F=7*8\n",
        "    PT_F=9\n",
        "\n",
        "    drop=0.1\n",
        "\n",
        "    import time\n",
        "    count=0\n",
        "    count_sum=iterate_num\n",
        "\n",
        "    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n",
        "\n",
        "    RMSE=np.zeros([iterate_num,1])\n",
        "    MAE =np.zeros([iterate_num,1])\n",
        "\n",
        "    for iterate_index in range(iterate_num):\n",
        "        count=count+1\n",
        "        time_start=time.time()\n",
        "        iterate=iterate_loop[iterate_index]\n",
        "\n",
        "        print(\"***** conv_model *****\")\n",
        "        model=DeepSTN(H=H,W=W,channel=channel,\n",
        "                      c=len_closeness,p=len_period,\n",
        "                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,\n",
        "                      is_plus=is_plus,\n",
        "                      plus=plus,rate=rate,\n",
        "                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,\n",
        "                      drop=drop)\n",
        "\n",
        "        file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n",
        "        #train conv_model\n",
        "        if train00:\n",
        "            model_checkpoint=ModelCheckpoint(\n",
        "                    filepath=file_conv,\n",
        "                    monitor='val_rmse',\n",
        "                    verbose=1,\n",
        "                    save_best_only=True,\n",
        "                    save_weights_only=True,\n",
        "                    mode='min',\n",
        "                    period=1\n",
        "                )\n",
        "\n",
        "            print('=' * 10)\n",
        "            print(\"***** training conv_model *****\")\n",
        "            history = model.fit(X_train, Y_train,\n",
        "                                epochs=epoch,\n",
        "                                batch_size=batch_size,\n",
        "                                validation_split=0.1,\n",
        "                                callbacks=[model_checkpoint],\n",
        "                                verbose=1)\n",
        "\n",
        "        print('=' * 10)\n",
        "        print('***** evaluate *****')\n",
        "        model.load_weights(file_conv)\n",
        "\n",
        "        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n",
        "        print('              mse     rmse    mae')\n",
        "        print('Train score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "        score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n",
        "        print('Test  score:',end=' ')\n",
        "        np.set_printoptions(precision=6, suppress=True)\n",
        "        print(np.array(score))\n",
        "\n",
        "        RMSE[iterate_index,0]=score[1]\n",
        "        MAE [iterate_index,0]=score[2]\n",
        "\n",
        "        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "\n",
        "        np.set_printoptions(precision=4, suppress=True)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        for_show=np.mean(for_show,axis=0)\n",
        "        print('RMSE  MAE')\n",
        "        print(for_show)\n",
        "\n",
        "        np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n",
        "\n",
        "        time_end=time.time()\n",
        "        print('iterate cost',time_end-time_start)\n",
        "        print(str(count)+'/'+str(count_sum))\n",
        "\n",
        "\n",
        "\n",
        "#Comparison\n",
        "X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "print('MODEL                     RMSE  MAE')\n",
        "if 0:\n",
        "    print('ResNet                  :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DST_SCORE/dst_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    print(for_show)\n",
        "if 0:\n",
        "    print('DeepSTN                 :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_00/SCORE/DeepSTN_00_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 1:\n",
        "    print('DeepSTN+ResPlus         :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_10/SCORE/DeepSTN_10_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 0:\n",
        "    print('DeepSTN+PoI&Time        :',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_01/SCORE/DeepSTN_01_score.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n",
        "if 1:\n",
        "    print('DeepSTN+ResPlus+PoI&Time:',end=' ')\n",
        "    [RMSE,MAE]=np.load('DeepSTN_11/SCORE/DeepSTN_11_score3.npy')\n",
        "    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n",
        "    for_show=np.mean(for_show,axis=0)\n",
        "    print(for_show)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}