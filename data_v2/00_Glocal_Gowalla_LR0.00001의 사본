{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"00_Glocal_Gowalla_LR0.00001","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KV_EdNWClAq9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627627682025,"user_tz":-540,"elapsed":20249,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}},"outputId":"026ebaea-d38d-4f06-b36e-42c5959ca519"},"source":["# google drive connect\n","Copied_path = '/content/drive/MyDrive/Colab Notebooks/MyPaper/GlocalSTN/gowalla' # Paste target directory here\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir(Copied_path)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KS2vPiaBYItm","executionInfo":{"status":"ok","timestamp":1627627688013,"user_tz":-540,"elapsed":1102,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# global\n","\n","import numpy as np\n","\n","class MM:\n","    def __init__(self,MM_max,MM_min):\n","        self.max=MM_max\n","        self.min=MM_min\n","\n","# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n","def lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n","    \n","    all_data=np.load('00/m7_global.npy')\n","    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n","    len_total,feature,map_height,map_width=all_data.shape\n","    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n","    #len_total,feature,map_height,map_width=all_data.shape\n","    print('all_data shape: ',all_data.shape)\n","    #mm=MM(np.max(all_data),np.min(all_data))\n","    print('max=',np.max(all_data),' min=',np.min(all_data))\n","    \n","    #for time\n","    time=np.arange(len_total,dtype=int)\n","    #hour\n","    time_hour=time%T_period\n","    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n","    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n","    for i in range(len_total):\n","        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #day\n","    time_day=(time//T_period)%7\n","    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n","    for i in range(len_total):\n","        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #con\n","    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n","    \n","    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n","    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n","    \n","    if len_trend>0:\n","        number_of_skip_hours=T_trend*len_trend\n","    elif len_period>0:\n","        number_of_skip_hours=T_period*len_period\n","    elif len_closeness>0:\n","        number_of_skip_hours=T_closeness*len_closeness  \n","    else:\n","        print(\"wrong\")\n","    print('number_of_skip_hours:',number_of_skip_hours)\n","    \n","    Y=all_data[number_of_skip_hours:len_total]\n","\n","    if len_closeness>0:\n","        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n","        for i in range(len_closeness-1):\n","            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n","    if len_period>0:\n","        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n","        for i in range(len_period-1):\n","            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n","    if len_trend>0:\n","        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n","        for i in range(len_trend-1):\n","            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n","    \n","    matrix_T=matrix_T[number_of_skip_hours:]\n","    \n","    X_closeness_train=X_closeness[:-len_test] \n","    X_period_train=X_period[:-len_test] \n","    X_trend_train=X_trend[:-len_test]  \n","    T_train=matrix_T[:-len_test] \n","    X_closeness_test=X_closeness[-len_test:] \n","    X_period_test=X_period[-len_test:] \n","    X_trend_test=X_trend[-len_test:]          \n","    T_test=matrix_T[-len_test:]         \n","    \n","    X_train=[X_closeness_train,X_period_train,X_trend_train]\n","    X_test=[X_closeness_test,X_period_test,X_trend_test]\n","    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n","    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n","    Y_train=Y[:-len_test] \n","    Y_test=Y[-len_test:] \n","\n","    len_train=X_closeness_train.shape[0]\n","    len_test=X_closeness_test.shape[0]\n","    print('len_train='+str(len_train))\n","    print('len_test ='+str(len_test ))\n","    \n","    '''\n","    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n","    for i in range(poi.shape[0]):\n","        poi[i]=poi[i]/np.max(poi[i])\n","    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n","    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n","    \n","    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n","    '''\n","    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n","    "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"AauHopRUnGYv","executionInfo":{"status":"ok","timestamp":1627627689091,"user_tz":-540,"elapsed":1079,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# m0_Community\n","\n","import numpy as np\n","\n","class MM:\n","    def __init__(self,MM_max,MM_min):\n","        self.max=MM_max\n","        self.min=MM_min\n","\n","# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n","def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n","    \n","    all_data=np.load('00/m0_Com.npy')\n","    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n","    len_total,feature,map_height,map_width=all_data.shape\n","    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n","    #len_total,feature,map_height,map_width=all_data.shape\n","    print('all_data shape: ',all_data.shape)\n","    #mm=MM(np.max(all_data),np.min(all_data))\n","    print('max=',np.max(all_data),' min=',np.min(all_data))\n","    \n","    #for time\n","    time=np.arange(len_total,dtype=int)\n","    #hour\n","    time_hour=time%T_period\n","    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n","    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n","    for i in range(len_total):\n","        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #day\n","    time_day=(time//T_period)%7\n","    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n","    for i in range(len_total):\n","        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #con\n","    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n","    \n","    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n","    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n","    \n","    if len_trend>0:\n","        number_of_skip_hours=T_trend*len_trend\n","    elif len_period>0:\n","        number_of_skip_hours=T_period*len_period\n","    elif len_closeness>0:\n","        number_of_skip_hours=T_closeness*len_closeness  \n","    else:\n","        print(\"wrong\")\n","    print('number_of_skip_hours:',number_of_skip_hours)\n","    \n","    Y=all_data[number_of_skip_hours:len_total]\n","\n","    if len_closeness>0:\n","        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n","        for i in range(len_closeness-1):\n","            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n","    if len_period>0:\n","        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n","        for i in range(len_period-1):\n","            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n","    if len_trend>0:\n","        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n","        for i in range(len_trend-1):\n","            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n","    \n","    matrix_T=matrix_T[number_of_skip_hours:]\n","    \n","    X_closeness_train=X_closeness[:-len_test] \n","    X_period_train=X_period[:-len_test] \n","    X_trend_train=X_trend[:-len_test]  \n","    T_train=matrix_T[:-len_test] \n","    X_closeness_test=X_closeness[-len_test:] \n","    X_period_test=X_period[-len_test:] \n","    X_trend_test=X_trend[-len_test:]          \n","    T_test=matrix_T[-len_test:]         \n","    \n","    X_train=[X_closeness_train,X_period_train,X_trend_train]\n","    X_test=[X_closeness_test,X_period_test,X_trend_test]\n","    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n","    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n","    Y_train=Y[:-len_test] \n","    Y_test=Y[-len_test:] \n","\n","    len_train=X_closeness_train.shape[0]\n","    len_test=X_closeness_test.shape[0]\n","    print('len_train='+str(len_train))\n","    print('len_test ='+str(len_test ))\n","    \n","    '''\n","    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n","    for i in range(poi.shape[0]):\n","        poi[i]=poi[i]/np.max(poi[i])\n","    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n","    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n","    \n","    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n","    '''\n","    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n","    "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"FgFX7E7fo_a8","executionInfo":{"status":"ok","timestamp":1627627690179,"user_tz":-540,"elapsed":1090,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# DST Network/ilayer.py\n","\n","from keras import backend as K\n","from keras.engine.topology import Layer\n","# from keras.layers import Dense\n","import numpy as np\n","\n","\n","class iLayer(Layer):\n","    def __init__(self, **kwargs):\n","        # self.output_dim = output_dim\n","        super(iLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        initial_weight_value = np.random.random(input_shape[1:])\n","        self.W = K.variable(initial_weight_value)\n","        #self.trainable_weights = [self.W]\n","        self.trainable_weight = [self.W]\n","\n","    def call(self, x, mask=None):\n","        return x * self.W\n","\n","    def get_output_shape_for(self, input_shape):\n","        return input_shape\n","\n","def avg_wo_minmax(x) :\n","  j = 0\n","  tot = 0\n","  max = x.max()\n","  min = x.min()\n","  for i in range(x.shape[0]) :\n","    if x[i,0] != max and x[i,0] != min :\n","      j = j + 1\n","      tot = tot + x[i,0]\n","  return tot / j"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"k925xkl8Uynj","executionInfo":{"status":"ok","timestamp":1627627690179,"user_tz":-540,"elapsed":2,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# \"metrics.py\"\n","\n","# import numpy as np\n","from keras import backend as K\n","\n","def mean_squared_error(y_true, y_pred):\n","    return K.mean(K.square(y_pred - y_true))\n","\n","\n","def root_mean_square_error(y_true, y_pred):\n","    return mean_squared_error(y_true, y_pred) ** 0.5\n","\n","def rmse(y_true, y_pred):\n","    return mean_squared_error(y_true, y_pred) ** 0.5\n","\n","\n","# aliases\n","mse = MSE = mean_squared_error\n","# rmse = RMSE = root_mean_square_error\n","\n","\n","def masked_mean_squared_error(y_true, y_pred):\n","    idx = (y_true > 1e-6).nonzero()\n","    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n","\n","def masked_rmse(y_true, y_pred):\n","    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n","\n","\n","def mean_absolute_error(y_true, y_pred):\n","    return K.mean(K.abs(y_pred - y_true))\n","\n","def mae(y_true, y_pred):\n","    return K.mean(K.abs(y_pred - y_true))\n","\n","\t\n","threshold=0.05\n","\n","def mean_absolute_percentage_error(y_true, y_pred):\n","    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n","\n","def mape(y_true, y_pred):\n","    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8M9judzxQIB","executionInfo":{"status":"ok","timestamp":1627627691224,"user_tz":-540,"elapsed":1047,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# \"DeepSTN_net.py\"\n","\n","from keras import backend as K\n","K.set_image_data_format('channels_first')\n","import numpy as np\n","\n","from keras.optimizers import Adam\n","from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n","from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n","from keras.models import Model\n","from keras.layers.convolutional import Conv2D\n","#import PPT3_network.metrics as metrics\n","\n","# Relu-BN-Conv2D 3x3\n","def conv_unit0(Fin,Fout,drop,H,W):\n","    unit_input=Input(shape=(Fin,H,W))\n","    \n","    unit_conv=Activation('relu')(unit_input)\n","    unit_conv=BatchNormalization()(unit_conv)\n","    unit_conv=Dropout(drop)(unit_conv)\n","    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n","    unit_model=Model(inputs=unit_input,outputs=unit_output)\n","    #print('kernel=(3,3)')\n","    return unit_model\n","\n","# Relu-BN-Conv2D 1x1\n","def conv_unit1(Fin,Fout,drop,H,W):\n","    unit_input=Input(shape=(Fin,H,W))\n","    \n","    unit_conv=Activation('relu')(unit_input)\n","    unit_conv=BatchNormalization()(unit_conv)\n","    unit_conv=Dropout(drop)(unit_conv)\n","    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n","    unit_model=Model(inputs=unit_input,outputs=unit_output)\n","    #print('kernel=(1,1)')\n","    return unit_model\n","\n","# efficient version of Res_plus\n","def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n","    cl_input=Input(shape=(F,H,W))\n","\n","    #normal channels\n","    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n","\n","    #separated channels\n","    if rate == 1:\n","        cl_conv1B=cl_input\n","    if rate !=1:\n","        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n","\n","    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n","    cl_conv1B=Activation('relu')(cl_conv1B)\n","    cl_conv1B=BatchNormalization()(cl_conv1B)\n","\n","    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n","    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n","    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n","    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n","    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n","\n","    #merge\n","    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n","\n","    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n","\n","    cl_out=Add()([cl_input,cl_conv2])\n","\n","    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n","\n","    return cl_model\n","\n","# new resdual block\n","def Res_plus(name,F,Fplus,rate,drop,H,W):\n","    cl_input=Input(shape=(F,H,W))\n","    \n","    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n","    \n","    if rate == 1:\n","        cl_conv1B=cl_input\n","    if rate !=1:\n","        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n","   \n","    cl_conv1B=Activation('relu')(cl_conv1B)\n","    cl_conv1B=BatchNormalization()(cl_conv1B) \n","    \n","    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n","\n","    cl_conv1B=plus_conv(cl_conv1B)\n","\n","    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n","\n","    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n","    \n","    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n","    \n","    cl_out=Add()([cl_input,cl_conv2])\n","    \n","    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n","\n","    return cl_model\n","\n","# normal residual block\n","def Res_normal(name,F,drop,H,W):\n","    cl_input=Input(shape=(F,H,W))\n","    \n","    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n","   \n","    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n","    \n","    cl_out=Add()([cl_input,cl_conv2])\n","    \n","    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n","\n","    return cl_model\n","    \n","def cpt_slice(x, h1, h2):  \n","    return x[:,h1:h2,:,:]  \n","\n","# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)  \n","def T_trans(T,T_F,H,W):\n","\n","    T_in=Input(shape=(T+7,H,W))\n","    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n","    T_act=Activation('relu')(T_mid)\n","    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n","    T_mul=Activation('relu')(T_fin)\n","    T_model=Model(inputs=T_in,outputs=T_mul)\n","\n","    return T_model    \n","  \n","# transfer Time vector and PoI matrix to time-weighted PoI matrix    \n","def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n","    if 1:\n","        poi_in=Input(shape=(P_N,H,W))\n","        # T_times/day + 7days/week \n","        time_in=Input(shape=(T+7,H,W))\n","\n","        if P_N>=2:\n","            T_x0 =T_trans(T,T_F,H,W)(time_in)\n","            T_x1 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=3:\n","            T_x2 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=4:\n","            T_x3 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=5:\n","            T_x4 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=6:\n","            T_x5 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=7:\n","            T_x6 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=8:\n","            T_x7 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=9:\n","            T_x8 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=10:\n","            T_x9 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=11:\n","            T_x10=T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=12:\n","            T_x11=T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=13:\n","            T_x12=T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=14:\n","            T_x13=T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=15:\n","            T_x14=T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=16:\n","            T_x15=T_trans(T,T_F,H,W)(time_in)\n","         \n","        \n","        if P_N==1:\n","            T_x=T_trans(T,T_F,H,W)(time_in)\n","        if P_N==2:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1])\n","        if P_N==3:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n","        if P_N==4:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n","        if P_N==5:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n","        if P_N==6:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n","        if P_N==7:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n","        if P_N==8:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n","        if P_N==9:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n","        if P_N==10:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n","        if P_N==11:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n","        if P_N==12:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n","        if P_N==13:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n","        if P_N==14:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n","        if P_N==15:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n","        if P_N==16:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n","            \n","        poi_time=Multiply()([poi_in,T_x])\n","        if isPT_F:\n","            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n","            print('PT_F = YES')\n","        else:\n","            print('PT_F = NO')\n","        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n","\n","        return PT_model\n","\n","# main model\n","def DeepSTN(H=14,W=12,channel=1,      #H-map_height W-map_width channel-map_channel\n","            c=3,p=4,t=4,              #c-closeness p-period t-trend\n","            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n","            is_plus=True,             #use ResPlus or mornal convolution\n","            is_plus_efficient=False,  #use the efficient version of ResPlus\n","            plus=8,rate=2,            #rate-pooling_rate\n","            is_pt=False,               #use PoI and Time or not\n","            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day \n","            drop=0,\n","            is_summary=False, #show detail\n","            lr=0.0002,\n","            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n","            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n","    \n","    all_channel = channel * (c+p+t)\n","            \n","    cut0 = int( 0 )\n","    cut1 = int( cut0 + channel*c )\n","    cut2 = int( cut1 + channel*p )\n","    cut3 = int( cut2 + channel*t )\n","       \n","    cpt_input=Input(shape=(all_channel,H,W))\n","                \n","    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n","    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n","    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n","    \n","    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n","    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n","    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n","    \n","\n","    cpt_global_input=Input(shape=(all_channel,H,W))\n","                \n","    c_global_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_global_input)\n","    p_global_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_global_input)\n","    t_global_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_global_input)\n","    \n","    c_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_global_input)\n","    p_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_global_input)\n","    t_global_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_global_input)\n","    \n","    if is_pt:\n","        poi_in=Input(shape=(P_N,H,W))\n","        # T_times/day + 7days/week \n","        time_in=Input(shape=(T+7,H,W))\n","\n","        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n","        \n","        poi_time=PT_model([poi_in,time_in])\n"," \n","        cpt_con1=Concatenate(axis=1)([c_global_out1,p_global_out1,t_global_out1,c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n","        if kernel1:\n","            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n","        else:\n","            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n","    \n","    else:\n","        cpt_con1=Concatenate(axis=1)([c_global_out1,p_global_out1,t_global_out1,c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n","        if kernel1:\n","            cpt=conv_unit1(pre_F*6,conv_F,drop,H,W)(cpt_con1)\n","        else:\n","            cpt=conv_unit0(pre_F*6,conv_F,drop,H,W)(cpt_con1)  \n","     \n","    if is_plus:\n","        if is_plus_efficient:\n","            for i in range(R_N):\n","                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n","        else:\n","            for i in range(R_N):\n","                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n","\n","    else:  \n","        for i in range(R_N):\n","            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n","\n","    \n","    cpt_conv2=Activation('relu')(cpt)\n","    cpt_out2=BatchNormalization()(cpt_conv2)\n","    cpt_conv1=Dropout(drop)(cpt_out2)\n","    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n","    \n","    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조    \n","    \n","    cpt_out1=Activation('tanh')(cpt_conv1)\n","            \n","    if is_pt:\n","        DeepSTN_model=Model(inputs=[cpt_global_input, cpt_input,poi_in,time_in],outputs=cpt_out1)\n","    else:\n","        DeepSTN_model=Model(inputs=[cpt_global_input, cpt_input],outputs=cpt_out1) # 여기에 글로벌 인풋도 추가\n","\n","    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[rmse,mae])\n","    \n","    if is_summary:\n","        DeepSTN_model.summary()\n","            \n","    '''\n","    print('***** pre_F : ',pre_F       )\n","    print('***** conv_F: ',conv_F      )\n","    print('***** R_N   : ',R_N         )\n","    \n","    print('***** plus  : ',plus*is_plus)\n","    print('***** rate  : ',rate*is_plus)\n","    \n","    print('***** P_N   : ',P_N*is_pt   )\n","    print('***** T_F   : ',T_F*is_pt   )\n","    print('***** PT_F  : ',PT_F*is_pt*isPT_F )            \n","    print('***** T     : ',T           ) \n","    \n","    print('***** drop  : ',drop        )\n","    ''' \n","    return DeepSTN_model"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"02uqnK3dUwka","executionInfo":{"status":"ok","timestamp":1627633769041,"user_tz":-540,"elapsed":2153608,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}},"outputId":"30b3ddd3-b78b-4621-d7ce-b5979a583000"},"source":["from keras.callbacks import ModelCheckpoint\n","import numpy as np\n","\n","NO=4\n","#for reproduction\n","seed=1\n","for i in range(NO):\n","    seed=seed*10+7\n","seed=seed*10+7\n","np.random.seed(seed)\n","import tensorflow as tf  #from V1707\n","from keras import backend as K\n","\n","\n","#hyperparameters\n","epoch = 800  # number of epoch at training stage\n","batch_size = 32  # batch size\n","lr = 0.00001  # learning rate\n","\n","\n","#H,W,channel = 21,12,2   # grid size\n","H,W,channel = 6,12,1   # grid size\n","\n","#T = 24*1  # number of time intervals in one day\n","T = 2*1  # number of time intervals in one day\n","\n","len_closeness = 3  # length of closeness dependent sequence\n","len_period = 4  # length of peroid dependent sequence\n","len_trend = 4  # length of trend dependent sequence\n","\n","T_closeness,T_period,T_trend=1,T,T*7\n","\n","# last 7 days for testing data\n","days_test = 28\n","len_test = T * days_test\n","\n","#the number of repetition and if retrain the model\n","iterate_num=10\n","\n","\n","XDST=0  #DST\n","X11=1   #DSTN+ResPlus+PoI&Time\n","X10=1   #DSTN+ResPlus\n","X01=0   #DSTN+PoI&Time\n","X00=0   #DSTN\n","\n","trainDST=1  #DST\n","train11=1   #DSTN+ResPlus+PoI&Time\n","train10=1   #DSTN+ResPlus\n","train01=1   #DSTN+PoI&Time\n","train00=1   #DSTN\n","\n","\n","\n","\n","X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","\n","X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","\n","X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n","X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n","\n","index=np.arange(9)\n","#P_train=P_train[:,index,:,:]\n","#P_test =P_test [:,index,:,:]\n","\n","pre_F=64\n","conv_F=64\n","R_N=10\n","    \n","is_plus=True\n","plus=8\n","rate=1\n","    \n","is_pt=False\n","P_N=9\n","T_F=7*8\n","PT_F=9\n","\n","drop=0.1\n","\n","import time\n","count=0\n","count_sum=iterate_num\n","\n","iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","\n","RMSE=np.zeros([iterate_num,1])\n","MAE =np.zeros([iterate_num,1])\n","\n","for iterate_index in range(iterate_num):\n","    count=count+1\n","    time_start=time.time()       \n","    iterate=iterate_loop[iterate_index]\n","    \n","    #print(\"***** conv_model *****\")\n","    model=DeepSTN(H=H,W=W,channel=channel,\n","                  c=len_closeness,p=len_period,                \n","                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                  is_plus=is_plus,\n","                  plus=plus,rate=rate,     \n","                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                  drop=drop)            \n","    \n","    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n","    #train conv_model\n","    \n","    if train00:\n","        '''\n","        model_checkpoint=ModelCheckpoint(\n","                filepath=file_conv,\n","                monitor='val_rmse',\n","                verbose=1,\n","                save_best_only=True,\n","                save_weights_only=True,\n","                mode='min',\n","                period=1\n","            )\n","        '''    \n","        #print('=' * 10)\n","        #print(\"***** training conv_model *****\")\n","        history = model.fit((X_train_global, X_train), Y_train,\n","                            epochs=epoch,\n","                            batch_size=batch_size,\n","                            validation_split=0.1,\n","                            #callbacks=[model_checkpoint],\n","                            verbose=0,\n","                            #validation_data=((X_test_global, X_test), Y_test)\n","                            )\n","        \n","    #print('=' * 10)\n","    #print('***** evaluate *****')\n","    #model.load_weights(file_conv)\n","    \n","    #print(X_train_global.shape, X_train.shape)\n","    \n","    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","    \n","    print('              mse     rmse    mae')\n","    print('Train score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    #print(X_test_global.shape, X_test.shape)\n","    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n","    print('Test  score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    \n","\n","    \n","    RMSE[iterate_index,0]=score[1]\n","    MAE [iterate_index,0]=score[2]\n","    \n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","\n","    np.set_printoptions(precision=4, suppress=True)\n","    print('RMSE  MAE')\n","    print(for_show)\n","        \n","    #for_show=np.mean(for_show,axis=0)\n","    #print('RMSE')\n"," \n","    \n","    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n","    '''    \n","    time_end=time.time()\n","    print('iterate cost',time_end-time_start)\n","    print(str(count)+'/'+str(count_sum))\n","    '''\n","print('AVG RMSE = ', avg_wo_minmax(RMSE))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["all_data shape:  (318, 1, 6, 12)\n","max= 9798.0  min= 0.0\n","mean= -0.991245619586935  variance= 0.05714575341805177\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","all_data shape:  (318, 1, 6, 12)\n","max= 203926.0  min= 0.0\n","mean= -0.9845239880352044  variance= 0.07316312563823049\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","              mse     rmse    mae\n","Train score: [0.000194 0.012016 0.003585]\n","Test  score: [0.00003  0.005434 0.001823]\n","RMSE  MAE\n","[[0.0054 0.0018]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000156 0.011092 0.003398]\n","Test  score: [0.000028 0.005255 0.001697]\n","RMSE  MAE\n","[[0.0054 0.0018]\n"," [0.0053 0.0017]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000239 0.013317 0.003973]\n","Test  score: [0.000024 0.004866 0.00167 ]\n","RMSE  MAE\n","[[0.0054 0.0018]\n"," [0.0053 0.0017]\n"," [0.0049 0.0017]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000215 0.012875 0.003566]\n","Test  score: [0.000017 0.004174 0.00128 ]\n","RMSE  MAE\n","[[0.0054 0.0018]\n"," [0.0053 0.0017]\n"," [0.0049 0.0017]\n"," [0.0042 0.0013]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000368 0.016707 0.00483 ]\n","Test  score: [0.000025 0.004977 0.00174 ]\n","RMSE  MAE\n","[[0.0054 0.0018]\n"," [0.0053 0.0017]\n"," [0.0049 0.0017]\n"," [0.0042 0.0013]\n"," [0.005  0.0017]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000164 0.011209 0.00337 ]\n","Test  score: [0.000017 0.004125 0.001375]\n","RMSE  MAE\n","[[0.0054 0.0018]\n"," [0.0053 0.0017]\n"," [0.0049 0.0017]\n"," [0.0042 0.0013]\n"," [0.005  0.0017]\n"," [0.0041 0.0014]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.00027  0.014253 0.004205]\n","Test  score: [0.000024 0.004939 0.001726]\n","RMSE  MAE\n","[[0.0054 0.0018]\n"," [0.0053 0.0017]\n"," [0.0049 0.0017]\n"," [0.0042 0.0013]\n"," [0.005  0.0017]\n"," [0.0041 0.0014]\n"," [0.0049 0.0017]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000208 0.012708 0.003823]\n","Test  score: [0.000026 0.00505  0.001799]\n","RMSE  MAE\n","[[0.0054 0.0018]\n"," [0.0053 0.0017]\n"," [0.0049 0.0017]\n"," [0.0042 0.0013]\n"," [0.005  0.0017]\n"," [0.0041 0.0014]\n"," [0.0049 0.0017]\n"," [0.0051 0.0018]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000189 0.012086 0.003832]\n","Test  score: [0.000027 0.005212 0.001978]\n","RMSE  MAE\n","[[0.0054 0.0018]\n"," [0.0053 0.0017]\n"," [0.0049 0.0017]\n"," [0.0042 0.0013]\n"," [0.005  0.0017]\n"," [0.0041 0.0014]\n"," [0.0049 0.0017]\n"," [0.0051 0.0018]\n"," [0.0052 0.002 ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.0002   0.012479 0.003798]\n","Test  score: [0.000027 0.00524  0.001926]\n","RMSE  MAE\n","[[0.0054 0.0018]\n"," [0.0053 0.0017]\n"," [0.0049 0.0017]\n"," [0.0042 0.0013]\n"," [0.005  0.0017]\n"," [0.0041 0.0014]\n"," [0.0049 0.0017]\n"," [0.0051 0.0018]\n"," [0.0052 0.002 ]\n"," [0.0052 0.0019]]\n","AVG RMSE =  0.004964152409229428\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BTujgh28pvkw","executionInfo":{"status":"ok","timestamp":1627633769632,"user_tz":-540,"elapsed":595,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# m1_Ent\n","\n","import numpy as np\n","\n","class MM:\n","    def __init__(self,MM_max,MM_min):\n","        self.max=MM_max\n","        self.min=MM_min\n","\n","# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n","def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n","    \n","    all_data=np.load('00/m1_Ent.npy')\n","    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n","    len_total,feature,map_height,map_width=all_data.shape\n","    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n","    #len_total,feature,map_height,map_width=all_data.shape\n","    print('all_data shape: ',all_data.shape)\n","    #mm=MM(np.max(all_data),np.min(all_data))\n","    print('max=',np.max(all_data),' min=',np.min(all_data))\n","    \n","    #for time\n","    time=np.arange(len_total,dtype=int)\n","    #hour\n","    time_hour=time%T_period\n","    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n","    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n","    for i in range(len_total):\n","        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #day\n","    time_day=(time//T_period)%7\n","    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n","    for i in range(len_total):\n","        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #con\n","    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n","    \n","    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n","    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n","    \n","    if len_trend>0:\n","        number_of_skip_hours=T_trend*len_trend\n","    elif len_period>0:\n","        number_of_skip_hours=T_period*len_period\n","    elif len_closeness>0:\n","        number_of_skip_hours=T_closeness*len_closeness  \n","    else:\n","        print(\"wrong\")\n","    print('number_of_skip_hours:',number_of_skip_hours)\n","    \n","    Y=all_data[number_of_skip_hours:len_total]\n","\n","    if len_closeness>0:\n","        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n","        for i in range(len_closeness-1):\n","            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n","    if len_period>0:\n","        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n","        for i in range(len_period-1):\n","            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n","    if len_trend>0:\n","        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n","        for i in range(len_trend-1):\n","            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n","    \n","    matrix_T=matrix_T[number_of_skip_hours:]\n","    \n","    X_closeness_train=X_closeness[:-len_test] \n","    X_period_train=X_period[:-len_test] \n","    X_trend_train=X_trend[:-len_test]  \n","    T_train=matrix_T[:-len_test] \n","    X_closeness_test=X_closeness[-len_test:] \n","    X_period_test=X_period[-len_test:] \n","    X_trend_test=X_trend[-len_test:]          \n","    T_test=matrix_T[-len_test:]         \n","    \n","    X_train=[X_closeness_train,X_period_train,X_trend_train]\n","    X_test=[X_closeness_test,X_period_test,X_trend_test]\n","    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n","    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n","    Y_train=Y[:-len_test] \n","    Y_test=Y[-len_test:] \n","\n","    len_train=X_closeness_train.shape[0]\n","    len_test=X_closeness_test.shape[0]\n","    print('len_train='+str(len_train))\n","    print('len_test ='+str(len_test ))\n","    \n","    '''\n","    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n","    for i in range(poi.shape[0]):\n","        poi[i]=poi[i]/np.max(poi[i])\n","    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n","    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n","    \n","    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n","    '''\n","    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n","    "],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zaareiVrtfkE","executionInfo":{"status":"ok","timestamp":1627635931139,"user_tz":-540,"elapsed":2161513,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}},"outputId":"a2c8de44-751c-4afa-96af-65ad8895a9d2"},"source":["from keras.callbacks import ModelCheckpoint\n","import numpy as np\n","\n","NO=4\n","#for reproduction\n","seed=1\n","for i in range(NO):\n","    seed=seed*10+7\n","seed=seed*10+7\n","np.random.seed(seed)\n","import tensorflow as tf  #from V1707\n","from keras import backend as K\n","\n","'''\n","#hyperparameters\n","epoch = 100  # number of epoch at training stage\n","batch_size = 32  # batch size\n","lr = 0.0001  # learning rate\n","\n","#H,W,channel = 21,12,2   # grid size\n","H,W,channel = 14,12,1   # grid size\n","\n","#T = 24*1  # number of time intervals in one day\n","T = 8*1  # number of time intervals in one day\n","\n","len_closeness = 3  # length of closeness dependent sequence\n","len_period = 4  # length of peroid dependent sequence\n","len_trend = 4  # length of trend dependent sequence\n","\n","T_closeness,T_period,T_trend=1,T,T*7\n","\n","# last 7 days for testing data\n","days_test = 14\n","len_test = T * days_test\n","\n","#the number of repetition and if retrain the model\n","iterate_num=1\n","\n","\n","XDST=0  #DST\n","X11=1   #DSTN+ResPlus+PoI&Time\n","X10=1   #DSTN+ResPlus\n","X01=0   #DSTN+PoI&Time\n","X00=0   #DSTN\n","\n","trainDST=1  #DST\n","train11=1   #DSTN+ResPlus+PoI&Time\n","train10=1   #DSTN+ResPlus\n","train01=1   #DSTN+PoI&Time\n","train00=1   #DSTN\n","'''\n","\n","\n","\n","X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","\n","X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","\n","X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n","X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n","\n","index=np.arange(9)\n","#P_train=P_train[:,index,:,:]\n","#P_test =P_test [:,index,:,:]\n","'''\n","pre_F=64\n","conv_F=64\n","#R_N=10\n","    \n","#is_plus=True\n","plus=8\n","rate=1\n","    \n","is_pt=False\n","P_N=9\n","T_F=7*8\n","PT_F=9\n","\n","drop=0.1\n","'''\n","import time\n","count=0\n","count_sum=iterate_num\n","\n","iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","\n","RMSE=np.zeros([iterate_num,1])\n","MAE =np.zeros([iterate_num,1])\n","\n","for iterate_index in range(iterate_num):\n","    count=count+1\n","    time_start=time.time()       \n","    iterate=iterate_loop[iterate_index]\n","    \n","    #print(\"***** conv_model *****\")\n","    model=DeepSTN(H=H,W=W,channel=channel,\n","                  c=len_closeness,p=len_period,                \n","                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                  is_plus=is_plus,\n","                  plus=plus,rate=rate,     \n","                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                  drop=drop)            \n","    \n","    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n","    #train conv_model\n","    \n","    if train00:\n","        '''\n","        model_checkpoint=ModelCheckpoint(\n","                filepath=file_conv,\n","                monitor='val_rmse',\n","                verbose=1,\n","                save_best_only=True,\n","                save_weights_only=True,\n","                mode='min',\n","                period=1\n","            )\n","        '''    \n","        #print('=' * 10)\n","        #print(\"***** training conv_model *****\")\n","        history = model.fit((X_train_global, X_train), Y_train,\n","                            epochs=epoch,\n","                            batch_size=batch_size,\n","                            validation_split=0.1,\n","                            #callbacks=[model_checkpoint],\n","                            verbose=0,\n","                            #validation_data=((X_test_global, X_test), Y_test)\n","                            )\n","        \n","    #print('=' * 10)\n","    #print('***** evaluate *****')\n","    #model.load_weights(file_conv)\n","    \n","    #print(X_train_global.shape, X_train.shape)\n","    \n","    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","    \n","    print('              mse     rmse    mae')\n","    print('Train score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    #print(X_test_global.shape, X_test.shape)\n","    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n","    print('Test  score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    \n","\n","    \n","    RMSE[iterate_index,0]=score[1]\n","    MAE [iterate_index,0]=score[2]\n","    \n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","\n","    np.set_printoptions(precision=4, suppress=True)\n","    print('RMSE  MAE')\n","    print(for_show)\n","        \n","    #for_show=np.mean(for_show,axis=0)\n","    #print('RMSE  MAE')\n","    #print(for_show)\n","    \n","    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n","    '''    \n","    time_end=time.time()\n","    print('iterate cost',time_end-time_start)\n","    print(str(count)+'/'+str(count_sum))\n","    '''\n","print('AVG RMSE = ', avg_wo_minmax(RMSE))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["all_data shape:  (318, 1, 6, 12)\n","max= 15067.0  min= 0.0\n","mean= -0.9915053200294218  variance= 0.04847561158837177\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","all_data shape:  (318, 1, 6, 12)\n","max= 203926.0  min= 0.0\n","mean= -0.9845239880352044  variance= 0.07316312563823049\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","              mse     rmse    mae\n","Train score: [0.000298 0.015348 0.004235]\n","Test  score: [0.000055 0.007418 0.002332]\n","RMSE  MAE\n","[[0.0074 0.0023]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000251 0.013999 0.003866]\n","Test  score: [0.000049 0.007027 0.002039]\n","RMSE  MAE\n","[[0.0074 0.0023]\n"," [0.007  0.002 ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000311 0.015836 0.00422 ]\n","Test  score: [0.00005  0.007065 0.002059]\n","RMSE  MAE\n","[[0.0074 0.0023]\n"," [0.007  0.002 ]\n"," [0.0071 0.0021]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000303 0.015288 0.004134]\n","Test  score: [0.000048 0.006907 0.002075]\n","RMSE  MAE\n","[[0.0074 0.0023]\n"," [0.007  0.002 ]\n"," [0.0071 0.0021]\n"," [0.0069 0.0021]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000261 0.01442  0.003953]\n","Test  score: [0.000058 0.007599 0.002294]\n","RMSE  MAE\n","[[0.0074 0.0023]\n"," [0.007  0.002 ]\n"," [0.0071 0.0021]\n"," [0.0069 0.0021]\n"," [0.0076 0.0023]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000232 0.013571 0.003839]\n","Test  score: [0.000053 0.007303 0.002255]\n","RMSE  MAE\n","[[0.0074 0.0023]\n"," [0.007  0.002 ]\n"," [0.0071 0.0021]\n"," [0.0069 0.0021]\n"," [0.0076 0.0023]\n"," [0.0073 0.0023]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000312 0.015597 0.004363]\n","Test  score: [0.000063 0.007929 0.002543]\n","RMSE  MAE\n","[[0.0074 0.0023]\n"," [0.007  0.002 ]\n"," [0.0071 0.0021]\n"," [0.0069 0.0021]\n"," [0.0076 0.0023]\n"," [0.0073 0.0023]\n"," [0.0079 0.0025]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000294 0.01522  0.004407]\n","Test  score: [0.000067 0.008176 0.002723]\n","RMSE  MAE\n","[[0.0074 0.0023]\n"," [0.007  0.002 ]\n"," [0.0071 0.0021]\n"," [0.0069 0.0021]\n"," [0.0076 0.0023]\n"," [0.0073 0.0023]\n"," [0.0079 0.0025]\n"," [0.0082 0.0027]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000306 0.015441 0.00427 ]\n","Test  score: [0.000064 0.007974 0.002569]\n","RMSE  MAE\n","[[0.0074 0.0023]\n"," [0.007  0.002 ]\n"," [0.0071 0.0021]\n"," [0.0069 0.0021]\n"," [0.0076 0.0023]\n"," [0.0073 0.0023]\n"," [0.0079 0.0025]\n"," [0.0082 0.0027]\n"," [0.008  0.0026]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000304 0.015369 0.004396]\n","Test  score: [0.000057 0.00755  0.00245 ]\n","RMSE  MAE\n","[[0.0074 0.0023]\n"," [0.007  0.002 ]\n"," [0.0071 0.0021]\n"," [0.0069 0.0021]\n"," [0.0076 0.0023]\n"," [0.0073 0.0023]\n"," [0.0079 0.0025]\n"," [0.0082 0.0027]\n"," [0.008  0.0026]\n"," [0.0076 0.0024]]\n","AVG RMSE =  0.007483080378733575\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n3Pu97ettf0M","executionInfo":{"status":"ok","timestamp":1627635931140,"user_tz":-540,"elapsed":10,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# m2_Food\n","\n","import numpy as np\n","\n","class MM:\n","    def __init__(self,MM_max,MM_min):\n","        self.max=MM_max\n","        self.min=MM_min\n","\n","# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n","def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n","    \n","    all_data=np.load('00/m2_Food.npy')\n","    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n","    len_total,feature,map_height,map_width=all_data.shape\n","    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n","    #len_total,feature,map_height,map_width=all_data.shape\n","    print('all_data shape: ',all_data.shape)\n","    #mm=MM(np.max(all_data),np.min(all_data))\n","    print('max=',np.max(all_data),' min=',np.min(all_data))\n","    \n","    #for time\n","    time=np.arange(len_total,dtype=int)\n","    #hour\n","    time_hour=time%T_period\n","    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n","    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n","    for i in range(len_total):\n","        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #day\n","    time_day=(time//T_period)%7\n","    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n","    for i in range(len_total):\n","        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #con\n","    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n","    \n","    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n","    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n","    \n","    if len_trend>0:\n","        number_of_skip_hours=T_trend*len_trend\n","    elif len_period>0:\n","        number_of_skip_hours=T_period*len_period\n","    elif len_closeness>0:\n","        number_of_skip_hours=T_closeness*len_closeness  \n","    else:\n","        print(\"wrong\")\n","    print('number_of_skip_hours:',number_of_skip_hours)\n","    \n","    Y=all_data[number_of_skip_hours:len_total]\n","\n","    if len_closeness>0:\n","        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n","        for i in range(len_closeness-1):\n","            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n","    if len_period>0:\n","        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n","        for i in range(len_period-1):\n","            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n","    if len_trend>0:\n","        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n","        for i in range(len_trend-1):\n","            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n","    \n","    matrix_T=matrix_T[number_of_skip_hours:]\n","    \n","    X_closeness_train=X_closeness[:-len_test] \n","    X_period_train=X_period[:-len_test] \n","    X_trend_train=X_trend[:-len_test]  \n","    T_train=matrix_T[:-len_test] \n","    X_closeness_test=X_closeness[-len_test:] \n","    X_period_test=X_period[-len_test:] \n","    X_trend_test=X_trend[-len_test:]          \n","    T_test=matrix_T[-len_test:]         \n","    \n","    X_train=[X_closeness_train,X_period_train,X_trend_train]\n","    X_test=[X_closeness_test,X_period_test,X_trend_test]\n","    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n","    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n","    Y_train=Y[:-len_test] \n","    Y_test=Y[-len_test:] \n","\n","    len_train=X_closeness_train.shape[0]\n","    len_test=X_closeness_test.shape[0]\n","    print('len_train='+str(len_train))\n","    print('len_test ='+str(len_test ))\n","    \n","    '''\n","    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n","    for i in range(poi.shape[0]):\n","        poi[i]=poi[i]/np.max(poi[i])\n","    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n","    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n","    \n","    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n","    '''\n","    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n","    "],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EBowoAPctgES","executionInfo":{"status":"ok","timestamp":1627638087742,"user_tz":-540,"elapsed":2156609,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}},"outputId":"029c1c8d-729a-4692-ca9a-ab6062d0b984"},"source":["from keras.callbacks import ModelCheckpoint\n","import numpy as np\n","\n","NO=4\n","#for reproduction\n","seed=1\n","for i in range(NO):\n","    seed=seed*10+7\n","seed=seed*10+7\n","np.random.seed(seed)\n","import tensorflow as tf  #from V1707\n","from keras import backend as K\n","\n","\n","'''\n","#hyperparameters\n","epoch = 400  # number of epoch at training stage\n","batch_size = 32  # batch size\n","lr = 0.000003  # learning rate\n","\n","#H,W,channel = 21,12,2   # grid size\n","H,W,channel = 8,12,1   # grid size\n","\n","#T = 24*1  # number of time intervals in one day\n","T = 2*1  # number of time intervals in one day\n","\n","len_closeness = 3  # length of closeness dependent sequence\n","len_period = 4  # length of peroid dependent sequence\n","len_trend = 4  # length of trend dependent sequence\n","\n","T_closeness,T_period,T_trend=1,T,T*7\n","\n","# last 7 days for testing data\n","days_test = 35\n","len_test = T * days_test\n","\n","#the number of repetition and if retrain the model\n","iterate_num=10\n","'''\n","\n","XDST=0  #DST\n","X11=1   #DSTN+ResPlus+PoI&Time\n","X10=1   #DSTN+ResPlus\n","X01=0   #DSTN+PoI&Time\n","X00=0   #DSTN\n","\n","trainDST=1  #DST\n","train11=1   #DSTN+ResPlus+PoI&Time\n","train10=1   #DSTN+ResPlus\n","train01=1   #DSTN+PoI&Time\n","train00=1   #DSTN\n","\n","\n","\n","\n","X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","\n","X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","\n","X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n","X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n","\n","index=np.arange(9)\n","#P_train=P_train[:,index,:,:]\n","#P_test =P_test [:,index,:,:]\n","'''\n","pre_F=64\n","conv_F=64\n","#R_N=10\n","    \n","#is_plus=True\n","plus=8\n","rate=1\n","    \n","is_pt=False\n","P_N=9\n","T_F=7*8\n","PT_F=9\n","\n","drop=0.1\n","'''\n","import time\n","count=0\n","count_sum=iterate_num\n","\n","iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","\n","RMSE=np.zeros([iterate_num,1])\n","MAE =np.zeros([iterate_num,1])\n","\n","for iterate_index in range(iterate_num):\n","    count=count+1\n","    time_start=time.time()       \n","    iterate=iterate_loop[iterate_index]\n","    \n","    #print(\"***** conv_model *****\")\n","    model=DeepSTN(H=H,W=W,channel=channel,\n","                  c=len_closeness,p=len_period,                \n","                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                  is_plus=is_plus,\n","                  plus=plus,rate=rate,     \n","                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                  drop=drop)            \n","    \n","    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n","    #train conv_model\n","    \n","    if train00:\n","        '''\n","        model_checkpoint=ModelCheckpoint(\n","                filepath=file_conv,\n","                monitor='val_rmse',\n","                verbose=1,\n","                save_best_only=True,\n","                save_weights_only=True,\n","                mode='min',\n","                period=1\n","            )\n","        '''    \n","        #print('=' * 10)\n","        #print(\"***** training conv_model *****\")\n","        history = model.fit((X_train_global, X_train), Y_train,\n","                            epochs=epoch,\n","                            batch_size=batch_size,\n","                            validation_split=0.1,\n","                            #callbacks=[model_checkpoint],\n","                            verbose=0,\n","                            #validation_data=((X_test_global, X_test), Y_test)\n","                            )\n","        \n","    #print('=' * 10)\n","    #print('***** evaluate *****')\n","    #model.load_weights(file_conv)\n","    \n","    #print(X_train_global.shape, X_train.shape)\n","    \n","    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","    \n","    print('              mse     rmse    mae')\n","    print('Train score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    #print(X_test_global.shape, X_test.shape)\n","    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n","    print('Test  score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    \n","\n","    \n","    RMSE[iterate_index,0]=score[1]\n","    MAE [iterate_index,0]=score[2]\n","    \n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","\n","    np.set_printoptions(precision=4, suppress=True)\n","    print('RMSE  MAE')\n","    print(for_show)\n","        \n","    #for_show=np.mean(for_show,axis=0)\n","    #print('RMSE  MAE')\n","    #print(for_show)\n","    \n","    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n","    '''    \n","    time_end=time.time()\n","    print('iterate cost',time_end-time_start)\n","    print(str(count)+'/'+str(count_sum))\n","    '''\n","print('AVG RMSE = ', avg_wo_minmax(RMSE))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["all_data shape:  (318, 1, 6, 12)\n","max= 25036.0  min= 0.0\n","mean= -0.9812750456952052  variance= 0.0932984935371525\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","all_data shape:  (318, 1, 6, 12)\n","max= 203926.0  min= 0.0\n","mean= -0.9845239880352044  variance= 0.07316312563823049\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","              mse     rmse    mae\n","Train score: [0.00023  0.013846 0.004738]\n","Test  score: [0.000117 0.010838 0.003363]\n","RMSE  MAE\n","[[0.0108 0.0034]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000218 0.013571 0.004882]\n","Test  score: [0.000117 0.010814 0.003398]\n","RMSE  MAE\n","[[0.0108 0.0034]\n"," [0.0108 0.0034]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000273 0.014944 0.005071]\n","Test  score: [0.000079 0.008871 0.003017]\n","RMSE  MAE\n","[[0.0108 0.0034]\n"," [0.0108 0.0034]\n"," [0.0089 0.003 ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000273 0.014826 0.005157]\n","Test  score: [0.0001   0.009982 0.003378]\n","RMSE  MAE\n","[[0.0108 0.0034]\n"," [0.0108 0.0034]\n"," [0.0089 0.003 ]\n"," [0.01   0.0034]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000269 0.015061 0.005034]\n","Test  score: [0.000138 0.011728 0.003351]\n","RMSE  MAE\n","[[0.0108 0.0034]\n"," [0.0108 0.0034]\n"," [0.0089 0.003 ]\n"," [0.01   0.0034]\n"," [0.0117 0.0034]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000242 0.014372 0.005211]\n","Test  score: [0.000138 0.01175  0.003727]\n","RMSE  MAE\n","[[0.0108 0.0034]\n"," [0.0108 0.0034]\n"," [0.0089 0.003 ]\n"," [0.01   0.0034]\n"," [0.0117 0.0034]\n"," [0.0117 0.0037]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000241 0.014018 0.004924]\n","Test  score: [0.000105 0.010231 0.003408]\n","RMSE  MAE\n","[[0.0108 0.0034]\n"," [0.0108 0.0034]\n"," [0.0089 0.003 ]\n"," [0.01   0.0034]\n"," [0.0117 0.0034]\n"," [0.0117 0.0037]\n"," [0.0102 0.0034]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000269 0.014999 0.004946]\n","Test  score: [0.000091 0.009563 0.002942]\n","RMSE  MAE\n","[[0.0108 0.0034]\n"," [0.0108 0.0034]\n"," [0.0089 0.003 ]\n"," [0.01   0.0034]\n"," [0.0117 0.0034]\n"," [0.0117 0.0037]\n"," [0.0102 0.0034]\n"," [0.0096 0.0029]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000253 0.014779 0.005464]\n","Test  score: [0.000212 0.01456  0.004301]\n","RMSE  MAE\n","[[0.0108 0.0034]\n"," [0.0108 0.0034]\n"," [0.0089 0.003 ]\n"," [0.01   0.0034]\n"," [0.0117 0.0034]\n"," [0.0117 0.0037]\n"," [0.0102 0.0034]\n"," [0.0096 0.0029]\n"," [0.0146 0.0043]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000235 0.013866 0.004774]\n","Test  score: [0.00011  0.0105   0.003336]\n","RMSE  MAE\n","[[0.0108 0.0034]\n"," [0.0108 0.0034]\n"," [0.0089 0.003 ]\n"," [0.01   0.0034]\n"," [0.0117 0.0034]\n"," [0.0117 0.0037]\n"," [0.0102 0.0034]\n"," [0.0096 0.0029]\n"," [0.0146 0.0043]\n"," [0.0105 0.0033]]\n","AVG RMSE =  0.010675722267478704\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kYwKj5jjtgR_","executionInfo":{"status":"ok","timestamp":1627638087742,"user_tz":-540,"elapsed":13,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# m3_Night\n","\n","import numpy as np\n","\n","class MM:\n","    def __init__(self,MM_max,MM_min):\n","        self.max=MM_max\n","        self.min=MM_min\n","\n","# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n","def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n","    \n","    all_data=np.load('00/m3_Night.npy')\n","    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n","    len_total,feature,map_height,map_width=all_data.shape\n","    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n","    #len_total,feature,map_height,map_width=all_data.shape\n","    print('all_data shape: ',all_data.shape)\n","    #mm=MM(np.max(all_data),np.min(all_data))\n","    print('max=',np.max(all_data),' min=',np.min(all_data))\n","    \n","    #for time\n","    time=np.arange(len_total,dtype=int)\n","    #hour\n","    time_hour=time%T_period\n","    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n","    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n","    for i in range(len_total):\n","        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #day\n","    time_day=(time//T_period)%7\n","    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n","    for i in range(len_total):\n","        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #con\n","    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n","    \n","    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n","    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n","    \n","    if len_trend>0:\n","        number_of_skip_hours=T_trend*len_trend\n","    elif len_period>0:\n","        number_of_skip_hours=T_period*len_period\n","    elif len_closeness>0:\n","        number_of_skip_hours=T_closeness*len_closeness  \n","    else:\n","        print(\"wrong\")\n","    print('number_of_skip_hours:',number_of_skip_hours)\n","    \n","    Y=all_data[number_of_skip_hours:len_total]\n","\n","    if len_closeness>0:\n","        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n","        for i in range(len_closeness-1):\n","            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n","    if len_period>0:\n","        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n","        for i in range(len_period-1):\n","            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n","    if len_trend>0:\n","        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n","        for i in range(len_trend-1):\n","            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n","    \n","    matrix_T=matrix_T[number_of_skip_hours:]\n","    \n","    X_closeness_train=X_closeness[:-len_test] \n","    X_period_train=X_period[:-len_test] \n","    X_trend_train=X_trend[:-len_test]  \n","    T_train=matrix_T[:-len_test] \n","    X_closeness_test=X_closeness[-len_test:] \n","    X_period_test=X_period[-len_test:] \n","    X_trend_test=X_trend[-len_test:]          \n","    T_test=matrix_T[-len_test:]         \n","    \n","    X_train=[X_closeness_train,X_period_train,X_trend_train]\n","    X_test=[X_closeness_test,X_period_test,X_trend_test]\n","    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n","    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n","    Y_train=Y[:-len_test] \n","    Y_test=Y[-len_test:] \n","\n","    len_train=X_closeness_train.shape[0]\n","    len_test=X_closeness_test.shape[0]\n","    print('len_train='+str(len_train))\n","    print('len_test ='+str(len_test ))\n","    \n","    '''\n","    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n","    for i in range(poi.shape[0]):\n","        poi[i]=poi[i]/np.max(poi[i])\n","    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n","    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n","    \n","    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n","    '''\n","    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n","    "],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhHEwFastgdY","executionInfo":{"status":"ok","timestamp":1627640244394,"user_tz":-540,"elapsed":2156661,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}},"outputId":"fcb8900d-a722-4204-985d-c7529067be2f"},"source":["from keras.callbacks import ModelCheckpoint\n","import numpy as np\n","\n","NO=4\n","#for reproduction\n","seed=1\n","for i in range(NO):\n","    seed=seed*10+7\n","seed=seed*10+7\n","np.random.seed(seed)\n","import tensorflow as tf  #from V1707\n","from keras import backend as K\n","\n","'''\n","#hyperparameters\n","epoch = 100  # number of epoch at training stage\n","batch_size = 32  # batch size\n","lr = 0.0001  # learning rate\n","\n","#H,W,channel = 21,12,2   # grid size\n","H,W,channel = 14,12,1   # grid size\n","\n","#T = 24*1  # number of time intervals in one day\n","T = 8*1  # number of time intervals in one day\n","\n","len_closeness = 3  # length of closeness dependent sequence\n","len_period = 4  # length of peroid dependent sequence\n","len_trend = 4  # length of trend dependent sequence\n","\n","T_closeness,T_period,T_trend=1,T,T*7\n","\n","# last 7 days for testing data\n","days_test = 14\n","len_test = T * days_test\n","\n","#the number of repetition and if retrain the model\n","iterate_num=1\n","'''\n","\n","XDST=0  #DST\n","X11=1   #DSTN+ResPlus+PoI&Time\n","X10=1   #DSTN+ResPlus\n","X01=0   #DSTN+PoI&Time\n","X00=0   #DSTN\n","\n","trainDST=1  #DST\n","train11=1   #DSTN+ResPlus+PoI&Time\n","train10=1   #DSTN+ResPlus\n","train01=1   #DSTN+PoI&Time\n","train00=1   #DSTN\n","\n","\n","\n","\n","X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","\n","X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","\n","X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n","X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n","\n","index=np.arange(9)\n","#P_train=P_train[:,index,:,:]\n","#P_test =P_test [:,index,:,:]\n","'''\n","pre_F=64\n","conv_F=64\n","#R_N=10\n","    \n","#is_plus=True\n","plus=8\n","rate=1\n","    \n","is_pt=False\n","P_N=9\n","T_F=7*8\n","PT_F=9\n","\n","drop=0.1\n","'''\n","import time\n","count=0\n","count_sum=iterate_num\n","\n","iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","\n","RMSE=np.zeros([iterate_num,1])\n","MAE =np.zeros([iterate_num,1])\n","\n","for iterate_index in range(iterate_num):\n","    count=count+1\n","    time_start=time.time()       \n","    iterate=iterate_loop[iterate_index]\n","    \n","    #print(\"***** conv_model *****\")\n","    model=DeepSTN(H=H,W=W,channel=channel,\n","                  c=len_closeness,p=len_period,                \n","                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                  is_plus=is_plus,\n","                  plus=plus,rate=rate,     \n","                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                  drop=drop)            \n","    \n","    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n","    #train conv_model\n","    \n","    if train00:\n","        '''\n","        model_checkpoint=ModelCheckpoint(\n","                filepath=file_conv,\n","                monitor='val_rmse',\n","                verbose=1,\n","                save_best_only=True,\n","                save_weights_only=True,\n","                mode='min',\n","                period=1\n","            )\n","        '''    \n","        #print('=' * 10)\n","        #print(\"***** training conv_model *****\")\n","        history = model.fit((X_train_global, X_train), Y_train,\n","                            epochs=epoch,\n","                            batch_size=batch_size,\n","                            validation_split=0.1,\n","                            #callbacks=[model_checkpoint],\n","                            verbose=0,\n","                            #validation_data=((X_test_global, X_test), Y_test)\n","                            )\n","        \n","    #print('=' * 10)\n","    #print('***** evaluate *****')\n","    #model.load_weights(file_conv)\n","    \n","    #print(X_train_global.shape, X_train.shape)\n","    \n","    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","    \n","    print('              mse     rmse    mae')\n","    print('Train score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    #print(X_test_global.shape, X_test.shape)\n","    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n","    print('Test  score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    \n","\n","    \n","    RMSE[iterate_index,0]=score[1]\n","    MAE [iterate_index,0]=score[2]\n","    \n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","\n","    np.set_printoptions(precision=4, suppress=True)\n","    print('RMSE  MAE')\n","    print(for_show)\n","        \n","    #for_show=np.mean(for_show,axis=0)\n","    #print('RMSE  MAE')\n","    #print(for_show)\n","    \n","    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n","    '''    \n","    time_end=time.time()\n","    print('iterate cost',time_end-time_start)\n","    print(str(count)+'/'+str(count_sum))\n","    '''\n","print('AVG RMSE = ', avg_wo_minmax(RMSE))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["all_data shape:  (318, 1, 6, 12)\n","max= 9798.0  min= 0.0\n","mean= -0.991245619586935  variance= 0.05714575341805177\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","all_data shape:  (318, 1, 6, 12)\n","max= 203926.0  min= 0.0\n","mean= -0.9845239880352044  variance= 0.07316312563823049\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","              mse     rmse    mae\n","Train score: [0.000226 0.013244 0.004119]\n","Test  score: [0.000048 0.006921 0.002395]\n","RMSE  MAE\n","[[0.0069 0.0024]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000195 0.012131 0.003784]\n","Test  score: [0.000025 0.004961 0.001831]\n","RMSE  MAE\n","[[0.0069 0.0024]\n"," [0.005  0.0018]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000235 0.013519 0.004144]\n","Test  score: [0.000028 0.005255 0.001872]\n","RMSE  MAE\n","[[0.0069 0.0024]\n"," [0.005  0.0018]\n"," [0.0053 0.0019]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000202 0.012296 0.003813]\n","Test  score: [0.000019 0.00433  0.001597]\n","RMSE  MAE\n","[[0.0069 0.0024]\n"," [0.005  0.0018]\n"," [0.0053 0.0019]\n"," [0.0043 0.0016]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000259 0.014146 0.004254]\n","Test  score: [0.000025 0.005035 0.001872]\n","RMSE  MAE\n","[[0.0069 0.0024]\n"," [0.005  0.0018]\n"," [0.0053 0.0019]\n"," [0.0043 0.0016]\n"," [0.005  0.0019]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000209 0.01247  0.003902]\n","Test  score: [0.000025 0.005045 0.001789]\n","RMSE  MAE\n","[[0.0069 0.0024]\n"," [0.005  0.0018]\n"," [0.0053 0.0019]\n"," [0.0043 0.0016]\n"," [0.005  0.0019]\n"," [0.005  0.0018]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000233 0.01312  0.003958]\n","Test  score: [0.000025 0.004957 0.00174 ]\n","RMSE  MAE\n","[[0.0069 0.0024]\n"," [0.005  0.0018]\n"," [0.0053 0.0019]\n"," [0.0043 0.0016]\n"," [0.005  0.0019]\n"," [0.005  0.0018]\n"," [0.005  0.0017]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000207 0.012567 0.003605]\n","Test  score: [0.000025 0.005028 0.001562]\n","RMSE  MAE\n","[[0.0069 0.0024]\n"," [0.005  0.0018]\n"," [0.0053 0.0019]\n"," [0.0043 0.0016]\n"," [0.005  0.0019]\n"," [0.005  0.0018]\n"," [0.005  0.0017]\n"," [0.005  0.0016]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000237 0.013066 0.003806]\n","Test  score: [0.000026 0.005062 0.001627]\n","RMSE  MAE\n","[[0.0069 0.0024]\n"," [0.005  0.0018]\n"," [0.0053 0.0019]\n"," [0.0043 0.0016]\n"," [0.005  0.0019]\n"," [0.005  0.0018]\n"," [0.005  0.0017]\n"," [0.005  0.0016]\n"," [0.0051 0.0016]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000184 0.011892 0.003441]\n","Test  score: [0.000025 0.004987 0.001479]\n","RMSE  MAE\n","[[0.0069 0.0024]\n"," [0.005  0.0018]\n"," [0.0053 0.0019]\n"," [0.0043 0.0016]\n"," [0.005  0.0019]\n"," [0.005  0.0018]\n"," [0.005  0.0017]\n"," [0.005  0.0016]\n"," [0.0051 0.0016]\n"," [0.005  0.0015]]\n","AVG RMSE =  0.005041255091782659\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xwwgQZLttgj7","executionInfo":{"status":"ok","timestamp":1627640244395,"user_tz":-540,"elapsed":17,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# m4_Outdoor\n","\n","import numpy as np\n","\n","class MM:\n","    def __init__(self,MM_max,MM_min):\n","        self.max=MM_max\n","        self.min=MM_min\n","\n","# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n","def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n","    \n","    all_data=np.load('00/m4_Outdoor.npy')\n","    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n","    len_total,feature,map_height,map_width=all_data.shape\n","    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n","    #len_total,feature,map_height,map_width=all_data.shape\n","    print('all_data shape: ',all_data.shape)\n","    #mm=MM(np.max(all_data),np.min(all_data))\n","    print('max=',np.max(all_data),' min=',np.min(all_data))\n","    \n","    #for time\n","    time=np.arange(len_total,dtype=int)\n","    #hour\n","    time_hour=time%T_period\n","    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n","    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n","    for i in range(len_total):\n","        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #day\n","    time_day=(time//T_period)%7\n","    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n","    for i in range(len_total):\n","        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #con\n","    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n","    \n","    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n","    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n","    \n","    if len_trend>0:\n","        number_of_skip_hours=T_trend*len_trend\n","    elif len_period>0:\n","        number_of_skip_hours=T_period*len_period\n","    elif len_closeness>0:\n","        number_of_skip_hours=T_closeness*len_closeness  \n","    else:\n","        print(\"wrong\")\n","    print('number_of_skip_hours:',number_of_skip_hours)\n","    \n","    Y=all_data[number_of_skip_hours:len_total]\n","\n","    if len_closeness>0:\n","        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n","        for i in range(len_closeness-1):\n","            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n","    if len_period>0:\n","        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n","        for i in range(len_period-1):\n","            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n","    if len_trend>0:\n","        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n","        for i in range(len_trend-1):\n","            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n","    \n","    matrix_T=matrix_T[number_of_skip_hours:]\n","    \n","    X_closeness_train=X_closeness[:-len_test] \n","    X_period_train=X_period[:-len_test] \n","    X_trend_train=X_trend[:-len_test]  \n","    T_train=matrix_T[:-len_test] \n","    X_closeness_test=X_closeness[-len_test:] \n","    X_period_test=X_period[-len_test:] \n","    X_trend_test=X_trend[-len_test:]          \n","    T_test=matrix_T[-len_test:]         \n","    \n","    X_train=[X_closeness_train,X_period_train,X_trend_train]\n","    X_test=[X_closeness_test,X_period_test,X_trend_test]\n","    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n","    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n","    Y_train=Y[:-len_test] \n","    Y_test=Y[-len_test:] \n","\n","    len_train=X_closeness_train.shape[0]\n","    len_test=X_closeness_test.shape[0]\n","    print('len_train='+str(len_train))\n","    print('len_test ='+str(len_test ))\n","    \n","    '''\n","    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n","    for i in range(poi.shape[0]):\n","        poi[i]=poi[i]/np.max(poi[i])\n","    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n","    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n","    \n","    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n","    '''\n","    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n","    "],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fUmWkrptgqb","executionInfo":{"status":"ok","timestamp":1627642404488,"user_tz":-540,"elapsed":2160106,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}},"outputId":"a3b4c440-707f-484c-efd0-349b85ec235e"},"source":["from keras.callbacks import ModelCheckpoint\n","import numpy as np\n","\n","NO=4\n","#for reproduction\n","seed=1\n","for i in range(NO):\n","    seed=seed*10+7\n","seed=seed*10+7\n","np.random.seed(seed)\n","import tensorflow as tf  #from V1707\n","from keras import backend as K\n","\n","'''\n","#hyperparameters\n","epoch = 100  # number of epoch at training stage\n","batch_size = 32  # batch size\n","lr = 0.0001  # learning rate\n","\n","#H,W,channel = 21,12,2   # grid size\n","H,W,channel = 14,12,1   # grid size\n","\n","#T = 24*1  # number of time intervals in one day\n","T = 8*1  # number of time intervals in one day\n","\n","len_closeness = 3  # length of closeness dependent sequence\n","len_period = 4  # length of peroid dependent sequence\n","len_trend = 4  # length of trend dependent sequence\n","\n","T_closeness,T_period,T_trend=1,T,T*7\n","\n","# last 7 days for testing data\n","days_test = 14\n","len_test = T * days_test\n","\n","#the number of repetition and if retrain the model\n","iterate_num=1\n","'''\n","\n","XDST=0  #DST\n","X11=1   #DSTN+ResPlus+PoI&Time\n","X10=1   #DSTN+ResPlus\n","X01=0   #DSTN+PoI&Time\n","X00=0   #DSTN\n","\n","trainDST=1  #DST\n","train11=1   #DSTN+ResPlus+PoI&Time\n","train10=1   #DSTN+ResPlus\n","train01=1   #DSTN+PoI&Time\n","train00=1   #DSTN\n","\n","\n","\n","\n","X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","\n","X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","\n","X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n","X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n","\n","index=np.arange(9)\n","#P_train=P_train[:,index,:,:]\n","#P_test =P_test [:,index,:,:]\n","'''\n","pre_F=64\n","conv_F=64\n","#R_N=10\n","    \n","#is_plus=True\n","plus=8\n","rate=1\n","    \n","is_pt=False\n","P_N=9\n","T_F=7*8\n","PT_F=9\n","\n","drop=0.1\n","'''\n","import time\n","count=0\n","count_sum=iterate_num\n","\n","iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","\n","RMSE=np.zeros([iterate_num,1])\n","MAE =np.zeros([iterate_num,1])\n","\n","for iterate_index in range(iterate_num):\n","    count=count+1\n","    time_start=time.time()       \n","    iterate=iterate_loop[iterate_index]\n","    \n","    #print(\"***** conv_model *****\")\n","    model=DeepSTN(H=H,W=W,channel=channel,\n","                  c=len_closeness,p=len_period,                \n","                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                  is_plus=is_plus,\n","                  plus=plus,rate=rate,     \n","                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                  drop=drop)            \n","    \n","    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n","    #train conv_model\n","    \n","    if train00:\n","        '''\n","        model_checkpoint=ModelCheckpoint(\n","                filepath=file_conv,\n","                monitor='val_rmse',\n","                verbose=1,\n","                save_best_only=True,\n","                save_weights_only=True,\n","                mode='min',\n","                period=1\n","            )\n","        '''    \n","        #print('=' * 10)\n","        #print(\"***** training conv_model *****\")\n","        history = model.fit((X_train_global, X_train), Y_train,\n","                            epochs=epoch,\n","                            batch_size=batch_size,\n","                            validation_split=0.1,\n","                            #callbacks=[model_checkpoint],\n","                            verbose=0,\n","                            #validation_data=((X_test_global, X_test), Y_test)\n","                            )\n","        \n","    #print('=' * 10)\n","    #print('***** evaluate *****')\n","    #model.load_weights(file_conv)\n","    \n","    #print(X_train_global.shape, X_train.shape)\n","    \n","    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","    \n","    print('              mse     rmse    mae')\n","    print('Train score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    #print(X_test_global.shape, X_test.shape)\n","    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n","    print('Test  score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    \n","\n","    \n","    RMSE[iterate_index,0]=score[1]\n","    MAE [iterate_index,0]=score[2]\n","    \n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","\n","    np.set_printoptions(precision=4, suppress=True)\n","    print('RMSE  MAE')\n","    print(for_show)\n","        \n","    #for_show=np.mean(for_show,axis=0)\n","    #print('RMSE  MAE')\n","    #print(for_show)\n","    \n","    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n","    '''    \n","    time_end=time.time()\n","    print('iterate cost',time_end-time_start)\n","    print(str(count)+'/'+str(count_sum))\n","    '''\n","\n","print('AVG RMSE = ', avg_wo_minmax(RMSE))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["all_data shape:  (318, 1, 6, 12)\n","max= 10538.0  min= 0.0\n","mean= -0.9897332557995496  variance= 0.051730023472895494\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","all_data shape:  (318, 1, 6, 12)\n","max= 203926.0  min= 0.0\n","mean= -0.9845239880352044  variance= 0.07316312563823049\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","              mse     rmse    mae\n","Train score: [0.000375 0.017512 0.005082]\n","Test  score: [0.000085 0.009228 0.002961]\n","RMSE  MAE\n","[[0.0092 0.003 ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000413 0.018392 0.005275]\n","Test  score: [0.000083 0.009108 0.002981]\n","RMSE  MAE\n","[[0.0092 0.003 ]\n"," [0.0091 0.003 ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000351 0.017255 0.004988]\n","Test  score: [0.000098 0.009885 0.003037]\n","RMSE  MAE\n","[[0.0092 0.003 ]\n"," [0.0091 0.003 ]\n"," [0.0099 0.003 ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000356 0.017178 0.005124]\n","Test  score: [0.000092 0.009609 0.00317 ]\n","RMSE  MAE\n","[[0.0092 0.003 ]\n"," [0.0091 0.003 ]\n"," [0.0099 0.003 ]\n"," [0.0096 0.0032]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000371 0.017512 0.005223]\n","Test  score: [0.000102 0.0101   0.003217]\n","RMSE  MAE\n","[[0.0092 0.003 ]\n"," [0.0091 0.003 ]\n"," [0.0099 0.003 ]\n"," [0.0096 0.0032]\n"," [0.0101 0.0032]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000353 0.017046 0.004965]\n","Test  score: [0.000078 0.008834 0.002803]\n","RMSE  MAE\n","[[0.0092 0.003 ]\n"," [0.0091 0.003 ]\n"," [0.0099 0.003 ]\n"," [0.0096 0.0032]\n"," [0.0101 0.0032]\n"," [0.0088 0.0028]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000351 0.017199 0.005002]\n","Test  score: [0.000083 0.009109 0.002854]\n","RMSE  MAE\n","[[0.0092 0.003 ]\n"," [0.0091 0.003 ]\n"," [0.0099 0.003 ]\n"," [0.0096 0.0032]\n"," [0.0101 0.0032]\n"," [0.0088 0.0028]\n"," [0.0091 0.0029]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000358 0.017223 0.005155]\n","Test  score: [0.000093 0.009647 0.003099]\n","RMSE  MAE\n","[[0.0092 0.003 ]\n"," [0.0091 0.003 ]\n"," [0.0099 0.003 ]\n"," [0.0096 0.0032]\n"," [0.0101 0.0032]\n"," [0.0088 0.0028]\n"," [0.0091 0.0029]\n"," [0.0096 0.0031]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.00038  0.017704 0.005296]\n","Test  score: [0.000092 0.009575 0.003174]\n","RMSE  MAE\n","[[0.0092 0.003 ]\n"," [0.0091 0.003 ]\n"," [0.0099 0.003 ]\n"," [0.0096 0.0032]\n"," [0.0101 0.0032]\n"," [0.0088 0.0028]\n"," [0.0091 0.0029]\n"," [0.0096 0.0031]\n"," [0.0096 0.0032]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000374 0.017654 0.005346]\n","Test  score: [0.000094 0.009717 0.003275]\n","RMSE  MAE\n","[[0.0092 0.003 ]\n"," [0.0091 0.003 ]\n"," [0.0099 0.003 ]\n"," [0.0096 0.0032]\n"," [0.0101 0.0032]\n"," [0.0088 0.0028]\n"," [0.0091 0.0029]\n"," [0.0096 0.0031]\n"," [0.0096 0.0032]\n"," [0.0097 0.0033]]\n","AVG RMSE =  0.009484717971645296\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pJBT50u6tgxG","executionInfo":{"status":"ok","timestamp":1627642404488,"user_tz":-540,"elapsed":12,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# m5_Shop\n","\n","import numpy as np\n","\n","class MM:\n","    def __init__(self,MM_max,MM_min):\n","        self.max=MM_max\n","        self.min=MM_min\n","\n","# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n","def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n","    \n","    all_data=np.load('00/m5_Shop.npy')\n","    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n","    len_total,feature,map_height,map_width=all_data.shape\n","    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n","    #len_total,feature,map_height,map_width=all_data.shape\n","    print('all_data shape: ',all_data.shape)\n","    #mm=MM(np.max(all_data),np.min(all_data))\n","    print('max=',np.max(all_data),' min=',np.min(all_data))\n","    \n","    #for time\n","    time=np.arange(len_total,dtype=int)\n","    #hour\n","    time_hour=time%T_period\n","    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n","    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n","    for i in range(len_total):\n","        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #day\n","    time_day=(time//T_period)%7\n","    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n","    for i in range(len_total):\n","        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #con\n","    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n","    \n","    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n","    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n","    \n","    if len_trend>0:\n","        number_of_skip_hours=T_trend*len_trend\n","    elif len_period>0:\n","        number_of_skip_hours=T_period*len_period\n","    elif len_closeness>0:\n","        number_of_skip_hours=T_closeness*len_closeness  \n","    else:\n","        print(\"wrong\")\n","    print('number_of_skip_hours:',number_of_skip_hours)\n","    \n","    Y=all_data[number_of_skip_hours:len_total]\n","\n","    if len_closeness>0:\n","        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n","        for i in range(len_closeness-1):\n","            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n","    if len_period>0:\n","        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n","        for i in range(len_period-1):\n","            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n","    if len_trend>0:\n","        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n","        for i in range(len_trend-1):\n","            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n","    \n","    matrix_T=matrix_T[number_of_skip_hours:]\n","    \n","    X_closeness_train=X_closeness[:-len_test] \n","    X_period_train=X_period[:-len_test] \n","    X_trend_train=X_trend[:-len_test]  \n","    T_train=matrix_T[:-len_test] \n","    X_closeness_test=X_closeness[-len_test:] \n","    X_period_test=X_period[-len_test:] \n","    X_trend_test=X_trend[-len_test:]          \n","    T_test=matrix_T[-len_test:]         \n","    \n","    X_train=[X_closeness_train,X_period_train,X_trend_train]\n","    X_test=[X_closeness_test,X_period_test,X_trend_test]\n","    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n","    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n","    Y_train=Y[:-len_test] \n","    Y_test=Y[-len_test:] \n","\n","    len_train=X_closeness_train.shape[0]\n","    len_test=X_closeness_test.shape[0]\n","    print('len_train='+str(len_train))\n","    print('len_test ='+str(len_test ))\n","    \n","    '''\n","    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n","    for i in range(poi.shape[0]):\n","        poi[i]=poi[i]/np.max(poi[i])\n","    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n","    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n","    \n","    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n","    '''\n","    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n","    "],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDucuOaUtg4C","executionInfo":{"status":"ok","timestamp":1627644558994,"user_tz":-540,"elapsed":2154516,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}},"outputId":"478221c3-e12b-4e67-f05b-1a2f8b10dc7f"},"source":["from keras.callbacks import ModelCheckpoint\n","import numpy as np\n","\n","NO=4\n","#for reproduction\n","seed=1\n","for i in range(NO):\n","    seed=seed*10+7\n","seed=seed*10+7\n","np.random.seed(seed)\n","import tensorflow as tf  #from V1707\n","from keras import backend as K\n","\n","'''\n","#hyperparameters\n","epoch = 400  # number of epoch at training stage\n","batch_size = 32  # batch size\n","lr = 0.00001  # learning rate\n","\n","\n","#H,W,channel = 21,12,2   # grid size\n","H,W,channel = 19,12,1   # grid size\n","\n","#T = 24*1  # number of time intervals in one day\n","T = 2*1  # number of time intervals in one day\n","\n","len_closeness = 3  # length of closeness dependent sequence\n","len_period = 4  # length of peroid dependent sequence\n","len_trend = 4  # length of trend dependent sequence\n","\n","T_closeness,T_period,T_trend=1,T,T*7\n","\n","# last 7 days for testing data\n","days_test = 28\n","len_test = T * days_test\n","\n","#the number of repetition and if retrain the model\n","iterate_num=10\n","'''\n","\n","XDST=0  #DST\n","X11=1   #DSTN+ResPlus+PoI&Time\n","X10=1   #DSTN+ResPlus\n","X01=0   #DSTN+PoI&Time\n","X00=0   #DSTN\n","\n","trainDST=1  #DST\n","train11=1   #DSTN+ResPlus+PoI&Time\n","train10=1   #DSTN+ResPlus\n","train01=1   #DSTN+PoI&Time\n","train00=1   #DSTN\n","\n","\n","\n","\n","X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","\n","X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","\n","X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n","X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n","\n","index=np.arange(9)\n","#P_train=P_train[:,index,:,:]\n","#P_test =P_test [:,index,:,:]\n","'''\n","pre_F=64\n","conv_F=64\n","R_N=10\n","    \n","is_plus=True\n","plus=8\n","rate=1\n","    \n","is_pt=False\n","P_N=9\n","T_F=7*8\n","PT_F=9\n","\n","drop=0.1\n","'''\n","import time\n","count=0\n","count_sum=iterate_num\n","\n","iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","\n","RMSE=np.zeros([iterate_num,1])\n","MAE =np.zeros([iterate_num,1])\n","\n","for iterate_index in range(iterate_num):\n","    count=count+1\n","    time_start=time.time()       \n","    iterate=iterate_loop[iterate_index]\n","    \n","    #print(\"***** conv_model *****\")\n","    model=DeepSTN(H=H,W=W,channel=channel,\n","                  c=len_closeness,p=len_period,                \n","                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                  is_plus=is_plus,\n","                  plus=plus,rate=rate,     \n","                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                  drop=drop)            \n","    \n","    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n","    #train conv_model\n","    \n","    if train00:\n","        '''\n","        model_checkpoint=ModelCheckpoint(\n","                filepath=file_conv,\n","                monitor='val_rmse',\n","                verbose=1,\n","                save_best_only=True,\n","                save_weights_only=True,\n","                mode='min',\n","                period=1\n","            )\n","        '''    \n","        #print('=' * 10)\n","        #print(\"***** training conv_model *****\")\n","        history = model.fit((X_train_global, X_train), Y_train,\n","                            epochs=epoch,\n","                            batch_size=batch_size,\n","                            validation_split=0.1,\n","                            #callbacks=[model_checkpoint],\n","                            verbose=0,\n","                            #validation_data=((X_test_global, X_test), Y_test)\n","                            )\n","        \n","    #print('=' * 10)\n","    #print('***** evaluate *****')\n","    #model.load_weights(file_conv)\n","    \n","    #print(X_train_global.shape, X_train.shape)\n","    \n","    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","    \n","    print('              mse     rmse    mae')\n","    print('Train score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    #print(X_test_global.shape, X_test.shape)\n","    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n","    print('Test  score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    \n","\n","    \n","    RMSE[iterate_index,0]=score[1]\n","    MAE [iterate_index,0]=score[2]\n","    \n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","\n","    np.set_printoptions(precision=4, suppress=True)\n","    print('RMSE  MAE')\n","    print(for_show)\n","        \n","    #for_show=np.mean(for_show,axis=0)\n","    #print('RMSE  MAE')\n","    #print(for_show)\n","    \n","    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n","    '''    \n","    time_end=time.time()\n","    print('iterate cost',time_end-time_start)\n","    print(str(count)+'/'+str(count_sum))\n","    '''\n","\n","print('AVG RMSE = ', avg_wo_minmax(RMSE))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["all_data shape:  (318, 1, 6, 12)\n","max= 23449.0  min= 0.0\n","mean= -0.9831642076523984  variance= 0.0809059464034166\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","all_data shape:  (318, 1, 6, 12)\n","max= 203926.0  min= 0.0\n","mean= -0.9845239880352044  variance= 0.07316312563823049\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","              mse     rmse    mae\n","Train score: [0.000243 0.013906 0.005196]\n","Test  score: [0.000054 0.007326 0.002917]\n","RMSE  MAE\n","[[0.0073 0.0029]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000329 0.017065 0.005937]\n","Test  score: [0.000103 0.010171 0.003255]\n","RMSE  MAE\n","[[0.0073 0.0029]\n"," [0.0102 0.0033]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000246 0.01422  0.004801]\n","Test  score: [0.000054 0.007329 0.002335]\n","RMSE  MAE\n","[[0.0073 0.0029]\n"," [0.0102 0.0033]\n"," [0.0073 0.0023]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000277 0.015205 0.005411]\n","Test  score: [0.000058 0.007608 0.002752]\n","RMSE  MAE\n","[[0.0073 0.0029]\n"," [0.0102 0.0033]\n"," [0.0073 0.0023]\n"," [0.0076 0.0028]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000231 0.013763 0.005081]\n","Test  score: [0.00006  0.007772 0.002752]\n","RMSE  MAE\n","[[0.0073 0.0029]\n"," [0.0102 0.0033]\n"," [0.0073 0.0023]\n"," [0.0076 0.0028]\n"," [0.0078 0.0028]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000355 0.016428 0.005965]\n","Test  score: [0.000048 0.006907 0.00267 ]\n","RMSE  MAE\n","[[0.0073 0.0029]\n"," [0.0102 0.0033]\n"," [0.0073 0.0023]\n"," [0.0076 0.0028]\n"," [0.0078 0.0028]\n"," [0.0069 0.0027]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000275 0.014633 0.004975]\n","Test  score: [0.000049 0.006976 0.002304]\n","RMSE  MAE\n","[[0.0073 0.0029]\n"," [0.0102 0.0033]\n"," [0.0073 0.0023]\n"," [0.0076 0.0028]\n"," [0.0078 0.0028]\n"," [0.0069 0.0027]\n"," [0.007  0.0023]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000233 0.013753 0.004964]\n","Test  score: [0.000052 0.007182 0.002546]\n","RMSE  MAE\n","[[0.0073 0.0029]\n"," [0.0102 0.0033]\n"," [0.0073 0.0023]\n"," [0.0076 0.0028]\n"," [0.0078 0.0028]\n"," [0.0069 0.0027]\n"," [0.007  0.0023]\n"," [0.0072 0.0025]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000243 0.013943 0.004746]\n","Test  score: [0.000059 0.00767  0.002535]\n","RMSE  MAE\n","[[0.0073 0.0029]\n"," [0.0102 0.0033]\n"," [0.0073 0.0023]\n"," [0.0076 0.0028]\n"," [0.0078 0.0028]\n"," [0.0069 0.0027]\n"," [0.007  0.0023]\n"," [0.0072 0.0025]\n"," [0.0077 0.0025]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000241 0.014083 0.00501 ]\n","Test  score: [0.000049 0.006979 0.002477]\n","RMSE  MAE\n","[[0.0073 0.0029]\n"," [0.0102 0.0033]\n"," [0.0073 0.0023]\n"," [0.0076 0.0028]\n"," [0.0078 0.0028]\n"," [0.0069 0.0027]\n"," [0.007  0.0023]\n"," [0.0072 0.0025]\n"," [0.0077 0.0025]\n"," [0.007  0.0025]]\n","AVG RMSE =  0.007355300418566912\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QxHVvubStg-d","executionInfo":{"status":"ok","timestamp":1627644559471,"user_tz":-540,"elapsed":484,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}}},"source":["# m6_Tra\n","\n","import numpy as np\n","\n","class MM:\n","    def __init__(self,MM_max,MM_min):\n","        self.max=MM_max\n","        self.min=MM_min\n","\n","# 전처리에서 한 타임슬롯이 6시간으로 처리, T_period는 기존에 24여서 4로, T_trend는 기존에 일주일이어서 4*7로 수정\n","def lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness=1,T_period=4,T_trend=4*7):\n","    \n","    all_data=np.load('00/m6_Tra.npy')\n","    all_data = all_data[:, np.newaxis, :, :] # 기존 코드와 dimension 맞춰주기 위해서 피쳐 축 하나 추가\n","    len_total,feature,map_height,map_width=all_data.shape\n","    #all_data=np.arange(48*24*7*256).reshape(-1,2,16,8)\n","    #len_total,feature,map_height,map_width=all_data.shape\n","    print('all_data shape: ',all_data.shape)\n","    #mm=MM(np.max(all_data),np.min(all_data))\n","    print('max=',np.max(all_data),' min=',np.min(all_data))\n","    \n","    #for time\n","    time=np.arange(len_total,dtype=int)\n","    #hour\n","    time_hour=time%T_period\n","    #matrix_hour=np.zeros([len_total,24,map_height,map_width]) # len_total(시간 단위) 마다 시간 별 맵 매트릭스 초기화\n","    matrix_hour=np.zeros([len_total,T_period,map_height,map_width]) # 24가 하루를 나타낸다고 보고, 한 슬랏에 6시간이니 24-> 4로 바꿈\n","    for i in range(len_total):\n","        matrix_hour[i,time_hour[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #day\n","    time_day=(time//T_period)%7\n","    matrix_day=np.zeros([len_total,7,map_height,map_width]) # len_total(시간 단위) 마다 요일 별 맵 매트릭스 초기화\n","    for i in range(len_total):\n","        matrix_day[i,time_day[i],:,:]=1 # 대각선 부분만 1로 초기화?\n","    #con\n","    matrix_T=np.concatenate((matrix_hour,matrix_day),axis=1) # 가로로 concat\n","    \n","    all_data=(2.0*all_data-(np.max(all_data)+np.min(all_data)))/(np.max(all_data)-np.min(all_data))\n","    print('mean=',np.mean(all_data),' variance=',np.std(all_data))\n","    \n","    if len_trend>0:\n","        number_of_skip_hours=T_trend*len_trend\n","    elif len_period>0:\n","        number_of_skip_hours=T_period*len_period\n","    elif len_closeness>0:\n","        number_of_skip_hours=T_closeness*len_closeness  \n","    else:\n","        print(\"wrong\")\n","    print('number_of_skip_hours:',number_of_skip_hours)\n","    \n","    Y=all_data[number_of_skip_hours:len_total]\n","\n","    if len_closeness>0:\n","        X_closeness=all_data[number_of_skip_hours-T_closeness:len_total-T_closeness]\n","        for i in range(len_closeness-1):\n","            X_closeness=np.concatenate((X_closeness,all_data[number_of_skip_hours-T_closeness*(2+i):len_total-T_closeness*(2+i)]),axis=1)\n","    if len_period>0:\n","        X_period=all_data[number_of_skip_hours-T_period:len_total-T_period]\n","        for i in range(len_period-1):\n","            X_period=np.concatenate((X_period,all_data[number_of_skip_hours-T_period*(2+i):len_total-T_period*(2+i)]),axis=1)\n","    if len_trend>0:\n","        X_trend=all_data[number_of_skip_hours-T_trend:len_total-T_trend]\n","        for i in range(len_trend-1):\n","            X_trend=np.concatenate((X_trend,all_data[number_of_skip_hours-T_trend*(2+i):len_total-T_trend*(2+i)]),axis=1)\n","    \n","    matrix_T=matrix_T[number_of_skip_hours:]\n","    \n","    X_closeness_train=X_closeness[:-len_test] \n","    X_period_train=X_period[:-len_test] \n","    X_trend_train=X_trend[:-len_test]  \n","    T_train=matrix_T[:-len_test] \n","    X_closeness_test=X_closeness[-len_test:] \n","    X_period_test=X_period[-len_test:] \n","    X_trend_test=X_trend[-len_test:]          \n","    T_test=matrix_T[-len_test:]         \n","    \n","    X_train=[X_closeness_train,X_period_train,X_trend_train]\n","    X_test=[X_closeness_test,X_period_test,X_trend_test]\n","    #X_train=np.concatenate((X_closeness_train,X_period_train,X_trend_train),axis=1)\n","    #X_test=np.concatenate((X_closeness_test,X_period_test,X_trend_test),axis=1)\n","    Y_train=Y[:-len_test] \n","    Y_test=Y[-len_test:] \n","\n","    len_train=X_closeness_train.shape[0]\n","    len_test=X_closeness_test.shape[0]\n","    print('len_train='+str(len_train))\n","    print('len_test ='+str(len_test ))\n","    \n","    '''\n","    poi=np.load('DATA/dataBikeNYC/poi_data.npy')\n","    for i in range(poi.shape[0]):\n","        poi[i]=poi[i]/np.max(poi[i])\n","    P_train=np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_train,axis=0)\n","    P_test =np.repeat(poi.reshape(1,poi.shape[0],map_height,map_width),len_test ,axis=0)\n","    \n","    return X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,mm.max-mm.min\n","    '''\n","    return X_train,T_train,Y_train,X_test,T_test,Y_test,np.max(all_data)-np.min(all_data)\n","    "],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KS2cSMb8thFU","executionInfo":{"status":"ok","timestamp":1627646719998,"user_tz":-540,"elapsed":2160543,"user":{"displayName":"HyunMo Ko","photoUrl":"","userId":"07323368702794150329"}},"outputId":"704f7cb8-4b57-4a45-e119-cb210abd8bae"},"source":["from keras.callbacks import ModelCheckpoint\n","import numpy as np\n","\n","NO=4\n","#for reproduction\n","seed=1\n","for i in range(NO):\n","    seed=seed*10+7\n","seed=seed*10+7\n","np.random.seed(seed)\n","import tensorflow as tf  #from V1707\n","from keras import backend as K\n","\n","'''\n","#hyperparameters\n","epoch = 800  # number of epoch at training stage\n","batch_size = 32  # batch size\n","lr = 0.00001  # learning rate\n","\n","\n","#H,W,channel = 21,12,2   # grid size\n","H,W,channel = 19,12,1   # grid size\n","\n","#T = 24*1  # number of time intervals in one day\n","T = 2*1  # number of time intervals in one day\n","\n","len_closeness = 3  # length of closeness dependent sequence\n","len_period = 4  # length of peroid dependent sequence\n","len_trend = 4  # length of trend dependent sequence\n","\n","T_closeness,T_period,T_trend=1,T,T*7\n","\n","# last 7 days for testing data\n","days_test = 28\n","len_test = T * days_test\n","\n","#the number of repetition and if retrain the model\n","iterate_num=10\n","'''\n","\n","XDST=0  #DST\n","X11=1   #DSTN+ResPlus+PoI&Time\n","X10=1   #DSTN+ResPlus\n","X01=0   #DSTN+PoI&Time\n","X00=0   #DSTN\n","\n","trainDST=1  #DST\n","train11=1   #DSTN+ResPlus+PoI&Time\n","train10=1   #DSTN+ResPlus\n","train01=1   #DSTN+PoI&Time\n","train00=1   #DSTN\n","\n","\n","\n","\n","X_train,T_train,Y_train,X_test,T_test,Y_test,MM = lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","X_train_global,T_train_global,Y_train_global,X_test_global,T_test_global,Y_test_global,MM_global = lzq_load_global_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","\n","X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","\n","X_train_global=np.concatenate((X_train_global[0],X_train_global[1],X_train_global[2]),axis=1)\n","X_test_global=np.concatenate((X_test_global[0],X_test_global[1],X_test_global[2]),axis=1)\n","\n","index=np.arange(9)\n","#P_train=P_train[:,index,:,:]\n","#P_test =P_test [:,index,:,:]\n","'''\n","pre_F=64\n","conv_F=64\n","R_N=10\n","    \n","is_plus=True\n","plus=8\n","rate=1\n","    \n","is_pt=False\n","P_N=9\n","T_F=7*8\n","PT_F=9\n","\n","drop=0.1\n","'''\n","import time\n","count=0\n","count_sum=iterate_num\n","\n","iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","\n","RMSE=np.zeros([iterate_num,1])\n","MAE =np.zeros([iterate_num,1])\n","\n","for iterate_index in range(iterate_num):\n","    count=count+1\n","    time_start=time.time()       \n","    iterate=iterate_loop[iterate_index]\n","    \n","    #print(\"***** conv_model *****\")\n","    model=DeepSTN(H=H,W=W,channel=channel,\n","                  c=len_closeness,p=len_period,                \n","                  pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                  is_plus=is_plus,\n","                  plus=plus,rate=rate,     \n","                  is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                  drop=drop)            \n","    \n","    #file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n","    #train conv_model\n","    \n","    if train00:\n","        '''\n","        model_checkpoint=ModelCheckpoint(\n","                filepath=file_conv,\n","                monitor='val_rmse',\n","                verbose=1,\n","                save_best_only=True,\n","                save_weights_only=True,\n","                mode='min',\n","                period=1\n","            )\n","        '''    \n","        #print('=' * 10)\n","        #print(\"***** training conv_model *****\")\n","        history = model.fit((X_train_global, X_train), Y_train,\n","                            epochs=epoch,\n","                            batch_size=batch_size,\n","                            validation_split=0.1,\n","                            #callbacks=[model_checkpoint],\n","                            verbose=0,\n","                            #validation_data=((X_test_global, X_test), Y_test)\n","                            )\n","        \n","    #print('=' * 10)\n","    #print('***** evaluate *****')\n","    #model.load_weights(file_conv)\n","    \n","    #print(X_train_global.shape, X_train.shape)\n","    \n","    score = model.evaluate((X_train_global, X_train), Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","    \n","    print('              mse     rmse    mae')\n","    print('Train score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    #print(X_test_global.shape, X_test.shape)\n","    score = model.evaluate((X_test_global, X_test),  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n","    print('Test  score:',end=' ')\n","    np.set_printoptions(precision=6, suppress=True)\n","    print(np.array(score))\n","    \n","\n","    \n","    RMSE[iterate_index,0]=score[1]\n","    MAE [iterate_index,0]=score[2]\n","    \n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","\n","    np.set_printoptions(precision=4, suppress=True)\n","    print('RMSE  MAE')\n","    print(for_show)\n","        \n","    #for_show=np.mean(for_show,axis=0)\n","    #print('RMSE  MAE')\n","    #print(for_show)\n","    \n","    #np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n","    '''    \n","    time_end=time.time()\n","    print('iterate cost',time_end-time_start)\n","    print(str(count)+'/'+str(count_sum))\n","    '''\n","\n","print('AVG RMSE = ', avg_wo_minmax(RMSE))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["all_data shape:  (318, 1, 6, 12)\n","max= 22775.0  min= 0.0\n","mean= -0.9907625757397933  variance= 0.05521582836169721\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","all_data shape:  (318, 1, 6, 12)\n","max= 203926.0  min= 0.0\n","mean= -0.9845239880352044  variance= 0.07316312563823049\n","number_of_skip_hours: 56\n","len_train=206\n","len_test =56\n","              mse     rmse    mae\n","Train score: [0.000381 0.015853 0.004653]\n","Test  score: [0.000047 0.006845 0.002476]\n","RMSE  MAE\n","[[0.0068 0.0025]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000342 0.015214 0.004679]\n","Test  score: [0.000036 0.006001 0.002296]\n","RMSE  MAE\n","[[0.0068 0.0025]\n"," [0.006  0.0023]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000314 0.01458  0.004454]\n","Test  score: [0.00005  0.007055 0.002635]\n","RMSE  MAE\n","[[0.0068 0.0025]\n"," [0.006  0.0023]\n"," [0.0071 0.0026]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000374 0.015827 0.004548]\n","Test  score: [0.000041 0.006387 0.002271]\n","RMSE  MAE\n","[[0.0068 0.0025]\n"," [0.006  0.0023]\n"," [0.0071 0.0026]\n"," [0.0064 0.0023]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.00032  0.014879 0.004741]\n","Test  score: [0.000051 0.007123 0.002859]\n","RMSE  MAE\n","[[0.0068 0.0025]\n"," [0.006  0.0023]\n"," [0.0071 0.0026]\n"," [0.0064 0.0023]\n"," [0.0071 0.0029]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000366 0.015744 0.004864]\n","Test  score: [0.000042 0.006471 0.002533]\n","RMSE  MAE\n","[[0.0068 0.0025]\n"," [0.006  0.0023]\n"," [0.0071 0.0026]\n"," [0.0064 0.0023]\n"," [0.0071 0.0029]\n"," [0.0065 0.0025]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000355 0.015663 0.004596]\n","Test  score: [0.000032 0.005656 0.002101]\n","RMSE  MAE\n","[[0.0068 0.0025]\n"," [0.006  0.0023]\n"," [0.0071 0.0026]\n"," [0.0064 0.0023]\n"," [0.0071 0.0029]\n"," [0.0065 0.0025]\n"," [0.0057 0.0021]\n"," [0.     0.    ]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000331 0.015888 0.004607]\n","Test  score: [0.000031 0.0056   0.001794]\n","RMSE  MAE\n","[[0.0068 0.0025]\n"," [0.006  0.0023]\n"," [0.0071 0.0026]\n"," [0.0064 0.0023]\n"," [0.0071 0.0029]\n"," [0.0065 0.0025]\n"," [0.0057 0.0021]\n"," [0.0056 0.0018]\n"," [0.     0.    ]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000472 0.017067 0.005045]\n","Test  score: [0.000067 0.008173 0.00306 ]\n","RMSE  MAE\n","[[0.0068 0.0025]\n"," [0.006  0.0023]\n"," [0.0071 0.0026]\n"," [0.0064 0.0023]\n"," [0.0071 0.0029]\n"," [0.0065 0.0025]\n"," [0.0057 0.0021]\n"," [0.0056 0.0018]\n"," [0.0082 0.0031]\n"," [0.     0.    ]]\n","              mse     rmse    mae\n","Train score: [0.000388 0.015548 0.004776]\n","Test  score: [0.000052 0.007221 0.002768]\n","RMSE  MAE\n","[[0.0068 0.0025]\n"," [0.006  0.0023]\n"," [0.0071 0.0026]\n"," [0.0064 0.0023]\n"," [0.0071 0.0029]\n"," [0.0065 0.0025]\n"," [0.0057 0.0021]\n"," [0.0056 0.0018]\n"," [0.0082 0.0031]\n"," [0.0072 0.0028]]\n","AVG RMSE =  0.006594828621018678\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aprWnDU5pwF7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-H5qRsgrtxzg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ToZhxhT_tx_s"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"69FLVj6KtyJj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpBHYYmrtyTE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSrTiF06tycZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMSrD5sAtylz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_92N60ZtyvP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LIUSHCm4ty4r"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wD_MEOuyty71"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4FxssZPtzNr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pS1x7kHVoG5d"},"source":["# \"DeepSTN_net.py\"\n","\n","from keras import backend as K\n","K.set_image_data_format('channels_first')\n","import numpy as np\n","\n","from keras.optimizers import Adam\n","from keras.layers import Input,Activation,Dropout,BatchNormalization,AveragePooling2D,ZeroPadding2D,Multiply\n","from keras.layers import Lambda,Reshape,Concatenate,Add,Permute,TimeDistributed\n","from keras.models import Model\n","from keras.layers.convolutional import Conv2D\n","import PPT3_network.metrics as metrics\n","\n","# Relu-BN-Conv2D 3x3\n","def conv_unit0(Fin,Fout,drop,H,W):\n","    unit_input=Input(shape=(Fin,H,W))\n","    \n","    unit_conv=Activation('relu')(unit_input)\n","    unit_conv=BatchNormalization()(unit_conv)\n","    unit_conv=Dropout(drop)(unit_conv)\n","    unit_output=Conv2D(filters=Fout,kernel_size=(3,3),padding=\"same\")(unit_conv)\n","    unit_model=Model(inputs=unit_input,outputs=unit_output)\n","    print('kernel=(3,3)')\n","    return unit_model\n","\n","# Relu-BN-Conv2D 1x1\n","def conv_unit1(Fin,Fout,drop,H,W):\n","    unit_input=Input(shape=(Fin,H,W))\n","    \n","    unit_conv=Activation('relu')(unit_input)\n","    unit_conv=BatchNormalization()(unit_conv)\n","    unit_conv=Dropout(drop)(unit_conv)\n","    unit_output=Conv2D(filters=Fout,kernel_size=(1,1),padding=\"same\")(unit_conv)\n","    unit_model=Model(inputs=unit_input,outputs=unit_output)\n","    print('kernel=(1,1)')\n","    return unit_model\n","\n","# efficient version of Res_plus\n","def Res_plus_E(name,F,Fplus,rate,drop,H,W):\n","    cl_input=Input(shape=(F,H,W))\n","\n","    #normal channels\n","    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n","\n","    #separated channels\n","    if rate == 1:\n","        cl_conv1B=cl_input\n","    if rate !=1:\n","        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n","\n","    HR,WR=int(np.floor(H/rate)),int(np.floor(W/rate))\n","    cl_conv1B=Activation('relu')(cl_conv1B)\n","    cl_conv1B=BatchNormalization()(cl_conv1B)\n","\n","    cl_conv1B=Conv2D(filters=Fplus,kernel_size=(1,1),use_bias=False,padding=\"same\")(cl_conv1B)\n","    cl_conv1B=Reshape((Fplus,1,HR,WR),input_shape=(Fplus,HR,WR))(cl_conv1B)\n","    attention=Conv2D(filters=H*W,kernel_size=(HR,WR),use_bias=False,W_constraint=nonneg(),padding=\"valid\")\n","    cl_conv1B=TimeDistributed(attention)(cl_conv1B)\n","    cl_conv1B=Reshape((Fplus,H,W),input_shape=(Fplus,H*W,1,1))(cl_conv1B)\n","\n","    #merge\n","    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n","\n","    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n","\n","    cl_out=Add()([cl_input,cl_conv2])\n","\n","    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n","\n","    return cl_model\n","\n","# new resdual block\n","def Res_plus(name,F,Fplus,rate,drop,H,W):\n","    cl_input=Input(shape=(F,H,W))\n","    \n","    cl_conv1A=conv_unit0(F,F-Fplus,drop,H,W)(cl_input)\n","    \n","    if rate == 1:\n","        cl_conv1B=cl_input\n","    if rate !=1:\n","        cl_conv1B=AveragePooling2D(pool_size=(rate,rate),strides=(rate,rate),padding=\"valid\")(cl_input)\n","   \n","    cl_conv1B=Activation('relu')(cl_conv1B)\n","    cl_conv1B=BatchNormalization()(cl_conv1B) \n","    \n","    plus_conv=Conv2D(filters=Fplus*H*W,kernel_size=(int(np.floor(H/rate)),int(np.floor(W/rate))),padding=\"valid\")\n","\n","    cl_conv1B=plus_conv(cl_conv1B)\n","\n","    cl_conv1B=Reshape((Fplus,H,W))(cl_conv1B)\n","\n","    cl_conv1=Concatenate(axis=1)([cl_conv1A,cl_conv1B])\n","    \n","    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n","    \n","    cl_out=Add()([cl_input,cl_conv2])\n","    \n","    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n","\n","    return cl_model\n","\n","# normal residual block\n","def Res_normal(name,F,drop,H,W):\n","    cl_input=Input(shape=(F,H,W))\n","    \n","    cl_conv1=conv_unit0(F,F,drop,H,W)(cl_input)\n","   \n","    cl_conv2=conv_unit0(F,F,drop,H,W)(cl_conv1)\n","    \n","    cl_out=Add()([cl_input,cl_conv2])\n","    \n","    cl_model=Model(inputs=cl_input,outputs=cl_out,name=name)\n","\n","    return cl_model\n","    \n","def cpt_slice(x, h1, h2):  \n","    return x[:,h1:h2,:,:]  \n","\n","# transfer Time vector to a number (e.g. corresponding to filters = 1 in Conv2D)  \n","def T_trans(T,T_F,H,W):\n","\n","    T_in=Input(shape=(T+7,H,W))\n","    T_mid=Conv2D(filters=T_F,kernel_size=(1,1),padding=\"same\")(T_in)\n","    T_act=Activation('relu')(T_mid)\n","    T_fin=Conv2D(filters=1,kernel_size=(1,1),padding=\"same\")(T_act)\n","    T_mul=Activation('relu')(T_fin)\n","    T_model=Model(inputs=T_in,outputs=T_mul)\n","\n","    return T_model    \n","  \n","# transfer Time vector and PoI matrix to time-weighted PoI matrix    \n","def PT_trans(name,P_N,PT_F,T,T_F,H,W,isPT_F):\n","    if 1:\n","        poi_in=Input(shape=(P_N,H,W))\n","        # T_times/day + 7days/week \n","        time_in=Input(shape=(T+7,H,W))\n","\n","        if P_N>=2:\n","            T_x0 =T_trans(T,T_F,H,W)(time_in)\n","            T_x1 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=3:\n","            T_x2 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=4:\n","            T_x3 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=5:\n","            T_x4 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=6:\n","            T_x5 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=7:\n","            T_x6 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=8:\n","            T_x7 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=9:\n","            T_x8 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=10:\n","            T_x9 =T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=11:\n","            T_x10=T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=12:\n","            T_x11=T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=13:\n","            T_x12=T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=14:\n","            T_x13=T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=15:\n","            T_x14=T_trans(T,T_F,H,W)(time_in)\n","        if P_N>=16:\n","            T_x15=T_trans(T,T_F,H,W)(time_in)\n","         \n","        \n","        if P_N==1:\n","            T_x=T_trans(T,T_F,H,W)(time_in)\n","        if P_N==2:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1])\n","        if P_N==3:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2])\n","        if P_N==4:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3])\n","        if P_N==5:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4])\n","        if P_N==6:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5])\n","        if P_N==7:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6])\n","        if P_N==8:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7])\n","        if P_N==9:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8])\n","        if P_N==10:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9])\n","        if P_N==11:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10])\n","        if P_N==12:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11])\n","        if P_N==13:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12])\n","        if P_N==14:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13])\n","        if P_N==15:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14])\n","        if P_N==16:\n","            T_x=Concatenate(axis=1)([T_x0,T_x1,T_x2,T_x3,T_x4,T_x5,T_x6,T_x7,T_x8,T_x9,T_x10,T_x11,T_x12,T_x13,T_x14,T_x15])\n","            \n","        poi_time=Multiply()([poi_in,T_x])\n","        if isPT_F:\n","            poi_time=Conv2D(filters=PT_F,kernel_size=(1,1),padding=\"same\")(poi_time)\n","            print('PT_F = YES')\n","        else:\n","            print('PT_F = NO')\n","        PT_model=Model(inputs=[poi_in,time_in],outputs=poi_time,name=name)\n","\n","        return PT_model\n","\n","# main model\n","def DeepSTN(H=12,W=14,channel=1,      #H-map_height W-map_width channel-map_channel\n","            c=3,p=4,t=4,              #c-closeness p-period t-trend\n","            pre_F=64,conv_F=64,R_N=2, #pre_F-prepare_conv_featrue conv_F-resnet_conv_featrue R_N-resnet_number\n","            is_plus=True,             #use ResPlus or mornal convolution\n","            is_plus_efficient=False,  #use the efficient version of ResPlus\n","            plus=8,rate=2,            #rate-pooling_rate\n","            is_pt=True,               #use PoI and Time or not\n","            P_N=6,T_F=28,PT_F=6,T=24, #P_N-poi_number T_F-time_feature PT_F-poi_time_feature T-T_times/day \n","            drop=0,\n","            is_summary=True, #show detail\n","            lr=0.0002,\n","            kernel1=1, #kernel1 decides whether early-fusion uses conv_unit0 or conv_unit1, 1 recommended\n","            isPT_F=1): #isPT_F decides whether PT_model uses one more Conv after multiplying PoI and Time, 1 recommended\n","    \n","    all_channel = channel * (c+p+t)\n","            \n","    cut0 = int( 0 )\n","    cut1 = int( cut0 + channel*c )\n","    cut2 = int( cut1 + channel*p )\n","    cut3 = int( cut2 + channel*t )\n","       \n","    cpt_input=Input(shape=(all_channel,H,W))\n","            \n","    c_input=Lambda(cpt_slice,arguments={'h1':cut0,'h2':cut1})(cpt_input)\n","    p_input=Lambda(cpt_slice,arguments={'h1':cut1,'h2':cut2})(cpt_input)\n","    t_input=Lambda(cpt_slice,arguments={'h1':cut2,'h2':cut3})(cpt_input)\n","    \n","    c_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(c_input)\n","    p_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(p_input)\n","    t_out1=Conv2D(filters=pre_F,kernel_size=(1,1),padding=\"same\")(t_input)\n","    \n","    if is_pt:\n","        poi_in=Input(shape=(P_N,H,W))\n","        # T_times/day + 7days/week \n","        time_in=Input(shape=(T+7,H,W))\n","\n","        PT_model=PT_trans('PT_trans',P_N,PT_F,T,T_F,H,W,isPT_F)\n","        \n","        poi_time=PT_model([poi_in,time_in])\n"," \n","        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1,poi_time]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n","        if kernel1:\n","            cpt=conv_unit1(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n","        else:\n","            cpt=conv_unit0(pre_F*3+PT_F*isPT_F+P_N*(not isPT_F),conv_F,drop,H,W)(cpt_con1)\n","    \n","    else:\n","        cpt_con1=Concatenate(axis=1)([c_out1,p_out1,t_out1]) # 그냥 CNN들이랑 더해서 Global PAttern 넣으려면 여기다 합치면 됨\n","        if kernel1:\n","            cpt=conv_unit1(pre_F*3,conv_F,drop,H,W)(cpt_con1)\n","        else:\n","            cpt=conv_unit0(pre_F*3,conv_F,drop,H,W)(cpt_con1)  \n","     \n","    if is_plus:\n","        if is_plus_efficient:\n","            for i in range(R_N):\n","                cpt=Res_plus_E('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n","        else:\n","            for i in range(R_N):\n","                cpt=Res_plus('Res_plus_'+str(i+1),conv_F,plus,rate,drop,H,W)(cpt)\n","\n","    else:  \n","        for i in range(R_N):\n","            cpt=Res_normal('Res_normal_'+str(i+1),conv_F,drop,H,W)(cpt)\n","\n","    \n","    cpt_conv2=Activation('relu')(cpt)\n","    cpt_out2=BatchNormalization()(cpt_conv2)\n","    cpt_conv1=Dropout(drop)(cpt_out2)\n","    cpt_conv1=Conv2D(filters=channel,kernel_size=(1, 1),padding=\"same\")(cpt_conv1)\n","    \n","    # 여기서 GCN 결과랑 parametric based fusion, ST REsnet 코드 참조    \n","    \n","    cpt_out1=Activation('tanh')(cpt_conv1)\n","            \n","    if is_pt:\n","        DeepSTN_model=Model(inputs=[cpt_input,poi_in,time_in],outputs=cpt_out1)\n","    else:\n","        DeepSTN_model=Model(inputs=cpt_input,outputs=cpt_out1)\n","\n","    DeepSTN_model.compile(loss='mse', optimizer=Adam(lr), metrics=[metrics.rmse,metrics.mae])\n","    \n","    if is_summary:\n","        DeepSTN_model.summary()\n","\n","            \n","    print('***** pre_F : ',pre_F       )\n","    print('***** conv_F: ',conv_F      )\n","    print('***** R_N   : ',R_N         )\n","    \n","    print('***** plus  : ',plus*is_plus)\n","    print('***** rate  : ',rate*is_plus)\n","    \n","    print('***** P_N   : ',P_N*is_pt   )\n","    print('***** T_F   : ',T_F*is_pt   )\n","    print('***** PT_F  : ',PT_F*is_pt*isPT_F )            \n","    print('***** T     : ',T           ) \n","    \n","    print('***** drop  : ',drop        )\n","     \n","    return DeepSTN_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M56kVKyKoUI5"},"source":["# \"metrics.py\"\n","\n","# import numpy as np\n","from keras import backend as K\n","\n","def mean_squared_error(y_true, y_pred):\n","    return K.mean(K.square(y_pred - y_true))\n","\n","\n","def root_mean_square_error(y_true, y_pred):\n","    return mean_squared_error(y_true, y_pred) ** 0.5\n","\n","def rmse(y_true, y_pred):\n","    return mean_squared_error(y_true, y_pred) ** 0.5\n","\n","\n","# aliases\n","mse = MSE = mean_squared_error\n","# rmse = RMSE = root_mean_square_error\n","\n","\n","def masked_mean_squared_error(y_true, y_pred):\n","    idx = (y_true > 1e-6).nonzero()\n","    return K.mean(K.square(y_pred[idx] - y_true[idx]))\n","\n","def masked_rmse(y_true, y_pred):\n","    return masked_mean_squared_error(y_true, y_pred) ** 0.5\n","\n","\n","def mean_absolute_error(y_true, y_pred):\n","    return K.mean(K.abs(y_pred - y_true))\n","\n","def mae(y_true, y_pred):\n","    return K.mean(K.abs(y_pred - y_true))\n","\n","\t\n","threshold=0.05\n","\n","def mean_absolute_percentage_error(y_true, y_pred):\n","    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n","\n","def mape(y_true, y_pred):\n","    return K.mean( K.abs(y_pred-y_true) / K.maximum(K.cast(threshold,'float32'),y_true+1.0) )\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KiE-NOpElnMI"},"source":["#ComparisionBikeNYC.py\n","\n","\n","#from __future__ import print_function\n","#from DATA.lzq_read_data_time_poi import lzq_load_data\n","from keras.callbacks import ModelCheckpoint\n","#import cPickle as pickle\n","import numpy as np\n","#import math\n","\n","NO=4\n","#for reproduction\n","seed=1\n","for i in range(NO):\n","    seed=seed*10+7\n","seed=seed*10+7\n","np.random.seed(seed)\n","#from ipdb import set_trace\n","#set_trace()\n","\n","#for GPU in Lab\n","device=6\n","\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(device)\n","import tensorflow as tf  #from V1707\n","config=tf.ConfigProto()  #from V1707\n","config.gpu_options.allow_growth=True  #from V1707\n","#config.gpu_options.per_process_gpu_memory_fraction=0.5\n","sess=tf.Session(config=config)  #from V1707\n","#import keras.backend.tensorflow_backend as KTF\n","#KTF._set_session(tf.Session(config=config))\n","import setproctitle  #from V1707\n","setproctitle.setproctitle('Comprison Start! @ ZiqianLin')  #from V1707\n","\n","from keras import backend as K\n","K.set_image_data_format('channels_first')\n","\n","\n","#hyperparameters\n","epoch = 1#350  # number of epoch at training stage\n","batch_size = 32  # batch size\n","lr = 0.0002  # learning rate\n","\n","#H,W,channel = 21,12,2   # grid size\n","H,W,channel = 18,15,1   # grid size\n","\n","#T = 24*1  # number of time intervals in one day\n","T = 4*1  # number of time intervals in one day\n","\n","len_closeness = 3  # length of closeness dependent sequence\n","len_period = 4  # length of peroid dependent sequence\n","len_trend = 4  # length of trend dependent sequence\n","\n","T_closeness,T_period,T_trend=1,T,T*7\n","\n","# last 7 days for testing data\n","days_test = 14\n","len_test = T * days_test\n","\n","#the number of repetition and if retrain the model\n","iterate_num=10\n","\n","XDST=0  #DST\n","X11=1   #DSTN+ResPlus+PoI&Time\n","X10=1   #DSTN+ResPlus\n","X01=0   #DSTN+PoI&Time\n","X00=0   #DSTN\n","\n","trainDST=1  #DST\n","train11=1   #DSTN+ResPlus+PoI&Time\n","train10=1   #DSTN+ResPlus\n","train01=1   #DSTN+PoI&Time\n","train00=1   #DSTN\n","\n","\n","\n","#DST result\n","if XDST:\n","    setproctitle.setproctitle('BJMobile DST @ ZiqianLin')  #from V1707\n","    \n","    print(\"loading data...\")\n","    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","    \n","    R_N = 4   # number of residual units\n","    \n","    from keras.optimizers import Adam\n","    from DST_network.STResNet import stresnet\n","    import DST_network.metrics as metrics\n","    \n","    def build_model(external_dim,CFN):\n","        c_conf = (len_closeness, channel, H, W) if len_closeness > 0 else None\n","        p_conf = (len_period,    channel, H, W) if len_period    > 0 else None\n","        t_conf = (len_trend,     channel, H, W) if len_trend     > 0 else None\n","    \n","        model = stresnet(c_conf=c_conf, p_conf=p_conf, t_conf=t_conf,\n","                         external_dim=external_dim, nb_residual_unit=R_N, CF=CFN)\n","        \n","        adam = Adam(lr=lr)\n","        model.compile(loss='mse', optimizer=adam, metrics=[metrics.rmse,metrics.mae])\n","        model.summary()\n","        #from keras.utils.visualize_util import plot\n","        #plot(model, to_file='model.png', show_shapes=True)\n","        return model\n","    \n","    \n","    CF=64\n","    \n","    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","    \n","    RMSE=np.zeros([iterate_num,1])\n","    MAE =np.zeros([iterate_num,1])\n","    count_sum=iterate_num\n","    \n","    import time\n","    \n","    count=0\n","    \n","        \n","    for iterate_index in range(iterate_num):\n","        count=count+1\n","        iterate=iterate_loop[iterate_index]\n","            \n","        time_start=time.time()\n","            \n","        F='DST_MODEL/dst_model_'+str(iterate)+'_.hdf5'\n","            \n","        model = build_model(external_dim=False,CFN=CF)\n","        if trainDST:\n","            model_checkpoint=ModelCheckpoint(\n","                filepath=F,\n","                monitor='val_rmse',\n","                verbose=1,\n","                save_best_only=True,\n","                save_weights_only=False,\n","                mode='min',\n","                period=1)\n","                \n","            print('=' * 10)\n","            print(\"training model...\")\n","            history = model.fit(X_train, Y_train,\n","                                nb_epoch=epoch,\n","                                batch_size=batch_size,\n","                                validation_split=0.1,\n","                                callbacks=[model_checkpoint],\n","                                verbose=1)\n","            \n","        print('=' * 10)\n","        print('evaluating using the model that has the best loss on the valid set')\n","        model.load_weights(F)\n","        \n","        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","        print('              mse     rmse    mae')\n","        print('Train score:',end=' ')\n","        np.set_printoptions(precision=6, suppress=True)\n","        score = model.evaluate(X_test, Y_test, batch_size=Y_test.shape[0], verbose=0)\n","        print('Test  score:',end=' ')\n","        np.set_printoptions(precision=6, suppress=True)\n","        print(np.array(score))\n","            \n","        RMSE[iterate_index,0]=score[1]\n","        MAE [iterate_index,0]=score[2]\n","            \n","        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","    \n","        np.set_printoptions(precision=4, suppress=True)\n","        print('RMSE  MAE')\n","        print(for_show)\n","           \n","        for_show=np.mean(for_show,axis=0)\n","        print('RMSE  MAE')\n","        print(for_show)\n","        \n","        np.save('DST_SCORE/dst_score.npy',[RMSE,MAE])\n","            \n","        time_end=time.time()\n","            \n","        print('totally cost',time_end-time_start)\n","        print(str(count)+'/'+str(count_sum))\n","\n","\n","\n","#DSTN+ResPlus+PoI&Time\n","if X11:\n","    setproctitle.setproctitle('BJMobile DSTN+ResPlus+PoI&Time @ ZiqianLin')  #from V1707\n","    from DeepSTN_network.DeepSTN_net import DeepSTN\n","    \n","    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","       \n","    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","    \n","    index=np.arange(9)\n","    P_train=P_train[:,index,:,:]\n","    P_test =P_test [:,index,:,:]\n","    \n","    pre_F=64\n","    conv_F=64\n","    R_N=2\n","       \n","    is_plus=True\n","    plus=8\n","    rate=1\n","       \n","    is_pt=True\n","    P_N=9\n","    T_F=7*8\n","    PT_F=9\n","    \n","    drop=0.1\n","    \n","    import time\n","    count=0\n","    count_sum=iterate_num\n","    \n","    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","    \n","    RMSE=np.zeros([iterate_num,1])\n","    MAE =np.zeros([iterate_num,1])\n","    \n","    for iterate_index in range(iterate_num):\n","        count=count+1\n","        time_start=time.time()       \n","        iterate=iterate_loop[iterate_index]\n","        \n","        print(\"***** conv_model *****\")\n","        model=DeepSTN(H=H,W=W,channel=channel,\n","                      c=len_closeness,p=len_period,                \n","                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                      is_plus=is_plus,\n","                      plus=plus,rate=rate,     \n","                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                      drop=drop)            \n","        \n","        file_conv='DeepSTN_11/MODEL/DeepSTN_11_model_'+str(iterate)+'.hdf5'\n","        #train conv_model\n","        if train11:\n","            model_checkpoint=ModelCheckpoint(\n","                    filepath=file_conv,\n","                    monitor='val_rmse',\n","                    verbose=1,\n","                    save_best_only=True,\n","                    save_weights_only=True,\n","                    mode='min',\n","                    period=1\n","                )\n","            \n","            print('=' * 10)\n","            print(\"***** training conv_model *****\")\n","            history = model.fit([X_train,P_train,T_train], Y_train,\n","                                epochs=epoch,\n","                                batch_size=batch_size,\n","                                validation_split=0.1,\n","                                callbacks=[model_checkpoint],\n","                                verbose=1)\n","            \n","        print('=' * 10)\n","        print('***** evaluate *****')\n","        model.load_weights(file_conv)\n","        \n","        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","        print('              mse     rmse    mae')\n","        print('Train score:',end=' ')\n","        np.set_printoptions(precision=6, suppress=True)\n","        print(np.array(score))\n","        score = model.evaluate([X_test ,P_test ,T_test ], Y_test, batch_size=Y_test.shape[0], verbose=0)\n","        print('Test  score:',end=' ')\n","        np.set_printoptions(precision=6, suppress=True)\n","        print(np.array(score))\n","        \n","        RMSE[iterate_index,0]=score[1]\n","        MAE [iterate_index,0]=score[2]\n","        \n","        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","    \n","        np.set_printoptions(precision=4, suppress=True)\n","        print('RMSE  MAE')\n","        print(for_show)\n","           \n","        for_show=np.mean(for_show,axis=0)\n","        print('RMSE  MAE')\n","        print(for_show)\n","        \n","        np.save('DeepSTN_11/SCORE/DeepSTN_11_score3.npy',[RMSE,MAE])\n","            \n","        time_end=time.time()\n","        print('iterate cost',time_end-time_start)\n","        print(str(count)+'/'+str(count_sum))\n","\n","\n","\n","#DSTN+ResPlus\n","if X10:\n","    setproctitle.setproctitle('BJMobile DSTN+ResPlus @ ZiqianLin')  #from V1707\n","    from DeepSTN_network.DeepSTN_net import DeepSTN\n","    \n","    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","       \n","    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","    \n","    #index=np.arange(9)\n","    #P_train=P_train[:,index,:,:]\n","    #P_test =P_test [:,index,:,:]\n","    \n","    pre_F=64\n","    conv_F=64\n","    R_N=2\n","       \n","    is_plus=True\n","    plus=8\n","    rate=1\n","       \n","    is_pt=False\n","    P_N=9\n","    T_F=7*8\n","    PT_F=9\n","    \n","    drop=0.1\n","    \n","    import time\n","    count=0\n","    count_sum=iterate_num\n","    \n","    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","    \n","    RMSE=np.zeros([iterate_num,1])\n","    MAE =np.zeros([iterate_num,1])\n","    \n","    for iterate_index in range(iterate_num):\n","        count=count+1\n","        time_start=time.time()       \n","        iterate=iterate_loop[iterate_index]\n","        \n","        print(\"***** conv_model *****\")\n","        model=DeepSTN(H=H,W=W,channel=channel,\n","                      c=len_closeness,p=len_period,                \n","                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                      is_plus=is_plus,\n","                      plus=plus,rate=rate,     \n","                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                      drop=drop)            \n","        \n","        file_conv='DeepSTN_10/MODEL/DeepSTN_10_model_'+str(iterate)+'.hdf5'\n","        #train conv_model\n","        if train10:\n","            model_checkpoint=ModelCheckpoint(\n","                    filepath=file_conv,\n","                    monitor='val_rmse',\n","                    verbose=1,\n","                    save_best_only=True,\n","                    save_weights_only=True,\n","                    mode='min',\n","                    period=1\n","                )\n","            \n","            print('=' * 10)\n","            print(\"***** training conv_model *****\")\n","            history = model.fit(X_train, Y_train,\n","                                epochs=epoch,\n","                                batch_size=batch_size,\n","                                validation_split=0.1,\n","                                callbacks=[model_checkpoint],\n","                                verbose=1)\n","            \n","        print('=' * 10)\n","        print('***** evaluate *****')\n","        model.load_weights(file_conv)\n","        \n","        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","        print('              mse     rmse    mae')\n","        print('Train score:',end=' ')\n","        np.set_printoptions(precision=6, suppress=True)\n","        print(np.array(score))\n","        score = model.evaluate(X_test,  Y_test, batch_size=Y_test.shape[0], verbose=0)\n","        print('Test  score:',end=' ')\n","        np.set_printoptions(precision=6, suppress=True)\n","        print(np.array(score))\n","        \n","        RMSE[iterate_index,0]=score[1]\n","        MAE [iterate_index,0]=score[2]\n","        \n","        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","    \n","        np.set_printoptions(precision=4, suppress=True)\n","        print('RMSE  MAE')\n","        print(for_show)\n","           \n","        for_show=np.mean(for_show,axis=0)\n","        print('RMSE  MAE')\n","        print(for_show)\n","        \n","        np.save('DeepSTN_10/SCORE/DeepSTN_10_score.npy',[RMSE,MAE])\n","            \n","        time_end=time.time()\n","        print('iterate cost',time_end-time_start)\n","        print(str(count)+'/'+str(count_sum))\n","    \n","    \n","    \n","#DSTN+PoI&Time\n","if X01:\n","    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n","    from DeepSTN_network.DeepSTN_net import DeepSTN\n","    \n","    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","      \n","    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","    \n","    #index=np.arange(9)\n","    #P_train=P_train[:,index,:,:]\n","    #P_test =P_test [:,index,:,:]\n","    \n","    pre_F=64\n","    conv_F=64\n","    R_N=2\n","       \n","    is_plus=False\n","    plus=8\n","    rate=1\n","       \n","    is_pt=True\n","    P_N=9\n","    T_F=7*8\n","    PT_F=9\n","    \n","    drop=0.1\n","    \n","    import time\n","    count=0\n","    count_sum=iterate_num\n","    \n","    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","    \n","    RMSE=np.zeros([iterate_num,1])\n","    MAE =np.zeros([iterate_num,1])\n","    \n","    for iterate_index in range(iterate_num):\n","        count=count+1\n","        time_start=time.time()       \n","        iterate=iterate_loop[iterate_index]\n","        \n","        print(\"***** conv_model *****\")\n","        model=DeepSTN(H=H,W=W,channel=channel,\n","                      c=len_closeness,p=len_period,                \n","                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                      is_plus=is_plus,\n","                      plus=plus,rate=rate,     \n","                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                      drop=drop)            \n","        \n","        file_conv='DeepSTN_01/MODEL/DeepSTN_01_model_'+str(iterate)+'.hdf5'\n","        #train conv_model\n","        if train01:\n","            model_checkpoint=ModelCheckpoint(\n","                    filepath=file_conv,\n","                    monitor='val_rmse',\n","                    verbose=1,\n","                    save_best_only=True,\n","                    save_weights_only=True,\n","                    mode='min',\n","                    period=1\n","                )\n","            \n","            print('=' * 10)\n","            print(\"***** training conv_model *****\")\n","            history = model.fit([X_train,P_train,T_train], Y_train,\n","                                epochs=epoch,\n","                                batch_size=batch_size,\n","                                validation_split=0.1,\n","                                callbacks=[model_checkpoint],\n","                                verbose=1)\n","            \n","        print('=' * 10)\n","        print('***** evaluate *****')\n","        model.load_weights(file_conv)\n","        \n","        score = model.evaluate([X_train,P_train,T_train], Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","        print('              mse     rmse    mae')\n","        print('Train score:',end=' ')\n","        np.set_printoptions(precision=6, suppress=True)\n","        print(np.array(score))\n","        score = model.evaluate([X_test, P_test, T_test ],  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n","        print('Test  score:',end=' ')\n","        np.set_printoptions(precision=6, suppress=True)\n","        print(np.array(score))\n","        \n","        RMSE[iterate_index,0]=score[1]\n","        MAE [iterate_index,0]=score[2]\n","        \n","        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","    \n","        np.set_printoptions(precision=4, suppress=True)\n","        print('RMSE  MAE')\n","        print(for_show)\n","           \n","        for_show=np.mean(for_show,axis=0)\n","        print('RMSE  MAE')\n","        print(for_show)\n","        \n","        np.save('DeepSTN_01/SCORE/DeepSTN_01_score.npy',[RMSE,MAE])\n","            \n","        time_end=time.time()\n","        print('iterate cost',time_end-time_start)\n","        print(str(count)+'/'+str(count_sum))\n"," \n","    \n","\n","#DSTN\n","if X00:\n","    setproctitle.setproctitle('BJMobile DSTN+PoI&Time @ ZiqianLin')  #from V1707\n","    from DeepSTN_network.DeepSTN_net import DeepSTN\n","    \n","    X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n","       \n","    X_train=np.concatenate((X_train[0],X_train[1],X_train[2]),axis=1)\n","    X_test =np.concatenate((X_test[0], X_test[1], X_test[2] ),axis=1)\n","    \n","    index=np.arange(9)\n","    P_train=P_train[:,index,:,:]\n","    P_test =P_test [:,index,:,:]\n","    \n","    pre_F=64\n","    conv_F=64\n","    R_N=2\n","       \n","    is_plus=False\n","    plus=8\n","    rate=1\n","       \n","    is_pt=False\n","    P_N=9\n","    T_F=7*8\n","    PT_F=9\n","    \n","    drop=0.1\n","    \n","    import time\n","    count=0\n","    count_sum=iterate_num\n","    \n","    iterate_loop=np.arange(iterate_num)+1+iterate_num*(NO-1)\n","    \n","    RMSE=np.zeros([iterate_num,1])\n","    MAE =np.zeros([iterate_num,1])\n","    \n","    for iterate_index in range(iterate_num):\n","        count=count+1\n","        time_start=time.time()       \n","        iterate=iterate_loop[iterate_index]\n","        \n","        print(\"***** conv_model *****\")\n","        model=DeepSTN(H=H,W=W,channel=channel,\n","                      c=len_closeness,p=len_period,                \n","                      pre_F=pre_F,conv_F=conv_F,R_N=R_N,    \n","                      is_plus=is_plus,\n","                      plus=plus,rate=rate,     \n","                      is_pt=is_pt,P_N=P_N,T_F=T_F,PT_F=PT_F,T=T,     \n","                      drop=drop)            \n","        \n","        file_conv='DeepSTN_00/MODEL/DeepSTN_00_model_'+str(iterate)+'.hdf5'\n","        #train conv_model\n","        if train00:\n","            model_checkpoint=ModelCheckpoint(\n","                    filepath=file_conv,\n","                    monitor='val_rmse',\n","                    verbose=1,\n","                    save_best_only=True,\n","                    save_weights_only=True,\n","                    mode='min',\n","                    period=1\n","                )\n","            \n","            print('=' * 10)\n","            print(\"***** training conv_model *****\")\n","            history = model.fit(X_train, Y_train,\n","                                epochs=epoch,\n","                                batch_size=batch_size,\n","                                validation_split=0.1,\n","                                callbacks=[model_checkpoint],\n","                                verbose=1)\n","            \n","        print('=' * 10)\n","        print('***** evaluate *****')\n","        model.load_weights(file_conv)\n","        \n","        score = model.evaluate(X_train, Y_train, batch_size=Y_train.shape[0] // 48, verbose=0)\n","        print('              mse     rmse    mae')\n","        print('Train score:',end=' ')\n","        np.set_printoptions(precision=6, suppress=True)\n","        print(np.array(score))\n","        score = model.evaluate(X_test,  Y_test,  batch_size=Y_test.shape[0], verbose=0)\n","        print('Test  score:',end=' ')\n","        np.set_printoptions(precision=6, suppress=True)\n","        print(np.array(score))\n","        \n","        RMSE[iterate_index,0]=score[1]\n","        MAE [iterate_index,0]=score[2]\n","        \n","        for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","    \n","        np.set_printoptions(precision=4, suppress=True)\n","        print('RMSE  MAE')\n","        print(for_show)\n","           \n","        for_show=np.mean(for_show,axis=0)\n","        print('RMSE  MAE')\n","        print(for_show)\n","        \n","        np.save('DeepSTN_00/SCORE/DeepSTN_00_score.npy',[RMSE,MAE])\n","            \n","        time_end=time.time()\n","        print('iterate cost',time_end-time_start)\n","        print(str(count)+'/'+str(count_sum))\n","\n","\n","\n","#Comparison\n","X_train,T_train,P_train,Y_train,X_test,T_test,P_test,Y_test,MM=lzq_load_data(len_test,len_closeness,len_period,len_trend,T_closeness,T_period,T_trend)\n"," \n","np.set_printoptions(precision=4, suppress=True)\n","print('MODEL                     RMSE  MAE')\n","if 0:\n","    print('ResNet                  :',end=' ')\n","    [RMSE,MAE]=np.load('DST_SCORE/dst_score.npy')\n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","    print(for_show)   \n","if 0:\n","    print('DeepSTN                 :',end=' ')\n","    [RMSE,MAE]=np.load('DeepSTN_00/SCORE/DeepSTN_00_score.npy')\n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","    for_show=np.mean(for_show,axis=0)\n","    print(for_show)   \n","if 1:\n","    print('DeepSTN+ResPlus         :',end=' ')\n","    [RMSE,MAE]=np.load('DeepSTN_10/SCORE/DeepSTN_10_score.npy')\n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","    for_show=np.mean(for_show,axis=0)\n","    print(for_show)   \n","if 0:\n","    print('DeepSTN+PoI&Time        :',end=' ')\n","    [RMSE,MAE]=np.load('DeepSTN_01/SCORE/DeepSTN_01_score.npy')\n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","    for_show=np.mean(for_show,axis=0)\n","    print(for_show)   \n","if 1:\n","    print('DeepSTN+ResPlus+PoI&Time:',end=' ')\n","    [RMSE,MAE]=np.load('DeepSTN_11/SCORE/DeepSTN_11_score3.npy')\n","    for_show=np.concatenate([RMSE,MAE],axis=1)*MM/2\n","    for_show=np.mean(for_show,axis=0)\n","    print(for_show)   \n"],"execution_count":null,"outputs":[]}]}